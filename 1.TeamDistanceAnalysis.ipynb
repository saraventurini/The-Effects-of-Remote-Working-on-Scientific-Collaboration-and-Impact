{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7469b0ad-ef92-47e8-a6e2-42955589ce33",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T20:20:41.120454Z",
     "iopub.status.busy": "2024-12-11T20:20:41.120209Z",
     "iopub.status.idle": "2024-12-11T20:20:41.149393Z",
     "shell.execute_reply": "2024-12-11T20:20:41.148994Z",
     "shell.execute_reply.started": "2024-12-11T20:20:41.120436Z"
    },
    "tags": []
   },
   "source": [
    "# Team Distances Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a21d76bb-6710-4b35-a7e2-6085b58d0a21",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from time import time\n",
    "from tqdm.auto import tqdm\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import networkx as nx\n",
    "import random\n",
    "tqdm.pandas()\n",
    "\n",
    "from scipy import optimize\n",
    "import numpy.polynomial.polynomial as npoly\n",
    "\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "import plotly.io as pio\n",
    "import seaborn as sns\n",
    "plt.rcParams.update({'font.size': 22})\n",
    "sns.set(style=\"ticks\", context=\"talk\")\n",
    "plt.style.use(\"dark_background\")\n",
    "pd.options.plotting.backend = 'plotly'\n",
    "pio.templates.default = 'plotly_dark+presentation'\n",
    "\n",
    "import matplotlib.dates as dates\n",
    "import matplotlib.dates as mdates\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "def form(x,pos):\n",
    "    if x<1e2:\n",
    "        return '%1.0f' % (x)\n",
    "    elif x<1e3:\n",
    "        return '%1.3f' % (x)\n",
    "    elif x<1e6:\n",
    "        return '%1.1fK' % (x * 1e-3)\n",
    "    else:\n",
    "        return '%1.1fM' % (x * 1e-6)\n",
    "formatter = FuncFormatter(form)\n",
    "\n",
    "def plot_(df_,x_column,y_column,x_label,title):\n",
    "    plt.style.use(\"dark_background\")\n",
    "    fig, ax = plt.subplots(figsize=(15, 5))\n",
    "    x_dates = list(df_[x_column])\n",
    "    y_data = df_[y_column]\n",
    "    x_data = x_dates\n",
    "\n",
    "    ax.plot(x_data, y_data, \"co-\", markersize=6,label='dataset')\n",
    "    ax.axvline(pd.Timestamp(2020, 3, 1),color='r')\n",
    "\n",
    "    plt.grid(True, linewidth=0.5)\n",
    "    ax.yaxis.set_major_formatter(formatter)\n",
    "\n",
    "    ax.set_xlabel(x_label,size=20)\n",
    "    ax.set_title(title,size=30)\n",
    "    #ax.legend()        \n",
    "    #plt.savefig(os.path.join(my_path_plots, title+'.png'), bbox_inches='tight', pad_inches=0.02)\n",
    "    \n",
    "def read_parquet(name, **args):\n",
    "    path = name\n",
    "    print(f'Reading {name!r}')\n",
    "    tic = time()\n",
    "    df = pd.read_parquet(path, engine='fastparquet', **args)\n",
    "    before = len(df)\n",
    "    # df.drop_duplicates(inplace=True)\n",
    "    toc = time()\n",
    "    after = len(df)\n",
    "        \n",
    "    print(f'Read {len(df):,} rows from {path.stem!r} in {toc-tic:.2f} sec. {before-after:,} duplicates.')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8fce74f-a4c7-41f6-80a5-b19d9bb9a59f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0675fae2-dd5a-44b9-b973-8b32f7e40536",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "basepath = Path('Tables_final') \n",
    "my_path_ = Path('TeamDistance')\n",
    "if not os.path.exists(my_path_):\n",
    "    os.makedirs(my_path_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480f9bea-a15d-4cfd-bd30-38925fcb9485",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T20:21:00.134437Z",
     "iopub.status.busy": "2024-12-11T20:21:00.134279Z",
     "iopub.status.idle": "2024-12-11T20:21:00.161195Z",
     "shell.execute_reply": "2024-12-11T20:21:00.160582Z",
     "shell.execute_reply.started": "2024-12-11T20:21:00.134422Z"
    }
   },
   "source": [
    "## Average Team Distance - ATD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654f31e7-f6f3-4521-9ea3-1cdc0e7caef4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "works = read_parquet(basepath / 'works')\n",
    "works_authors_aff = read_parquet(basepath / 'works_authors_aff')\n",
    "I_dist = read_parquet(basepath / 'I_dist_threshold')\n",
    "I_dist1 = I_dist\n",
    "I_dist2 = I_dist[I_dist.source!=I_dist.target]\n",
    "I_dist[['target','source']] = I_dist[['source','target']]\n",
    "I_dist = pd.concat([I_dist1,I_dist2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a11c3b-99ac-4f9b-bfdf-24fd5bfbbffb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f'{len(works.work_id)} considered preprints (no missing info)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4079c0-8d4e-4a27-bc02-8d0f23c87f66",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "works_authors_aff_ = works_authors_aff[['work_id','institution_id','author_id']]\n",
    "work_authors_set_df = works_authors_aff_.groupby('work_id').author_id.apply(set).to_frame().reset_index()\n",
    "\n",
    "#8 mins\n",
    "def create_edges(author_list):\n",
    "    G = nx.complete_graph(author_list)\n",
    "    return nx.to_pandas_edgelist(G)\n",
    "\n",
    "# Apply the function to each set of authors and concatenate the resulting DataFrames\n",
    "edge_list_dfs = work_authors_set_df.author_id.apply(create_edges)\n",
    "work_authors_edges_df = pd.concat(edge_list_dfs.tolist(), ignore_index=True)\n",
    "\n",
    "work_authors_set_df['num_authors'] = work_authors_set_df['author_id'].apply(lambda x:len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98feeb5-09b1-4e01-8e9f-da3e7a92d97c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import scipy.special\n",
    "binom_list = [scipy.special.binom(x, 2) for x in range(31)]\n",
    "binom_dict = dict(zip(list(range(31)),binom_list))\n",
    "\n",
    "work_authors_set_df['binom_n_authors'] = work_authors_set_df['num_authors'].map(binom_dict)\n",
    "work_authors_set_df['binom_n_authors'] = work_authors_set_df['binom_n_authors'].astype(int)\n",
    "\n",
    "work_id_list = work_authors_set_df['work_id']\n",
    "binom_list = work_authors_set_df['binom_n_authors']\n",
    "work_mult_binom = [[work_id_list[i]]*binom_list[i] for i in range(len(work_id_list))]\n",
    "import itertools\n",
    "work_mult_binom_ = list(itertools.chain(*work_mult_binom))\n",
    "work_authors_edges_df['work_id'] = work_mult_binom_\n",
    "work_authors_edges_df = work_authors_edges_df[['work_id','source','target']]\n",
    "\n",
    "#each paper #list edges #institutions #list distances #mean\n",
    "#institutions authors\n",
    "work_authors_edges_df = work_authors_edges_df.merge(works_authors_aff[['work_id','author_id','institution_id']].rename(columns={'author_id':'source','institution_id':'source_inst'}),on=['work_id','source'], how='left')\n",
    "work_authors_edges_df = work_authors_edges_df.merge(works_authors_aff[['work_id','author_id','institution_id']].rename(columns={'author_id':'target','institution_id':'target_inst'}),on=['work_id','target'], how='left')\n",
    "\n",
    "#distance institutions\n",
    "work_authors_edges_df = work_authors_edges_df.merge(I_dist.rename(columns={'source':'source_inst','target':'target_inst'}),on=['source_inst','target_inst'], how='left')\n",
    "\n",
    "my_file = 'work_authors_edges_df_dist'\n",
    "work_authors_edges_df.to_parquet(os.path.join(my_path_, my_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f224da8-4177-44c5-bd47-51c487f18b91",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#euclidean mean\n",
    "work_edges_dist_mean = work_authors_edges_df.groupby('work_id').dist.mean().to_frame().reset_index()\n",
    "work_edges_dist_mean = work_edges_dist_mean.merge(works[['work_id']].reset_index(),on='work_id')\n",
    "my_file = \"work_edges_dist_mean.csv\"    \n",
    "work_edges_dist_mean.to_csv(os.path.join(my_path_, my_file),index=False)\n",
    "work_edges_dist_mean_monthly = work_edges_dist_mean.groupby('publication_date_1').dist.mean().to_frame().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f36577-1d79-41ae-825d-25333fb9d9d3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cede4c11-7d76-4f5c-a291-69e65724d62f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#plot only point #not rolling average #fit 3 lines\n",
    "\n",
    "my_file = \"work_edges_dist_mean.csv\"    \n",
    "work_edges_dist_mean = pd.read_csv(os.path.join(my_path_, my_file))\n",
    "work_edges_dist_mean['publication_date_1'] = pd.to_datetime(work_edges_dist_mean['publication_date_1'])\n",
    "work_edges_dist_mean_monthly = work_edges_dist_mean.groupby('publication_date_1').dist.mean().to_frame().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82d98d4-eb17-454d-8db5-432ef61d396f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_fit_rolling_breakpoints2(df,x_column,x_label,title,window_size,num_breakpoints,ff):\n",
    "    \n",
    "    #save for all possible combination of breakpoints the correspondent error \n",
    "    def f(breakpoints, x, y, fcache): \n",
    "        breakpoints = tuple(map(int, sorted(breakpoints)))\n",
    "        if breakpoints not in fcache:\n",
    "            total_error = 0\n",
    "            for f, xi, yi in find_best_piecewise_polynomial(breakpoints, x, y):\n",
    "                total_error += ((f(xi) - yi)**2).sum()\n",
    "            fcache[breakpoints] = total_error \n",
    "        # print('{} --> {}'.format(breakpoints, fcache[breakpoints]))\n",
    "        return fcache[breakpoints]\n",
    "\n",
    "    def find_best_piecewise_polynomial(breakpoints, x, y):\n",
    "        breakpoints = tuple(map(int, sorted(breakpoints)))\n",
    "        xs = np.split(x, breakpoints)\n",
    "        ys = np.split(y, breakpoints)\n",
    "        result = []\n",
    "        for xi, yi in zip(xs, ys):\n",
    "            if len(xi) < 2: continue\n",
    "            coefs = npoly.polyfit(xi, yi, 1)\n",
    "            f = npoly.Polynomial(coefs)\n",
    "            result.append([f, xi, yi])\n",
    "        return result\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(15, 5))\n",
    "    x_dates = list(df['publication_date_1'])\n",
    "    y_data = df[x_column].rolling(window=window_size).mean()[window_size-1:]\n",
    "    x_data = x_dates[window_size-1:]\n",
    "    \n",
    "    ax.plot(x_data, y_data, \"co\", markersize=6)\n",
    "\n",
    "    plt.grid(True, linewidth=0.5)\n",
    "    #ax.yaxis.set_major_formatter(formatter)\n",
    "\n",
    "    ax.set_xlabel(x_label,size=20)\n",
    "    ax.set_title(title,size=30)\n",
    "\n",
    "    #ax.xaxis.set_major_locator(mdates.MonthLocator()) # Make ticks on occurrences of each month\n",
    "    ax.xaxis.set_major_formatter(mdates.DateFormatter('%b %y')) # Get only the month to show in the x-axis\n",
    "\n",
    "    ax.axvline(pd.Timestamp(2020, 3, 1),color='r') #ax.axvline(x_dates[84],color='r')\n",
    "\n",
    "    x_num = dates.date2num(x_data)\n",
    "    breakpoints = optimize.brute(f, [slice(1, len(x_data), 1)]*num_breakpoints, args=(x_num, y_data, {}), finish=None)\n",
    "    if num_breakpoints==1:\n",
    "        breakpoints = [breakpoints] \n",
    "\n",
    "    for f, xi, yi in find_best_piecewise_polynomial(breakpoints, x_num, y_data):\n",
    "        xi_min = x_data[np.where(x_num == xi.min())[0][0]]\n",
    "        xi_max = x_data[np.where(x_num == xi.max())[0][0]]\n",
    "        x_interval = np.array([xi_min, xi_max]) \n",
    "        #print('y = {:35s}, if x in [{}, {}]'.format(str(f), *x_interval))\n",
    "        x_interval = np.array([xi.min(), xi.max()])\n",
    "        #ax.plot(x_interval, f(x_interval), 'yo-')\n",
    "        coef = f.convert().coef\n",
    "        b = coef[0]\n",
    "        a = coef[1]\n",
    "        if ff==1:\n",
    "            if b>0:\n",
    "                ll = 'y = {:.2f} x + {:.0f}'.format(a,b)\n",
    "            else: #b<0\n",
    "                ll = 'y = {:.2f} x - {:.0f}'.format(a,abs(b))    \n",
    "        else:\n",
    "            if b>0:\n",
    "                ll = 'y = {:.5f} x + {:.0f}'.format(a,b)\n",
    "            else: #b<0\n",
    "                ll = 'y = {:.5f} x - {:.0f}'.format(a,abs(b))\n",
    "        ax.plot(x_interval, f(x_interval), 'yo-',label=ll,linewidth=2, markersize=7)\n",
    "        \n",
    "    ax.legend()\n",
    "    ax.set_title(title,size=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8445973f-ca4b-4974-a794-524cb5fa31b6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = work_edges_dist_mean_monthly\n",
    "df = df[df.publication_date_1>='2000']\n",
    "df = df[df.publication_date_1<'2024']\n",
    "x_column='publication_date_1'\n",
    "x_column='dist'\n",
    "x_label='month'\n",
    "title='Monthly average mean (euclidean) distances edges works (without rolling average)'\n",
    "window_size=1\n",
    "num_breakpoints=2\n",
    "ff=1\n",
    "plot_fit_rolling_breakpoints2(df,x_column,x_label,title,window_size,num_breakpoints,ff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f493ba1d-e7d5-47f6-b091-586dc750af95",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_breakpoints=1\n",
    "plot_fit_rolling_breakpoints2(df,x_column,x_label,title,window_size,num_breakpoints,ff)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede51ed0-a8f4-4338-af28-416b67014250",
   "metadata": {},
   "source": [
    "### COVID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165f8c9f-2c6d-4a2f-9335-8aadaa30049a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#COVID\n",
    "my_file = 'preprint_id_set_COVID'\n",
    "with open(os.path.join(basepath, my_file),\"rb\") as fp:\n",
    "    preprint_id_set_COVID = pickle.load(fp)\n",
    "my_file = 'preprint_id_set_noCOVID'\n",
    "with open(os.path.join(basepath, my_file),\"rb\") as fp:\n",
    "    preprint_id_set_noCOVID = pickle.load(fp)\n",
    "works_authors_aff_COVID = works_authors_aff[works_authors_aff.work_id.isin(preprint_id_set_COVID)]\n",
    "works_authors_aff_noCOVID = works_authors_aff[works_authors_aff.work_id.isin(preprint_id_set_noCOVID)]\n",
    "\n",
    "works_authors_aff_COVID_ = works_authors_aff_COVID[['work_id','author_id']]\n",
    "works_authors_aff_noCOVID_ = works_authors_aff_noCOVID[['work_id','author_id']]\n",
    "work_authors_set_df_COVID = works_authors_aff_COVID_.groupby('work_id').author_id.apply(set).to_frame().reset_index()\n",
    "work_authors_set_df_noCOVID = works_authors_aff_noCOVID_.groupby('work_id').author_id.apply(set).to_frame().reset_index()\n",
    "\n",
    "edge_list_dfs_COVID = work_authors_set_df_COVID.author_id.apply(create_edges)\n",
    "work_authors_edges_df_COVID = pd.concat(edge_list_dfs_COVID.tolist(), ignore_index=True)\n",
    "work_authors_set_df_COVID['num_authors'] = work_authors_set_df_COVID['author_id'].apply(lambda x:len(x))\n",
    "work_authors_set_df_COVID['binom_n_authors'] = work_authors_set_df_COVID['num_authors'].map(binom_dict)\n",
    "work_authors_set_df_COVID['binom_n_authors'] = work_authors_set_df_COVID['binom_n_authors'].astype(int)\n",
    "work_id_list_COVID = work_authors_set_df_COVID['work_id']\n",
    "binom_list_COVID = work_authors_set_df_COVID['binom_n_authors']\n",
    "work_mult_binom_COVID = [[work_id_list_COVID[i]]*binom_list_COVID[i] for i in range(len(work_id_list_COVID))]\n",
    "work_mult_binom_COVID_ = list(itertools.chain(*work_mult_binom_COVID))\n",
    "work_authors_edges_df_COVID['work_id'] = work_mult_binom_COVID_\n",
    "work_authors_edges_df_COVID = work_authors_edges_df_COVID[['work_id','source','target']]\n",
    "my_file = 'work_authors_edges_df_COVID'\n",
    "work_authors_edges_df_COVID.to_parquet(os.path.join(my_path_, my_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697d923f-23f9-4040-9bdf-2688eed2bf47",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "edge_list_dfs_noCOVID = work_authors_set_df_noCOVID.author_id.apply(create_edges)\n",
    "work_authors_edges_df_noCOVID = pd.concat(edge_list_dfs_noCOVID.tolist(), ignore_index=True)\n",
    "work_authors_set_df_noCOVID['num_authors'] = work_authors_set_df_noCOVID['author_id'].apply(lambda x:len(x))\n",
    "work_authors_set_df_noCOVID['binom_n_authors'] = work_authors_set_df_noCOVID['num_authors'].map(binom_dict)\n",
    "work_authors_set_df_noCOVID['binom_n_authors'] = work_authors_set_df_noCOVID['binom_n_authors'].astype(int)\n",
    "work_id_list_noCOVID = work_authors_set_df_noCOVID['work_id']\n",
    "binom_list_noCOVID = work_authors_set_df_noCOVID['binom_n_authors']\n",
    "work_mult_binom_noCOVID = [[work_id_list_noCOVID[i]]*binom_list_noCOVID[i] for i in range(len(work_id_list_noCOVID))]\n",
    "work_mult_binom_noCOVID_ = list(itertools.chain(*work_mult_binom_noCOVID))\n",
    "work_authors_edges_df_noCOVID['work_id'] = work_mult_binom_noCOVID_\n",
    "work_authors_edges_df_noCOVID = work_authors_edges_df_noCOVID[['work_id','source','target']]\n",
    "my_file = 'work_authors_edges_df_noCOVID'\n",
    "work_authors_edges_df_noCOVID.to_parquet(os.path.join(my_path_, my_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db0bca9-b0d1-4013-857d-19a2163a7caa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "work_authors_edges_df_COVID = read_parquet(my_path_ / 'work_authors_edges_df_COVID')\n",
    "works_authors_aff_COVID['author_id'] = works_authors_aff_COVID['author_id'].astype('int64')\n",
    "work_authors_edges_df_COVID = work_authors_edges_df_COVID.merge(works_authors_aff_COVID[['work_id','author_id','institution_id']].rename(columns={'author_id':'source','institution_id':'source_inst'}),on=['work_id','source'], how='left')\n",
    "work_authors_edges_df_COVID = work_authors_edges_df_COVID.merge(works_authors_aff_COVID[['work_id','author_id','institution_id']].rename(columns={'author_id':'target','institution_id':'target_inst'}),on=['work_id','target'], how='left')\n",
    "work_authors_edges_df_COVID['source_inst'] = work_authors_edges_df_COVID['source_inst'].astype(int)\n",
    "work_authors_edges_df_COVID['target_inst'] = work_authors_edges_df_COVID['target_inst'].astype(int)\n",
    "work_authors_edges_df_COVID = work_authors_edges_df_COVID.merge(I_dist.rename(columns={'source':'source_inst','target':'target_inst'}),on=['source_inst','target_inst'], how='left')\n",
    "\n",
    "work_authors_edges_df_noCOVID = read_parquet(my_path_ / 'work_authors_edges_df_noCOVID')\n",
    "works_authors_aff_noCOVID['author_id'] = works_authors_aff_noCOVID['author_id'].astype('int64')\n",
    "work_authors_edges_df_noCOVID = work_authors_edges_df_noCOVID.merge(works_authors_aff_noCOVID[['work_id','author_id','institution_id']].rename(columns={'author_id':'source','institution_id':'source_inst'}),on=['work_id','source'], how='left')\n",
    "work_authors_edges_df_noCOVID = work_authors_edges_df_noCOVID.merge(works_authors_aff_noCOVID[['work_id','author_id','institution_id']].rename(columns={'author_id':'target','institution_id':'target_inst'}),on=['work_id','target'], how='left')\n",
    "work_authors_edges_df_noCOVID['source_inst'] = work_authors_edges_df_noCOVID['source_inst'].astype(int)\n",
    "work_authors_edges_df_noCOVID['target_inst'] = work_authors_edges_df_noCOVID['target_inst'].astype(int)\n",
    "work_authors_edges_df_noCOVID = work_authors_edges_df_noCOVID.merge(I_dist.rename(columns={'source':'source_inst','target':'target_inst'}),on=['source_inst','target_inst'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5477a34a-e141-43ca-b7c6-806d27fa0767",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "work_edges_dist_mean_COVID = work_authors_edges_df_COVID.groupby('work_id').dist.mean().to_frame().reset_index()\n",
    "work_edges_dist_mean_COVID = work_edges_dist_mean_COVID.merge(works[['work_id']].reset_index(),on='work_id')\n",
    "work_edges_dist_mean_monthly_COVID = work_edges_dist_mean_COVID.groupby('publication_date_1').dist.mean().to_frame().reset_index()\n",
    "my_file = \"work_edges_dist_mean_COVID.csv\"    \n",
    "work_edges_dist_mean_COVID.to_csv(os.path.join(my_path_, my_file),index=False)\n",
    "\n",
    "work_edges_dist_mean_noCOVID = work_authors_edges_df_noCOVID.groupby('work_id').dist.mean().to_frame().reset_index()\n",
    "work_edges_dist_mean_noCOVID = work_edges_dist_mean_noCOVID.merge(works[['work_id']].reset_index(),on='work_id')\n",
    "work_edges_dist_mean_monthly_noCOVID = work_edges_dist_mean_noCOVID.groupby('publication_date_1').dist.mean().to_frame().reset_index()\n",
    "my_file = \"work_edges_dist_mean_noCOVID.csv\"    \n",
    "work_edges_dist_mean_noCOVID.to_csv(os.path.join(my_path_, my_file),index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d11d20-e39c-4953-bf87-ed56c5abe025",
   "metadata": {},
   "source": [
    "## Power-law"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd61579f-85b7-44cf-b7e5-1af91991ddac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#all preprints - edges\n",
    "\n",
    "def my_weight(G, u, v, weight=\"weight\"):\n",
    "    w = 0\n",
    "    for nbr in set(G[u]) & set(G[v]):\n",
    "        w += (G[u][nbr].get(weight, 1) + G[v][nbr].get(weight, 1))/2\n",
    "    return w\n",
    "\n",
    "def make_institution_graph(works_authors_rows):\n",
    "    \n",
    "    institution_id_set = set(works_authors_rows.institution_id)\n",
    "                                  \n",
    "    bip_g = nx.from_pandas_edgelist(\n",
    "        works_authors_rows,\n",
    "        source='work_id', target='institution_id', edge_attr ='weight'\n",
    "    )\n",
    "\n",
    "    inst_graph = nx.bipartite.generic_weighted_projected_graph(bip_g,nodes=institution_id_set,weight_function=my_weight)    \n",
    "    #inst_graph = nx.bipartite.weighted_projected_graph(bip_g,nodes=institution_id_set) \n",
    "\n",
    "    return inst_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b9e958-992d-4f1c-9f5f-ae2a2f48fbdf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def mean_scatter_plot_multi(func,edges_df,degree_df,col,columnx,columny,labelx,labely,title,logx=False,logy=False,limity=False,geomspace=False):\n",
    "    fig, ax = plt.subplots(figsize=(8,5))\n",
    "    for y in periods_list[0:]:\n",
    "        df1 = func(edges_df,degree_df,col,y)\n",
    "        x1 = np.array(df1[columnx]) \n",
    "        y1 = np.array(df1[columny])  \n",
    "        # Define the grid\n",
    "        gridsize = 10**2\n",
    "        if geomspace:\n",
    "            xbins1 = np.geomspace(x1.min(), x1.max(), gridsize)\n",
    "            #xbins2 = np.geomspace(x2.min(), x2.max(), gridsize)\n",
    "        else:\n",
    "            xbins1 = np.linspace(x1.min(), x1.max(), gridsize)\n",
    "            #xbins2 = np.linspace(x2.min(), x2.max(), gridsize)\n",
    "        # Calculate the mean values within each column\n",
    "        mean_values1 = []\n",
    "        for i in range(len(xbins1) - 1):\n",
    "            mask = (x1 >= xbins1[i]) & (x1 < xbins1[i + 1])\n",
    "            mean_y1 = np.mean(y1[mask])\n",
    "            mean_values1.append(mean_y1) \n",
    "        print((xbins1[0] + xbins1[1]) / 2)\n",
    "        ax.plot((xbins1[:-1] + xbins1[1:]) / 2, mean_values1, '.',color=color_dict[y],markersize=5, label=periods_labels[y])          \n",
    "    ax.set_xlabel(labelx,size=20)\n",
    "    ax.set_ylabel(labely,size=20)\n",
    "    ax.set_title(title,size=30)\n",
    "    if logx==True:\n",
    "        ax.set_xscale('log')  \n",
    "    if logy==True:\n",
    "        ax.set_yscale('log')\n",
    "    if limity==True:\n",
    "        ax.set_ylim([0, 1])  \n",
    "    ax.legend(bbox_to_anchor=(1.0, 0.9), prop={'size': 15}, markerscale=3)\n",
    "    plt.show() \n",
    "def scipy_fun_multi(func1,func2,edges_df,degree_df,col,x_max,labelx,labely,title):\n",
    "    fig, ax = plt.subplots(figsize=(8,5))  \n",
    "    for y in periods_list[0:]:\n",
    "        x1,y1 = func1(func2,edges_df,degree_df,col,y)\n",
    "        y1 = np.array(y1)\n",
    "        popt1, pcov1 = scipy.optimize.curve_fit(linear, x1[x1<=x_max], y1[x1<=x_max])\n",
    "        perr1 = np.sqrt(np.diag(pcov1))\n",
    "        print(f'{periods_labels[y]}: gamma {popt1[0]} (perr {perr1[0]:.4f})') \n",
    "        ax.scatter(x1[x1<=x_max], y1[x1<=x_max],marker='.',color=color_dict[y],linewidths=0.01, label=periods_labels[y]) \n",
    "        ax.plot(x1[x1<=x_max], linear(x1[x1<=x_max], *popt1),color=color_dict[y],linewidth=2.0)   \n",
    "    ax.set_xlabel(labelx,size=20)\n",
    "    ax.set_ylabel(labely,size=20)\n",
    "    ax.legend(bbox_to_anchor=(1.0, 0.9), prop={'size': 15}, markerscale=3)\n",
    "    ax.set_title(title+\" - linear\",size=30)\n",
    "    plt.show()\n",
    "def linear(x, b):\n",
    "    return  b * x \n",
    "def linear_fit(func,edges_df,degree_df,col,yy):\n",
    "    df1 = func(edges_df,degree_df,col,yy)\n",
    "    x1 = np.array(df1[\"s\"]) \n",
    "    y1 = np.array(df1[col])  \n",
    "    gridsize = 10**2\n",
    "    xbins1 = np.linspace(x1.min(), x1.max(), gridsize)    \n",
    "    mean_values1 = []\n",
    "    for i in range(len(xbins1) - 1):\n",
    "        mask = (x1 >= xbins1[i]) & (x1 < xbins1[i + 1])\n",
    "        mean_y1 = np.mean(y1[mask])\n",
    "        mean_values1.append(mean_y1)     \n",
    "    x = (xbins1[:-1] + xbins1[1:]) / 2\n",
    "    y = np.array(mean_values1)\n",
    "    x = x[~np.isnan(y)]\n",
    "    y = y[~np.isnan(y)]\n",
    "    return x,y\n",
    "def fig_2B(edges_df,degree_df,col,y):\n",
    "    edges_year_df = edges_df.query('period == @y')\n",
    "    degree_year_df = degree_df.query('period == @y')\n",
    "    loops_w_count = edges_year_df[edges_year_df.source == edges_year_df.target][['source',col]].rename(columns={'source':'institution_id'})\n",
    "    loops_w_count = loops_w_count.merge(degree_year_df,on='institution_id')\n",
    "    return loops_w_count\n",
    "def fig_4B(df1,df2,col,y):\n",
    "    df1 = df1.query('period == @y')[['source','target',col,'dist']]\n",
    "    df2 = df2.query('period == @y')[['institution_id','s']]\n",
    "    df2_dict = df2.set_index('institution_id').to_dict()['s']\n",
    "    df1['source_s'] = df1['source'].map(df2_dict)\n",
    "    df1['target_s'] = df1['target'].map(df2_dict)\n",
    "    df1['ratio'] = df1[col] / (df1['source_s']*df1['target_s'])**(1/2)\n",
    "    df1 = df1[[\"dist\", \"ratio\"]]\n",
    "    #delate zero\n",
    "    df1 = df1[df1.dist>0]\n",
    "    return df1\n",
    "def fig_4B_prods(df1,df2,col,y):\n",
    "    df1 = df1.query('period == @y')[['source','target',col,'dist']]\n",
    "    df2 = df2.query('period == @y')[['institution_id','s']]\n",
    "    df2_dict = df2.set_index('institution_id').to_dict()['s']\n",
    "    df1['source_s'] = df1['source'].map(df2_dict)\n",
    "    df1['target_s'] = df1['target'].map(df2_dict)\n",
    "    df1['ratio'] = df1[col] / (df1['source_s']*df1['target_s'])\n",
    "    df1 = df1[[\"dist\", \"ratio\"]]\n",
    "    #delate zero\n",
    "    df1 = df1[df1.dist>0]\n",
    "    return df1\n",
    "def scipy_fun_multi2(func1,func2,edges_df,degree_df,col,x_min,labelx,labely,title):\n",
    "    fig, ax = plt.subplots(figsize=(8,5))  \n",
    "    for y in periods_list[0:]:\n",
    "        x1,y1 = func1(func2,edges_df,degree_df,col,y)\n",
    "        y1 = np.array(y1)\n",
    "        popt1, pcov1 = scipy.optimize.curve_fit(power2, x1[x1>=x_min], y1[x1>=x_min])\n",
    "        perr1 = np.sqrt(np.diag(pcov1))\n",
    "        print(f'{periods_labels[y]}: alpha {(-1)*popt1[1]:.3f} (perr {perr1[1]:.4f}), beta {popt1[0]:.6f} (perr {perr1[0]:.4f})') \n",
    "        ax.scatter(x1[x1>=x_min], y1[x1>=x_min],marker='.',color=color_dict[y],linewidths=0.01, label=periods_labels[y]) \n",
    "        ax.plot(x1[x1>=x_min], power2(x1[x1>=x_min], *popt1),color=color_dict[y],linewidth=2.0)   \n",
    "    ax.set_xscale('log')  \n",
    "    ax.set_yscale('log')\n",
    "    ax.set_xlabel(labelx,size=20)\n",
    "    ax.set_ylabel(labely,size=20)\n",
    "    ax.legend(bbox_to_anchor=(1.0, 0.9), prop={'size': 15}, markerscale=3)\n",
    "    ax.set_title(title+\" - power law\",size=30)\n",
    "    plt.show()\n",
    "def power2(x, b, c):\n",
    "    return  b * x ** c \n",
    "def powerlaw_fit(func,edges_df,degree_df,col,yy):\n",
    "    df = func(edges_df,degree_df,col,yy)\n",
    "    x = np.array(df[\"dist\"]) \n",
    "    y = np.array(df[\"ratio\"])\n",
    "    gridsize = 10**2\n",
    "    xbins = np.geomspace(x.min(), x.max(), gridsize)\n",
    "    # Calculate the mean values within each column\n",
    "    mean_values = []\n",
    "    for i in range(len(xbins) - 1):\n",
    "        mask = (x >= xbins[i]) & (x < xbins[i + 1])\n",
    "        mean_y = np.mean(y[mask])\n",
    "        mean_values.append(mean_y) \n",
    "    x = (xbins[:-1] + xbins[1:]) / 2\n",
    "    y = np.array(mean_values)\n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151b0152-ca34-4d90-ba33-c9c24ffc5737",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#all period\n",
    "\n",
    "works = read_parquet(basepath / 'works')\n",
    "works = works.reset_index()\n",
    "works['publication_year'] = works['publication_date_1'].dt.year\n",
    "works_yearly_count = works.groupby('publication_year').work_id.count().to_frame()#.reset_index()\n",
    "works = works.set_index('publication_year')\n",
    " \n",
    "#works_authors_aff = read_parquet(basepath3 / 'works_authorships_fill')\n",
    "works_authors_aff = read_parquet(basepath / 'works_authors_aff')\n",
    "works_authors_aff = works_authors_aff.reset_index()\n",
    "works_authors_aff['publication_year'] = works_authors_aff['publication_date_1'].dt.year\n",
    "works_authors_aff['publication_year'] = works_authors_aff['publication_year'].astype('int64')\n",
    "works_authors_aff = works_authors_aff.drop_duplicates(['work_id','institution_id']).reset_index()\n",
    "works_authors_aff = works_authors_aff[['publication_year','work_id','institution_id']]\n",
    "works_authors_aff = works_authors_aff.sort_values(by='publication_year')\n",
    "works_authors_aff['institution_id'] = works_authors_aff['institution_id'].astype(int)\n",
    "\n",
    "I_dist = read_parquet(basepath / 'I_dist_threshold')\n",
    "I_dist1 = I_dist\n",
    "I_dist2 = I_dist[I_dist.source!=I_dist.target]\n",
    "I_dist[['target','source']] = I_dist[['source','target']]\n",
    "I_dist = pd.concat([I_dist1,I_dist2])\n",
    "\n",
    "my_file = \"inst_id_name_dict.pickle\" #dictionary inst_id and inst_name\n",
    "with open(os.path.join(Path('./Tables'), my_file),\"rb\") as fp:\n",
    "    inst_id_name_dict = pickle.load(fp)\n",
    "    \n",
    "#count number (unique) institutions per paper\n",
    "works_authors_aff['num_affs'] = works_authors_aff.groupby('work_id')['institution_id'].transform('size')\n",
    "print(f'{min(works_authors_aff.num_affs)}-{max(works_authors_aff.num_affs)} min-max number (unique) affiliations per work')\n",
    "works_authors_aff['weight'] = 2 / ( works_authors_aff['num_affs']*(works_authors_aff['num_affs']-1) ) \n",
    "works_authors_aff.loc[works_authors_aff.num_affs==1,'weight'] = 1 #one affiliation\n",
    "\n",
    "years_list = list(set(works_authors_aff.publication_year))\n",
    "years_list.sort()\n",
    "\n",
    "works_authors_aff_noloops = works_authors_aff[works_authors_aff.num_affs>1]\n",
    "works_authors_aff_loops = works_authors_aff[works_authors_aff.num_affs==1]\n",
    "works_authors_aff_loops['institution_id2'] = works_authors_aff_loops['institution_id']\n",
    "works_authors_aff_noloops = works_authors_aff_noloops.set_index('publication_year')\n",
    "works_authors_aff_loops = works_authors_aff_loops.set_index('publication_year')\n",
    "\n",
    "works_yearly_count = works_yearly_count.loc[2000:2024]\n",
    "print(f\"Total number preprints (from {min(works_yearly_count.index)} to {max(works_yearly_count.index)}): {works_yearly_count.work_id.sum()}\")\n",
    "\n",
    "noloops_df = works_authors_aff_noloops\n",
    "loops_df = works_authors_aff_loops\n",
    "loops_df['weight'] = loops_df[['institution_id','institution_id2','weight']].groupby(['institution_id']).weight.transform('sum')\n",
    "loops_df = loops_df.drop_duplicates('institution_id')\n",
    "I_year = make_institution_graph(noloops_df)\n",
    "I_year.add_weighted_edges_from([tuple(r) for r in loops_df[['institution_id','institution_id2','weight']].to_numpy()])\n",
    "Iloops_year = nx.Graph()\n",
    "Iloops_year.add_weighted_edges_from([tuple(r) for r in loops_df[['institution_id','institution_id2','weight']].to_numpy()])\n",
    "\n",
    "\n",
    "info_df = pd.DataFrame.from_dict({'period':[0], 'N': [I_year.number_of_nodes()], 'E': [I_year.size()], 'W' : [I_year.size(weight='weight')], 'Nloops': [Iloops_year.number_of_nodes()], 'Eloops': [Iloops_year.size()], 'Wloops' : [Iloops_year.size(weight='weight')]})  \n",
    "degree = {'k': I_year.degree(), 's' : I_year.degree(weight='weight'),'kloops': Iloops_year.degree(), 'sloops' : Iloops_year.degree(weight='weight')}\n",
    "\n",
    "dict_y = degree\n",
    "df_k = pd.DataFrame.from_dict(dict_y['k']).rename(columns={0:'institution_id',1:'k'})\n",
    "df_s = pd.DataFrame.from_dict(dict_y['s']).rename(columns={0:'institution_id',1:'s'})\n",
    "df_kloops = pd.DataFrame.from_dict(dict_y['kloops']).rename(columns={0:'institution_id',1:'kloops'})\n",
    "df_sloops = pd.DataFrame.from_dict(dict_y['sloops']).rename(columns={0:'institution_id',1:'sloops'})\n",
    "if len(df_kloops)!=0:\n",
    "    df_year = (df_k.merge(df_s,on='institution_id')).merge(df_kloops.merge(df_sloops,on='institution_id'),on='institution_id',how='left')\n",
    "else:\n",
    "    df_year = df_k.merge(df_s,on='institution_id')\n",
    "    df_year[\"kloops\"] = np.nan\n",
    "    df_year[\"sloops\"] = np.nan\n",
    "degree_df = df_year\n",
    "my_file = \"degree_df_whole.csv\"    \n",
    "degree_df.to_csv(os.path.join(my_path_, my_file),index=False)  \n",
    "\n",
    "I_year_df = nx.to_pandas_edgelist(I_year)\n",
    "I_year_df = I_year_df.merge(I_dist,on=['source','target'],how='left')\n",
    "edges_df = I_year_df\n",
    "my_file = \"edges_df_whole.csv\"    \n",
    "edges_df.to_csv(os.path.join(my_path_, my_file),index=False)   \n",
    "\n",
    "I_comp = nx.complement(I_year) \n",
    "I_comp_df = nx.to_pandas_edgelist(I_comp)\n",
    "I_comp_df = I_comp_df.merge(I_dist,on=['source','target'])\n",
    "edges_comp_df = I_comp_df\n",
    "my_file = \"edges_comp_df_whole.csv\"    \n",
    "edges_comp_df.to_csv(os.path.join(my_path_, my_file),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e677988-6a3b-4d88-add8-40a8e254f7b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#plot\n",
    "edges_df = pd.read_csv(my_path_ / 'edges_df_whole.csv')  \n",
    "degree_df = pd.read_csv(my_path_ / 'degree_df_whole.csv')  \n",
    "degree_df.fillna(0, inplace=True)\n",
    "edges_comp_df = pd.read_csv(my_path_ / 'edges_comp_df_whole.csv') \n",
    "edges_comp_df['weight'] = 0\n",
    "edges_df_tot = pd.concat([edges_df,edges_comp_df])\n",
    "edges_df.insert(0, 'period', 0)\n",
    "edges_df_tot.insert(0, 'period', 0)\n",
    "degree_df.insert(0, 'period', 0)\n",
    "edges_comp_df.insert(0, 'period', 0)\n",
    "periods_list = [0]\n",
    "periods_labels = ['[2000,2024]']\n",
    "color_dict = {0: 'green'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a56b30-7b92-40d9-b4d4-59e3c43d7cad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mean_scatter_plot_multi(fig_2B,edges_df_tot,degree_df,\"weight\",\"s\",\"weight\",\"s_i\",\"w_ii\",'Fig. 2B',False,False,False,False)\n",
    "scipy_fun_multi(linear_fit,fig_2B,edges_df_tot,degree_df,\"weight\",2*10**4,\"s_i\",\"w_ii\",\"Fig. 2B\")\n",
    "mean_scatter_plot_multi(fig_4B,edges_df_tot,degree_df,\"weight\",\"dist\", \"ratio\",\"d_ij\",\"w_ij / (s_i*s_j)^(1/2) \",'Fig. 4B - sqrt prod. strengths',True,True,False,True)\n",
    "scipy_fun_multi2(powerlaw_fit,fig_4B,edges_df_tot,degree_df,\"weight\",10**1,\"d_ij\",\"w_ij / (s_i*s_j)^(1/2) \",\"Fig. 4B - power law - sqrt prod. strengths\")\n",
    "scipy_fun_multi2(powerlaw_fit,fig_4B,edges_df_tot,degree_df,\"weight\",10**2,\"d_ij\",\"w_ij / (s_i*s_j)^(1/2) \",\"Fig. 4B - power law - sqrt prod. strengths\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb623a8-b942-4f78-85a7-ea8dde9803a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mean_scatter_plot_multi(fig_4B_prods,edges_df_tot,degree_df,\"weight\",\"dist\", \"ratio\",\"d_ij\",\"w_ij / (s_i*s_j) \",'Fig. 4B - prod. strengths',True,True,False,True)\n",
    "scipy_fun_multi2(powerlaw_fit,fig_4B_prods,edges_df_tot,degree_df,\"weight\",10**1,\"d_ij\",\"w_ij / (s_i*s_j) \",\"Fig. 4B - power law - prod. strengths\")\n",
    "scipy_fun_multi2(powerlaw_fit,fig_4B_prods,edges_df_tot,degree_df,\"weight\",10**2,\"d_ij\",\"w_ij / (s_i*s_j) \",\"Fig. 4B - power law - prod. strengths\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8843c6c3-7317-4476-8e67-a4c9b87d9dfe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89cfebd4-fa6b-475a-b252-5226723fc1a6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#divided into periods\n",
    "\n",
    "works = read_parquet(basepath / 'works')\n",
    "works = works.reset_index()\n",
    "works['publication_year'] = works['publication_date_1'].dt.year\n",
    "works_yearly_count = works.groupby('publication_year').work_id.count().to_frame()#.reset_index()\n",
    "works = works.set_index('publication_year')\n",
    "\n",
    "works_authors_aff = read_parquet(basepath  / 'works_authors_aff')\n",
    "works_authors_aff = works_authors_aff.reset_index()\n",
    "works_authors_aff['publication_year'] = works_authors_aff['publication_date_1'].dt.year\n",
    "works_authors_aff['publication_year'] = works_authors_aff['publication_year'].astype('int64')\n",
    "works_authors_aff = works_authors_aff.drop_duplicates(['work_id','institution_id']).reset_index()\n",
    "works_authors_aff = works_authors_aff[['publication_year','work_id','institution_id']]\n",
    "works_authors_aff = works_authors_aff.sort_values(by='publication_year')\n",
    "works_authors_aff['institution_id'] = works_authors_aff['institution_id'].astype(int)\n",
    "\n",
    "I_dist = read_parquet(basepath / 'I_dist_threshold')\n",
    "I_dist1 = I_dist\n",
    "I_dist2 = I_dist[I_dist.source!=I_dist.target]\n",
    "I_dist[['target','source']] = I_dist[['source','target']]\n",
    "I_dist = pd.concat([I_dist1,I_dist2])\n",
    "\n",
    "my_file = \"inst_id_name_dict.pickle\" #dictionary inst_id and inst_name\n",
    "with open(os.path.join(Path('./Tables'), my_file),\"rb\") as fp:\n",
    "    inst_id_name_dict = pickle.load(fp)\n",
    "    \n",
    "#count number (unique) institutions per paper\n",
    "works_authors_aff['num_affs'] = works_authors_aff.groupby('work_id')['institution_id'].transform('size')\n",
    "print(f'{min(works_authors_aff.num_affs)}-{max(works_authors_aff.num_affs)} min-max number (unique) affiliations per work')\n",
    "works_authors_aff['weight'] = 2 / ( works_authors_aff['num_affs']*(works_authors_aff['num_affs']-1) ) \n",
    "works_authors_aff.loc[works_authors_aff.num_affs==1,'weight'] = 1 #one affiliation\n",
    "\n",
    "years_list = list(set(works_authors_aff.publication_year))\n",
    "years_list.sort()\n",
    "\n",
    "works_authors_aff_noloops = works_authors_aff[works_authors_aff.num_affs>1]\n",
    "works_authors_aff_loops = works_authors_aff[works_authors_aff.num_affs==1]\n",
    "works_authors_aff_loops['institution_id2'] = works_authors_aff_loops['institution_id']\n",
    "works_authors_aff_noloops = works_authors_aff_noloops.set_index('publication_year')\n",
    "works_authors_aff_loops = works_authors_aff_loops.set_index('publication_year')\n",
    "\n",
    "works_yearly_count = works_yearly_count.loc[2000:2024]\n",
    "print(f\"Total number preprints (from {min(works_yearly_count.index)} to {max(works_yearly_count.index)}): {works_yearly_count.work_id.sum()}\")\n",
    "\n",
    "start_year_0 = 2000\n",
    "end_year_0 = 2009\n",
    "works_yearly_count_0 = works_yearly_count.loc[start_year_0:end_year_0]\n",
    "print(f\"Number preprints (from {start_year_0} to {end_year_0}): {works_yearly_count_0.work_id.sum()}\")\n",
    "start_year_1 = 2010\n",
    "end_year_1 = 2014\n",
    "works_yearly_count_1 = works_yearly_count.loc[start_year_1:end_year_1]\n",
    "print(f\"Number preprints (from {start_year_1} to {end_year_1}): {works_yearly_count_1.work_id.sum()}\")\n",
    "start_year_2 = 2015\n",
    "end_year_2 = 2019\n",
    "works_yearly_count_2 = works_yearly_count.loc[start_year_2:end_year_2]\n",
    "print(f\"Number preprints (from {start_year_2} to {end_year_2}): {works_yearly_count_2.work_id.sum()}\")\n",
    "start_year_3 = 2020\n",
    "end_year_3 = 2020\n",
    "works_yearly_count_3 = works_yearly_count.loc[start_year_3:end_year_3]\n",
    "print(f\"Number preprints (from {start_year_3} to {end_year_3}): {works_yearly_count_3.work_id.sum()}\")\n",
    "start_year_4 = 2021\n",
    "end_year_4 = 2024\n",
    "works_yearly_count_4 = works_yearly_count.loc[start_year_4:end_year_4]\n",
    "print(f\"Number preprints (from {start_year_4} to {end_year_4}): {works_yearly_count_4.work_id.sum()}\")\n",
    "\n",
    "works_0 = works.loc[start_year_0:end_year_0]\n",
    "works_set_0 = set(works_0.work_id)\n",
    "works_1 = works.loc[start_year_1:end_year_1]\n",
    "works_set_1 = set(works_1.work_id)\n",
    "works_2 = works.loc[start_year_2:end_year_2]\n",
    "works_set_2 = set(works_2.work_id)\n",
    "works_3 = works.loc[start_year_3:end_year_3]\n",
    "works_set_3 = set(works_3.work_id)\n",
    "works_4 = works.loc[start_year_4:end_year_4]\n",
    "works_set_4 = set(works_4.work_id)\n",
    "\n",
    "works_authors_aff_noloops_0 = works_authors_aff_noloops.loc[start_year_0:end_year_0]\n",
    "works_authors_aff_loops_0 = works_authors_aff_loops.loc[start_year_0:end_year_0]\n",
    "works_authors_aff_noloops_1 = works_authors_aff_noloops.loc[start_year_1:end_year_1]\n",
    "works_authors_aff_loops_1 = works_authors_aff_loops.loc[start_year_1:end_year_1]\n",
    "works_authors_aff_noloops_2 = works_authors_aff_noloops.loc[start_year_2:end_year_2]\n",
    "works_authors_aff_loops_2 = works_authors_aff_loops.loc[start_year_2:end_year_2]\n",
    "works_authors_aff_noloops_3 = works_authors_aff_noloops.loc[start_year_3:end_year_3]\n",
    "works_authors_aff_loops_3 = works_authors_aff_loops.loc[start_year_3:end_year_3]\n",
    "works_authors_aff_noloops_4 = works_authors_aff_noloops.loc[start_year_4:end_year_4]\n",
    "works_authors_aff_loops_4 = works_authors_aff_loops.loc[start_year_4:end_year_4]\n",
    "\n",
    "works_authors_aff_noloops_0['period'] = 0\n",
    "works_authors_aff_noloops_1['period'] = 1\n",
    "works_authors_aff_noloops_2['period'] = 2\n",
    "works_authors_aff_noloops_3['period'] = 3\n",
    "works_authors_aff_noloops_4['period'] = 4\n",
    "works_authors_aff_loops_0['period'] = 0\n",
    "works_authors_aff_loops_1['period'] = 1\n",
    "works_authors_aff_loops_2['period'] = 2\n",
    "works_authors_aff_loops_3['period'] = 3\n",
    "works_authors_aff_loops_4['period'] = 4\n",
    "works_authors_aff_noloops_periods = pd.concat([works_authors_aff_noloops_0,works_authors_aff_noloops_1])\n",
    "works_authors_aff_noloops_periods = pd.concat([works_authors_aff_noloops_periods,works_authors_aff_noloops_2])\n",
    "works_authors_aff_noloops_periods = pd.concat([works_authors_aff_noloops_periods,works_authors_aff_noloops_3])\n",
    "works_authors_aff_noloops_periods = pd.concat([works_authors_aff_noloops_periods,works_authors_aff_noloops_4])\n",
    "works_authors_aff_loops_periods = pd.concat([works_authors_aff_loops_0,works_authors_aff_loops_1])\n",
    "works_authors_aff_loops_periods = pd.concat([works_authors_aff_loops_periods,works_authors_aff_loops_2])\n",
    "works_authors_aff_loops_periods = pd.concat([works_authors_aff_loops_periods,works_authors_aff_loops_3])\n",
    "works_authors_aff_loops_periods = pd.concat([works_authors_aff_loops_periods,works_authors_aff_loops_4])\n",
    "\n",
    "for y in tqdm(range(5)):\n",
    "    noloops_df = works_authors_aff_noloops_periods.query(\"period == @y\")\n",
    "    loops_df = works_authors_aff_loops_periods.query(\"period == @y\")\n",
    "    loops_df['weight'] = loops_df[['institution_id','institution_id2','weight']].groupby(['institution_id']).weight.transform('sum')\n",
    "    loops_df = loops_df.drop_duplicates('institution_id')\n",
    "    I = make_institution_graph(noloops_df)\n",
    "    I.add_weighted_edges_from([tuple(r) for r in loops_df[['institution_id','institution_id2','weight']].to_numpy()])\n",
    "    my_file = \"I_period\"+str(y)+\".pickle\"\n",
    "    pickle.dump(I, open(os.path.join(my_path_, my_file), 'wb'))  \n",
    "    #only loops\n",
    "    I_loops = nx.Graph()\n",
    "    I_loops.add_weighted_edges_from([tuple(r) for r in loops_df[['institution_id','institution_id2','weight']].to_numpy()])\n",
    "    my_file = \"Iloops_period\"+str(y)+\".pickle\"\n",
    "    pickle.dump(I_loops, open(os.path.join(my_path_, my_file), 'wb'))\n",
    "    \n",
    "info_df = pd.DataFrame()\n",
    "degree = {}\n",
    "for y in tqdm(range(5)):\n",
    "    my_file = \"I_period\"+str(y)+\".pickle\"\n",
    "    with open(os.path.join(my_path_, my_file),\"rb\") as fp:\n",
    "        I_year = pickle.load(fp)\n",
    "    my_file = \"Iloops_period\"+str(y)+\".pickle\"\n",
    "    with open(os.path.join(my_path_, my_file),\"rb\") as fp:\n",
    "        Iloops_year = pickle.load(fp)\n",
    "    info_y = pd.DataFrame.from_dict({'period':[y], 'N': [I_year.number_of_nodes()], 'E': [I_year.size()], 'W' : [I_year.size(weight='weight')], 'Nloops': [Iloops_year.number_of_nodes()], 'Eloops': [Iloops_year.size()], 'Wloops' : [Iloops_year.size(weight='weight')]})\n",
    "    info_df = pd.concat([info_df,info_y])    \n",
    "    degree[y] = {'k': I_year.degree(), 's' : I_year.degree(weight='weight'),'kloops': Iloops_year.degree(), 'sloops' : Iloops_year.degree(weight='weight')}\n",
    "my_file = \"info_period_df.csv\"\n",
    "info_df.to_csv(os.path.join(my_path_, my_file),index=False)    \n",
    "my_file = \"degree_period.pickle\"\n",
    "pickle.dump(degree, open(os.path.join(my_path_, my_file), 'wb'))\n",
    "my_file = \"info_period_df.csv\"\n",
    "info_period_df = pd.read_csv(os.path.join(my_path_, my_file))\n",
    "display(info_df)\n",
    "\n",
    "degree_df = pd.DataFrame()\n",
    "for y in tqdm(range(5)):\n",
    "    dict_y = degree[y]\n",
    "    df_k = pd.DataFrame.from_dict(dict_y['k']).rename(columns={0:'institution_id',1:'k'})\n",
    "    df_s = pd.DataFrame.from_dict(dict_y['s']).rename(columns={0:'institution_id',1:'s'})\n",
    "    df_kloops = pd.DataFrame.from_dict(dict_y['kloops']).rename(columns={0:'institution_id',1:'kloops'})\n",
    "    df_sloops = pd.DataFrame.from_dict(dict_y['sloops']).rename(columns={0:'institution_id',1:'sloops'})\n",
    "    if len(df_kloops)!=0:\n",
    "        df_year = (df_k.merge(df_s,on='institution_id')).merge(df_kloops.merge(df_sloops,on='institution_id'),on='institution_id',how='left')\n",
    "    else:\n",
    "        df_year = df_k.merge(df_s,on='institution_id')\n",
    "        df_year[\"kloops\"] = np.nan\n",
    "        df_year[\"sloops\"] = np.nan\n",
    "    df_year.insert(0, 'period', y)\n",
    "    degree_df = pd.concat([degree_df,df_year])\n",
    "degree_df = degree_df.fillna(0)\n",
    "my_file = \"degree_df.csv\"    \n",
    "degree_df.to_csv(os.path.join(my_path_, my_file),index=False) \n",
    "my_file = \"degree_df.csv\"\n",
    "degree_df = pd.read_csv(os.path.join(my_path_, my_file))\n",
    "display(degree_df)\n",
    "\n",
    "edges_df = pd.DataFrame()\n",
    "for y in tqdm(range(5)):\n",
    "    my_file = \"I_period\"+str(y)+\".pickle\"\n",
    "    with open(os.path.join(my_path_, my_file),\"rb\") as fp:\n",
    "        I_year = pickle.load(fp)\n",
    "    I_year_df = nx.to_pandas_edgelist(I_year)\n",
    "    I_year_df = I_year_df.merge(I_dist,on=['source','target'],how='left')\n",
    "    I_year_df.insert(0, 'period', y)\n",
    "    edges_df = pd.concat([edges_df,I_year_df])\n",
    "my_file = \"edges_df.csv\"    \n",
    "edges_df.to_csv(os.path.join(my_path_, my_file),index=False) \n",
    "my_file = \"edges_df.csv\"\n",
    "edges_df = pd.read_csv(os.path.join(my_path_, my_file))\n",
    "display(edges_df)\n",
    "\n",
    "edges_comp_df = pd.DataFrame()\n",
    "for y in tqdm(range(5)):\n",
    "    my_file = \"I_period\"+str(y)+\".pickle\" \n",
    "    with open(os.path.join(my_path_, my_file),\"rb\") as fp:\n",
    "        I = pickle.load(fp)\n",
    "    I_comp = nx.complement(I) #20 mins\n",
    "    # my_file = \"I_period\"+str(y)+\"_comp.pickle\"\n",
    "    # pickle.dump(I_comp, open(os.path.join(my_path_, my_file), 'wb')) \n",
    "    I_comp_df = nx.to_pandas_edgelist(I_comp)\n",
    "    I_comp_df = I_comp_df.merge(I_dist,on=['source','target'])\n",
    "    I_comp_df.insert(0, 'period', y)\n",
    "    edges_comp_df = pd.concat([edges_comp_df,I_comp_df])    \n",
    "my_file = \"edges_comp_df.csv\"    \n",
    "edges_comp_df.to_csv(os.path.join(my_path_, my_file),index=False)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9aec077-a83a-46ce-af94-b160742e69b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "periods_list = [0,1,2,3,4]\n",
    "periods_labels = ['[2000,2009]','[2010,2014]','[2015,2019]','[2020,2020]','[2021,2024]']\n",
    "color_dict = {0: 'yellow', 1: 'magenta', 2: 'orange', 3: 'red', 4: 'cyan'}\n",
    "\n",
    "#strengths: (a) during the period (b) till that period (included) (c) till that period (not included)\n",
    "degree_df = pd.read_csv(my_path_ / 'degree_df.csv')\n",
    "degree_df_a = degree_df\n",
    "degree_df_b = pd.DataFrame()\n",
    "for y in periods_list:\n",
    "    df_y = (degree_df[degree_df.period<=y][['institution_id','s','sloops']]).groupby(['institution_id']).sum().reset_index()\n",
    "    df_y.insert(0,'period',y)\n",
    "    degree_df_b = pd.concat([degree_df_b,df_y])\n",
    "degree_df_c = pd.DataFrame()\n",
    "for y in periods_list:\n",
    "    df_y = (degree_df[degree_df.period<y][['institution_id','s','sloops']]).groupby(['institution_id']).sum().reset_index()\n",
    "    df_y.insert(0,'period',y)\n",
    "    degree_df_c = pd.concat([degree_df_c,df_y])\n",
    "    \n",
    "edges_df = pd.read_csv(my_path_ / 'edges_df.csv')  \n",
    "edges_comp_df = pd.read_csv(my_path_ / 'edges_comp_df.csv') \n",
    "edges_comp_df['weight'] = 0\n",
    "edges_df_tot = pd.concat([edges_df,edges_comp_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a93dd4-6595-435b-95ac-dad2e5cfaab9",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "mean_scatter_plot_multi(fig_2B,edges_df_tot,degree_df,\"weight\",\"s\",\"weight\",\"s_i\",\"w_ii\",'Fig. 2B',False,False,False,False)\n",
    "scipy_fun_multi(linear_fit,fig_2B,edges_df_tot,degree_df,\"weight\",2*10**4,\"s_i\",\"w_ii\",\"Fig. 2B\")\n",
    "mean_scatter_plot_multi(fig_4B,edges_df_tot,degree_df,\"weight\",\"dist\", \"ratio\",\"d_ij\",\"w_ij / (s_i*s_j)^(1/2) \",'Fig. 4B',True,True,False,True)\n",
    "scipy_fun_multi2(powerlaw_fit,fig_4B,edges_df_tot,degree_df,\"weight\",10**1,\"d_ij\",\"w_ij / (s_i*s_j)^(1/2) \",\"Fig. 4B - power law\")\n",
    "scipy_fun_multi2(powerlaw_fit,fig_4B,edges_df_tot,degree_df,\"weight\",10**2,\"d_ij\",\"w_ij / (s_i*s_j)^(1/2) \",\"Fig. 4B - power law\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4a786a-bf9c-40e7-bc36-64cf8fb3b9ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d455e89-945c-4543-946f-1f04ac902bb2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f7ac1cfa-22e4-4f0f-a6ea-2ec7371f8c24",
   "metadata": {},
   "source": [
    "## Inter-intra institutions collaborations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f06a8e-f637-4170-8c08-b41f63cf480c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "works = read_parquet(basepath / 'works')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b097d08-3d9e-453a-b205-5745569ac24d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "work_authors_edges_df = read_parquet(my_path_ / 'work_authors_edges_df_dist')\n",
    "work_authors_edges_df = works[['work_id']].reset_index().merge(work_authors_edges_df,on='work_id')\n",
    "df1 = work_authors_edges_df.groupby('publication_date_1').intra.count().reset_index().rename(columns={'intra':'total'})\n",
    "df2 = work_authors_edges_df.groupby('publication_date_1').intra.sum().to_frame().reset_index()\n",
    "intra_inter_df = df1.merge(df2,on='publication_date_1')\n",
    "intra_inter_df['inter'] = intra_inter_df['total'] - intra_inter_df['intra']\n",
    "intra_inter_df['frac_intra'] = intra_inter_df['intra'] / intra_inter_df['total']\n",
    "intra_inter_df['frac_inter'] = intra_inter_df['inter'] / intra_inter_df['total']\n",
    "my_file = \"intra_inter_df.csv\"    \n",
    "intra_inter_df.to_csv(os.path.join(my_path_, my_file),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38a7f12-2f51-4fca-9d67-f0bd6e87e2e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "my_file = 'preprint_id_set_COVID'\n",
    "with open(os.path.join(basepath, my_file),\"rb\") as fp:\n",
    "    preprint_id_set_COVID = pickle.load(fp)\n",
    "my_file = 'preprint_id_set_noCOVID'\n",
    "with open(os.path.join(basepath, my_file),\"rb\") as fp:\n",
    "    preprint_id_set_noCOVID = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bbb3060-8516-406e-b7f1-ebdf91360ab0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "work_authors_edges_df = read_parquet(my_path_ / 'work_authors_edges_df_dist')\n",
    "work_authors_edges_df = work_authors_edges_df[work_authors_edges_df.work_id.isin(preprint_id_set_COVID)]\n",
    "work_authors_edges_df = works[['work_id']].reset_index().merge(work_authors_edges_df,on='work_id')\n",
    "df1 = work_authors_edges_df.groupby('publication_date_1').intra.count().reset_index().rename(columns={'intra':'total'})\n",
    "df2 = work_authors_edges_df.groupby('publication_date_1').intra.sum().to_frame().reset_index()\n",
    "intra_inter_df = df1.merge(df2,on='publication_date_1')\n",
    "intra_inter_df['inter'] = intra_inter_df['total'] - intra_inter_df['intra']\n",
    "intra_inter_df['frac_intra'] = intra_inter_df['intra'] / intra_inter_df['total']\n",
    "intra_inter_df['frac_inter'] = intra_inter_df['inter'] / intra_inter_df['total']\n",
    "my_file = \"intra_inter_df_COVID.csv\"    \n",
    "intra_inter_df.to_csv(os.path.join(my_path_, my_file),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7436ab9c-7c4d-43cf-a1eb-f7f7f9f2867a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "work_authors_edges_df = read_parquet(my_path_ / 'work_authors_edges_df_dist')\n",
    "work_authors_edges_df = work_authors_edges_df[work_authors_edges_df.work_id.isin(preprint_id_set_noCOVID)]\n",
    "work_authors_edges_df = works[['work_id']].reset_index().merge(work_authors_edges_df,on='work_id')\n",
    "df1 = work_authors_edges_df.groupby('publication_date_1').intra.count().reset_index().rename(columns={'intra':'total'})\n",
    "df2 = work_authors_edges_df.groupby('publication_date_1').intra.sum().to_frame().reset_index()\n",
    "intra_inter_df = df1.merge(df2,on='publication_date_1')\n",
    "intra_inter_df['inter'] = intra_inter_df['total'] - intra_inter_df['intra']\n",
    "intra_inter_df['frac_intra'] = intra_inter_df['intra'] / intra_inter_df['total']\n",
    "intra_inter_df['frac_inter'] = intra_inter_df['inter'] / intra_inter_df['total']\n",
    "my_file = \"intra_inter_df_noCOVID.csv\"    \n",
    "intra_inter_df.to_csv(os.path.join(my_path_, my_file),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf3cc5f2-b489-4af3-acb1-1b00ac6e1255",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248c87d5-b1b7-4f63-bcdd-fbfc37b7f91e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from scipy import optimize\n",
    "import numpy.polynomial.polynomial as npoly\n",
    "\n",
    "def plot_fit_rolling_breakpoints_2(df,x_column,x_column2,x_label,title,window_size,num_breakpoints):\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(15, 5))\n",
    "    x_dates = list(df['publication_date_1'])\n",
    "    y_data = df[x_column].rolling(window=window_size).mean()[window_size-1:]\n",
    "    y_data2 = df[x_column2].rolling(window=window_size).mean()[window_size-1:]\n",
    "    x_data = x_dates[window_size-1:]\n",
    "    ax.set_ylim([0, 1])\n",
    "    \n",
    "    ax.plot(x_data, y_data, \"o-\", color='orange', markersize=3,label='intra-institution')\n",
    "    ax.plot(x_data, y_data2, \"o-\", color='green', markersize=3,label='inter-institution')\n",
    "\n",
    "    plt.grid(True, linewidth=0.5)\n",
    "    #ax.yaxis.set_major_formatter(formatter)\n",
    "\n",
    "    ax.set_xlabel(x_label,size=20)\n",
    "    #ax.set_title(title,size=30)\n",
    "\n",
    "    #ax.xaxis.set_major_locator(mdates.MonthLocator()) # Make ticks on occurrences of each month\n",
    "    ax.xaxis.set_major_formatter(mdates.DateFormatter('%b %y')) # Get only the month to show in the x-axis\n",
    "\n",
    "    ax.axvline(pd.Timestamp(2020, 3, 1),color='r') #ax.axvline(x_dates[84],color='r')\n",
    "\n",
    "    #save for all possible combination of breakpoints the correspondent error \n",
    "    def f(breakpoints, x, y, fcache): \n",
    "        breakpoints = tuple(map(int, sorted(breakpoints)))\n",
    "        if breakpoints not in fcache:\n",
    "            total_error = 0\n",
    "            for f, xi, yi in find_best_piecewise_polynomial(breakpoints, x, y):\n",
    "                total_error += ((f(xi) - yi)**2).sum()\n",
    "            fcache[breakpoints] = total_error \n",
    "        # print('{} --> {}'.format(breakpoints, fcache[breakpoints]))\n",
    "        return fcache[breakpoints]\n",
    "\n",
    "    def find_best_piecewise_polynomial(breakpoints, x, y):\n",
    "        breakpoints = tuple(map(int, sorted(breakpoints)))\n",
    "        xs = np.split(x, breakpoints)\n",
    "        ys = np.split(y, breakpoints)\n",
    "        result = []\n",
    "        for xi, yi in zip(xs, ys):\n",
    "            if len(xi) < 2: continue\n",
    "            coefs = npoly.polyfit(xi, yi, 1)\n",
    "            f = npoly.Polynomial(coefs)\n",
    "            result.append([f, xi, yi])\n",
    "        return result\n",
    "    \n",
    "    x_num = dates.date2num(x_data)\n",
    "    breakpoints = optimize.brute(f, [slice(1, len(x_data), 1)]*num_breakpoints, args=(x_num, y_data, {}), finish=None)\n",
    "    if num_breakpoints==1:\n",
    "        breakpoints = [breakpoints] \n",
    "    for f, xi, yi in find_best_piecewise_polynomial(breakpoints, x_num, y_data):\n",
    "        xi_min = x_data[np.where(x_num == xi.min())[0][0]]\n",
    "        xi_max = x_data[np.where(x_num == xi.max())[0][0]]\n",
    "        x_interval = np.array([xi_min, xi_max]) \n",
    "        #print('y = {:35s}, if x in [{}, {}]'.format(str(f), *x_interval))\n",
    "        x_interval = np.array([xi.min(), xi.max()])\n",
    "        #ax.plot(x_interval, f(x_interval), 'yo-')\n",
    "        coef = f.convert().coef\n",
    "        b = coef[0]\n",
    "        a = coef[1]\n",
    "        if b>0:\n",
    "            ll = 'y = {:.2f} x + {:.0f}'.format(a,b)\n",
    "        else: #b<0\n",
    "            ll = 'y = {:.2f} x - {:.0f}'.format(a,abs(b))    \n",
    "        ax.plot(x_interval, f(x_interval), 'yo-',linewidth=2, markersize=7)       \n",
    "    #save for all possible combination of breakpoints the correspondent error\n",
    "    \n",
    "    def f(breakpoints, x, y, fcache): \n",
    "        breakpoints = tuple(map(int, sorted(breakpoints)))\n",
    "        if breakpoints not in fcache:\n",
    "            total_error = 0\n",
    "            for f, xi, yi in find_best_piecewise_polynomial(breakpoints, x, y):\n",
    "                total_error += ((f(xi) - yi)**2).sum()\n",
    "            fcache[breakpoints] = total_error \n",
    "        # print('{} --> {}'.format(breakpoints, fcache[breakpoints]))\n",
    "        return fcache[breakpoints]\n",
    "\n",
    "    def find_best_piecewise_polynomial(breakpoints, x, y):\n",
    "        breakpoints = tuple(map(int, sorted(breakpoints)))\n",
    "        xs = np.split(x, breakpoints)\n",
    "        ys = np.split(y, breakpoints)\n",
    "        result = []\n",
    "        for xi, yi in zip(xs, ys):\n",
    "            if len(xi) < 2: continue\n",
    "            coefs = npoly.polyfit(xi, yi, 1)\n",
    "            f = npoly.Polynomial(coefs)\n",
    "            result.append([f, xi, yi])\n",
    "        return result\n",
    "    \n",
    "    x_num = dates.date2num(x_data)\n",
    "    breakpoints = optimize.brute(f, [slice(1, len(x_data), 1)]*num_breakpoints, args=(x_num, y_data2, {}), finish=None)\n",
    "    if num_breakpoints==1:\n",
    "        breakpoints = [breakpoints] \n",
    "    for f, xi, yi in find_best_piecewise_polynomial(breakpoints, x_num, y_data2):\n",
    "        xi_min = x_data[np.where(x_num == xi.min())[0][0]]\n",
    "        xi_max = x_data[np.where(x_num == xi.max())[0][0]]\n",
    "        x_interval = np.array([xi_min, xi_max]) \n",
    "        #print('y = {:35s}, if x in [{}, {}]'.format(str(f), *x_interval))\n",
    "        x_interval = np.array([xi.min(), xi.max()])\n",
    "        #ax.plot(x_interval, f(x_interval), 'yo-')\n",
    "        coef = f.convert().coef\n",
    "        b = coef[0]\n",
    "        a = coef[1]\n",
    "        if b>0:\n",
    "            ll = 'y = {:.2f} x + {:.0f}'.format(a,b)\n",
    "        else: #b<0\n",
    "            ll = 'y = {:.2f} x - {:.0f}'.format(a,abs(b))    \n",
    "        ax.plot(x_interval, f(x_interval), 'yo-',linewidth=2, markersize=7) \n",
    "       \n",
    "    ax.legend()        \n",
    "    ax.set_title(title,size=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a697a6-c582-460f-bbd3-6f2c11a22292",
   "metadata": {},
   "source": [
    "### COVID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42b4687-1f42-4bb5-b3bb-5c0e56aa27d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "my_file = \"intra_inter_df_noCOVID.csv\"    \n",
    "intra_inter_df = pd.read_csv(os.path.join(my_path_, my_file))\n",
    "intra_inter_df['publication_date_1'] = intra_inter_df['publication_date_1'].apply(pd.to_datetime)\n",
    "intra_inter_df = intra_inter_df[intra_inter_df.publication_date_1<'2024-01-01']\n",
    "x_column = 'frac_intra'\n",
    "x_column2 = 'frac_inter'\n",
    "x_label = 'publication_date_1'\n",
    "title = 'Fraction collabs. intra/inter-inst. - no COVID'\n",
    "window_size = 1\n",
    "num_breakpoints = 2\n",
    "plot_fit_rolling_breakpoints_2(intra_inter_df,x_column,x_column2,x_label,title,window_size,num_breakpoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43345c88-b9ca-4ed7-b7ee-712d0f570338",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "my_file = \"intra_inter_df_COVID.csv\"    \n",
    "intra_inter_df = pd.read_csv(os.path.join(my_path_, my_file))\n",
    "intra_inter_df['publication_date_1'] = intra_inter_df['publication_date_1'].apply(pd.to_datetime)\n",
    "intra_inter_df = intra_inter_df[intra_inter_df.publication_date_1>='2020-01-01']\n",
    "intra_inter_df = intra_inter_df[intra_inter_df.publication_date_1<'2024-01-01']\n",
    "x_column = 'frac_intra'\n",
    "x_column2 = 'frac_inter'\n",
    "x_label = 'publication_date_1'\n",
    "title = 'Fraction collabs. intra/inter-inst. - COVID'\n",
    "num_breakpoints = 1\n",
    "plot_fit_rolling_breakpoints_2(intra_inter_df,x_column,x_column2,x_label,title,window_size,num_breakpoints)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d031986f-d461-468a-83af-e3480da51404",
   "metadata": {},
   "source": [
    "## Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c65a6c-a558-48fb-b7e6-0ebee0c0daf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_file = \"preprint_dict.pickle\"\n",
    "with open(os.path.join(Path('./Tables'), my_file),\"rb\") as fp:\n",
    "    preprint_dict = pickle.load(fp)\n",
    "rows = [(category, work_id) for category, work_ids in preprint_dict.items() for work_id in work_ids]\n",
    "preprint_df = pd.DataFrame(rows, columns=['tax_name', 'work_id'])\n",
    "preprint_categories_list = list(preprint_df.groupby('tax_name').work_id.count().to_frame().sort_values(by='work_id',ascending=False).index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae2dbb6-3800-4e40-bbdf-17f62965391d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16cf5700-a21c-4a2d-87b6-2867e33d13e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "my_file = \"work_edges_dist_mean.csv\"    \n",
    "work_edges_dist_mean = pd.read_csv(os.path.join(my_path_, my_file))\n",
    "work_edges_dist_mean['publication_date_1'] = pd.to_datetime(work_edges_dist_mean['publication_date_1'])\n",
    "df = work_edges_dist_mean\n",
    "df = df[df.publication_date_1>='2000']\n",
    "df = df[df.publication_date_1<'2024']\n",
    "df = df.merge(preprint_df,on='work_id')\n",
    "df_stat_tot = pd.DataFrame()\n",
    "for k in preprint_categories_list:\n",
    "    df_k = df[df.tax_name==k]\n",
    "    df_stat = df_k[['publication_date_1','dist']].groupby('publication_date_1').dist.agg(['mean', 'std','sem']).reset_index()\n",
    "    df_stat.insert(1, 'category', k)\n",
    "    df_stat_tot = pd.concat([df_stat_tot,df_stat])    \n",
    "    \n",
    "    plt.style.use(\"dark_background\")\n",
    "    fig, ax = plt.subplots(figsize=(15, 5))\n",
    "    x_data = list(df_stat['publication_date_1'])\n",
    "    y_mean = df_stat['mean']\n",
    "    y_sem = df_stat['sem']\n",
    "    ax.plot(x_data, y_mean, \"o\", color = 'green',markersize=5)\n",
    "    #ax.fill_between(x_data, y_mean - y_sem, y_mean + y_sem, color = 'green',alpha=0.4)\n",
    "    ax.axvline(pd.Timestamp(2020, 3, 1),color='r')\n",
    "    plt.grid(True, linewidth=0.5)\n",
    "    ax.yaxis.set_major_formatter(formatter)\n",
    "\n",
    "    ax.set_xlabel('month',size=20)\n",
    "    title = 'ATD - '+k\n",
    "    ax.set_title(title,size=30)    \n",
    "\n",
    "my_file = \"work_edges_dist_mean_cat.pickle\"\n",
    "pickle.dump(df_stat_tot, open(os.path.join(my_path_, my_file), 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2672e26-bca2-401a-8fc4-db95aa851e8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54447c7a-ff63-4ae7-a86a-2b354685e306",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "works = read_parquet(basepath / 'works')\n",
    "work_authors_edges_df = read_parquet(my_path_ / 'work_authors_edges_df_dist')\n",
    "work_authors_edges_df = works[['work_id']].reset_index().merge(work_authors_edges_df,on='work_id')\n",
    "work_authors_edges_df = work_authors_edges_df[work_authors_edges_df.publication_date_1>='2000-01-01']\n",
    "work_authors_edges_df = work_authors_edges_df[work_authors_edges_df.publication_date_1<'2024-01-01']\n",
    "df = work_authors_edges_df\n",
    "df = df.merge(preprint_df,on='work_id')\n",
    "intra_inter_df_tot = pd.DataFrame()\n",
    "for k in preprint_categories_list:\n",
    "    df_k = df[df.tax_name==k]\n",
    "    df1 = df_k.groupby('publication_date_1').intra.count().reset_index().rename(columns={'intra':'total'})\n",
    "    df2 = df_k.groupby('publication_date_1').intra.sum().to_frame().reset_index()\n",
    "    intra_inter_df = df1.merge(df2,on='publication_date_1')\n",
    "    intra_inter_df['inter'] = intra_inter_df['total'] - intra_inter_df['intra']\n",
    "    intra_inter_df['frac_intra'] = intra_inter_df['intra'] / intra_inter_df['total']\n",
    "    intra_inter_df['frac_inter'] = intra_inter_df['inter'] / intra_inter_df['total']\n",
    "    intra_inter_df.insert(1, 'category', k)\n",
    "    intra_inter_df_tot = pd.concat([intra_inter_df_tot,intra_inter_df])    \n",
    "    \n",
    "    plt.style.use(\"dark_background\")\n",
    "    fig, ax = plt.subplots(figsize=(15, 5))\n",
    "    x_data = list(intra_inter_df['publication_date_1'])\n",
    "    y_mean = intra_inter_df['frac_inter']\n",
    "    ax.plot(x_data, y_mean, \"o\", color = 'magenta',markersize=5)\n",
    "    #ax.fill_between(x_data, y_mean - y_sem, y_mean + y_sem, color = 'green',alpha=0.4)\n",
    "    ax.axvline(pd.Timestamp(2020, 3, 1),color='r')\n",
    "    plt.grid(True, linewidth=0.5)\n",
    "    #ax.yaxis.set_major_formatter(formatter)\n",
    "\n",
    "    ax.set_xlabel('month',size=20)\n",
    "    title = 'inter-institution collaborations - '+k\n",
    "    ax.set_title(title,size=30)  \n",
    "   \n",
    "my_file = \"intra_inter_df_cat.csv\"    \n",
    "intra_inter_df_tot.to_csv(os.path.join(my_path_, my_file),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5b3810-df89-466f-b9ce-a87248d5be41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96a0749-bf9d-4cb0-b9cc-38602b780f4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd251a69-6986-48e4-966f-0f48839e3960",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c08ea4-ff46-48e2-b046-0a6457cc90fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "673e78d2-9878-4e2b-a926-30934c9ac6e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
