{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b932d5cb-029a-491b-ac04-1038406b081a",
   "metadata": {},
   "source": [
    "# Model - edges - 2 authors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292ac750-81c2-4b20-bf84-49ef6bcb1c9a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from pathlib import Path\n",
    "import networkx as nx\n",
    "import igraph as ig\n",
    "import pickle\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.spatial import distance\n",
    "import seaborn as sns\n",
    "from time import time\n",
    "from tqdm.auto import tqdm\n",
    "import random \n",
    "import os\n",
    "from itertools import chain, combinations\n",
    "import itertools\n",
    "import collections\n",
    "\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "import plotly.io as pio\n",
    "plt.rcParams.update({'font.size': 22})\n",
    "sns.set(style=\"ticks\", context=\"talk\")\n",
    "plt.style.use(\"dark_background\")\n",
    "pd.options.plotting.backend = 'plotly'\n",
    "pio.templates.default = 'plotly_dark+presentation'\n",
    "                    \n",
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "#pd.options.mode.chained_assignment = None \n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "import matplotlib.dates as dates\n",
    "import matplotlib.dates as mdates\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "from scipy import optimize\n",
    "import numpy.polynomial.polynomial as npoly\n",
    "\n",
    "def form(x,pos):\n",
    "    if x<1e3:\n",
    "        return '%1.3f' % (x)\n",
    "    elif x<1e6:\n",
    "        return '%1.1fK' % (x * 1e-3)\n",
    "    else:\n",
    "        return '%1.1fM' % (x * 1e-6)\n",
    "formatter = FuncFormatter(form)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def read_parquet(name, **args):\n",
    "    path = name\n",
    "    print(f'Reading {name!r}')\n",
    "    tic = time()\n",
    "    df = pd.read_parquet(path, engine='fastparquet', **args)\n",
    "    before = len(df)\n",
    "    # df.drop_duplicates(inplace=True)\n",
    "    toc = time()\n",
    "    after = len(df)\n",
    "        \n",
    "    print(f'Read {len(df):,} rows from {path.stem!r} in {toc-tic:.2f} sec. {before-after:,} duplicates.')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac043be-27a4-44b4-a9c4-a4b9b0f7d4bf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "basepath = Path('./Tables_final') \n",
    "my_path_ = Path('./Model_2authors')\n",
    "if not os.path.exists(my_path_):\n",
    "    os.makedirs(my_path_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b511e6-dcf3-44b8-9c4e-3f8c9e134b2c",
   "metadata": {},
   "source": [
    "## Model 2 variables\n",
    "\n",
    "B_{ij} = beta * ((s_i*s_j)^a/(d_{ij}+c)^alpha) if i!=j </br>\n",
    "B_{ii} = gamma * s_i if i!=j </br>\n",
    "\n",
    "P_{ij} = B_{ij}/N(alpha,beta,gamma)</br>\n",
    "P_{ii} = B_{ii}/N(alpha,beta,gamma)</br>\n",
    "with N(alpha,beta,gamma) = sum_{i} B_{ii} + sum_{(i,j)} B_{ij}</br>\n",
    "\n",
    "Parameters: a=1/2, c=10 </br>\n",
    "Variables: alpha>=0, beta>=0, gamma>=0 </br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb09bab-183f-4812-b97b-57c7fb8e7870",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_collaboration_graph(works_authors_rows):\n",
    "    \n",
    "    authors_id_set = set(works_authors_rows.author_id)\n",
    "                                  \n",
    "    bip_g = nx.from_pandas_edgelist(\n",
    "        works_authors_rows,\n",
    "        source='work_id', target='author_id'\n",
    "    )\n",
    "    \n",
    "    collab_graph = nx.bipartite.weighted_projected_graph(bip_g,nodes=authors_id_set) #bipartite.weighted_projected_graph(bip_g,nodes=authors_id)\n",
    "    return collab_graph\n",
    "\n",
    "def strength_update(df_intra,df_inter,W,a,c,params):\n",
    "    N = ( params[1] *((df_inter['m_prod'])/ ((df_inter['dist']+c)**params[0])) ).sum() + (params[2] *df_intra['m_source']).sum() \n",
    "    df_inter2 = df_inter.copy()\n",
    "    df_inter2[['target','source','m_target','m_source']] = df_inter2[['source','target','m_source','m_target']] \n",
    "    df_inter = pd.concat([df_inter,df_inter2])\n",
    "    df_inter['a'] = params[1] *((df_inter['m_prod'])/ ((df_inter['dist']+c)**params[0]))\n",
    "    df_intra['a'] = 2*params[2] *df_intra['m_source']\n",
    "    df = pd.concat([df_inter,df_intra])\n",
    "    df = df.groupby(['source','m_source']).a.sum().to_frame().reset_index()\n",
    "    df['m_source'] = df['m_source'] + (W/N)*df['a']\n",
    "    df = df[['source','m_source']].rename(columns={'source':'institution_id','m_source':'strength'})\n",
    "    inst_str_dict = df.set_index('institution_id').to_dict()['strength']\n",
    "    return inst_str_dict\n",
    "\n",
    "def model_function1(df_intra,df_inter,a,c,params):    \n",
    "    N = ( params[1] *((df_inter['m_prod'])/ ((df_inter['dist']+c)**params[0])) ).sum() + (params[2] *df_intra['m_source']).sum() \n",
    "    u = ( (params[2] *df_intra['m_source']).sum() )  / N\n",
    "    return u\n",
    "    \n",
    "def model_function2(df_intra,df_inter,a,c,params):\n",
    "    N = ( params[1] *((df_inter['m_prod'])/ ((df_inter['dist']+c)**params[0])) ).sum() + (params[2] *df_intra['m_source']).sum()\n",
    "    u = (  ( (df_inter['dist']) * ( ((params[1] *df_inter['m_prod'])/ ((df_inter['dist']+c)**(params[0]))) ) ).sum() ) / N\n",
    "    return u\n",
    "    \n",
    "def objective_function(params,a,c, x_data, y_data):\n",
    "    df_intra = x_data[0]\n",
    "    df_inter = x_data[1]\n",
    "\n",
    "    y_pred1 = model_function1(df_intra,df_inter,a,c,params)\n",
    "    y_pred2 = model_function2(df_intra,df_inter,a,c,params)\n",
    "    of = ((y_pred1 - y_data[0]) / y_data[0])**2  +  (((y_pred2 - y_data[1]) / y_data[1])**2)  \n",
    "\n",
    "    return of \n",
    "\n",
    "def model():\n",
    "    my_file = \"dfs_2authors.pickle\"\n",
    "    with open(os.path.join(my_path_, my_file),\"rb\") as fp:\n",
    "        [works_2authors,works,works_authors_aff] = pickle.load(fp)  \n",
    "    works = works.loc['2000-01-01':'2023-12-01'] \n",
    "    works_authors_aff = works_authors_aff.loc['2000-01-01':'2023-12-01']\n",
    "    N_dict = works_authors_aff.groupby('publication_date_1').work_id.nunique().to_frame().to_dict()['work_id']\n",
    "    months_list = list(N_dict.keys())\n",
    "    months_list.sort()\n",
    "    months_list = [i.strftime('%Y-%m-%d') for i in months_list]\n",
    "    start_index = 0\n",
    "    end_index = 120 #180\n",
    "    start_month = months_list[start_index]\n",
    "    end_month = months_list[end_index-1]\n",
    "    my_file = \"df_strengths0.csv\"     \n",
    "    df = pd.read_csv(os.path.join(my_path_, my_file))\n",
    "    inst_str_dict = df.set_index('institution_id').to_dict()['strength']\n",
    "    my_file = \"inst_set.pickle\"\n",
    "    with open(os.path.join(my_path_, my_file),\"rb\") as fp:\n",
    "        inst_set = pickle.load(fp)    \n",
    "    # #calculate d_ij \n",
    "    my_file = \"I_dist_threshold.csv\"\n",
    "    I_dist = read_parquet(basepath / 'I_dist_threshold')\n",
    "    I_dist = I_dist[['source','target','dist']].reset_index(drop=True)\n",
    "    I_dist = I_dist.query('source.isin(@inst_set) & target.isin(@inst_set)')\n",
    "\n",
    "    #calculate #each month: F_INTRA\n",
    "\n",
    "    my_file = \"df_data1.csv\"    \n",
    "    df_data1 = pd.read_csv(os.path.join(my_path_, my_file))\n",
    "    df_data1 = df_data1.set_index('publication_date_1')\n",
    "    df_data3 = df_data1\n",
    "    df_data3 = df_data3[['total']]\n",
    "    df_data3['total'] = df_data3['total'].astype(int)\n",
    "    df_data3 = df_data3.loc[months_list[end_index]:months_list[-1]]\n",
    "    df_data3_list = list(df_data3['total'])\n",
    "    df_data1 = df_data1[['frac_intra']]\n",
    "    df_data1 = df_data1.rename(columns={'frac_intra':'F_aff'})\n",
    "    df_data1 = df_data1.loc[months_list[end_index]:months_list[-1]] #[months_list[end_index-1]:months_list[-1]]\n",
    "    #each month: average distance\n",
    "    my_file = \"work_edges_dist_mean_monthly.csv\"    \n",
    "    df_data2 = pd.read_csv(os.path.join(my_path_, my_file))\n",
    "    df_data2 = df_data2.set_index('publication_date_1').loc[months_list[end_index]:months_list[-1]]\n",
    "\n",
    "    I_dist = I_dist.query('source.isin(@inst_set) & target.isin(@inst_set)')\n",
    "\n",
    "    c = 100\n",
    "    a = 0.5\n",
    "    \n",
    "    df = I_dist.copy()\n",
    "    df['m_source'] = df['source'].map(inst_str_dict)\n",
    "    df['m_target'] = df['target'].map(inst_str_dict)\n",
    "    df['m_prod'] = (df['m_source']*df['m_target'])**a\n",
    "    df_intra = df[df.source == df.target]\n",
    "    df_inter = df[df.source != df.target]\n",
    "    #df_intra = df[df.dist==0]\n",
    "    #df_intra = df[df.dist>0]\n",
    "\n",
    "    opt_dict = {}\n",
    "    x_data = [df_intra,df_inter]\n",
    "    y_data = np.array([list(df_data1['F_aff'])[0],list(df_data2['dist'])[0]])\n",
    "\n",
    "    #randomstart #starting point can be not feasible\n",
    "    np.random.seed(0)\n",
    "    initial_params_list = [[2.0,1.0,1.0]] + [list(np.concatenate([np.random.uniform(0, 5, 1),np.random.uniform(0, 1e3, 2)])) for _ in range(9)]\n",
    "    err_ = +np.inf\n",
    "    initial_params_ = np.nan\n",
    "    result_ = np.nan\n",
    "    for initial_params in tqdm(initial_params_list):\n",
    "        result = minimize(objective_function, initial_params, args=(a,c,x_data,y_data), bounds=((0, np.inf), (0, np.inf), (0, np.inf)), tol = 1e-10, options={'eps': 1e-10, 'ftol': 1e-15}) #, method='SLSQP'\n",
    "        err = result.fun\n",
    "        success = result.success\n",
    "        print(err,success,result.message)\n",
    "        if err<err_ and success:\n",
    "            initial_params_ = initial_params\n",
    "            result_ = result\n",
    "            err_ = err\n",
    "\n",
    "#     my_file = \"initial_params_randomstart.pickle\"\n",
    "#     pickle.dump([initial_params_,result_], open(os.path.join(path_2authors, my_file), 'wb'))\n",
    "\n",
    "#     # #run on the best one\n",
    "#     # my_file = \"initial_params_randomstart.pickle\"\n",
    "#     # with open(os.path.join(path_2authors, my_file),\"rb\") as fp:\n",
    "#     #     [initial_params_,result_] = pickle.load(fp)\n",
    "\n",
    "    params = result_.x\n",
    "    err = result_.fun\n",
    "    success = result_.success\n",
    "    message = result_.message\n",
    "    print(f'{0} {list(df_data3.index)[0]} {params[0]:.5f} {params[1]:.5f} {params[2]:.5f} {params[1]/params[2]:.5f}  {err:.2e} {success} {message}')\n",
    "    opt_dict[0] = {'optimized_alpha':params[0], 'optimized_beta':params[1], 'optimized_gamma':params[2],'beta/gamma':params[1]/params[2],'optimized_err':err, 'success':success, 'message':message}\n",
    "\n",
    "    #update strenght with parameters\n",
    "    for i in tqdm(range(len(df_data3_list)-1)):\n",
    "        W = int(1e5) #df_data3_list[i]\n",
    "        inst_str_dict = strength_update(df_intra,df_inter,W,a,c,params)\n",
    "\n",
    "        df = I_dist.copy()\n",
    "        df['m_source'] = df['source'].map(inst_str_dict)\n",
    "        df['m_target'] = df['target'].map(inst_str_dict)\n",
    "        df['m_prod'] = (df['m_source']*df['m_target'])**a\n",
    "        df_intra = df[df.source == df.target]\n",
    "        df_inter = df[df.source != df.target]\n",
    "\n",
    "        x_data = [df_intra,df_inter]\n",
    "        y_data = np.array([list(df_data1['F_aff'])[i+1],list(df_data2['dist'])[i+1]])\n",
    "\n",
    "        result = minimize(objective_function, params, args=(a,c,x_data,y_data), bounds=((0, np.inf), (0, np.inf), (0, np.inf)), tol = 1e-10, options={'eps': 1e-10, 'ftol': 1e-15}) \n",
    "        params = result.x\n",
    "        err = result.fun\n",
    "        success = result.success\n",
    "        message = result.message\n",
    "        print(f'{i+1} {list(df_data3.index)[i+1]} {params[0]:.5f} {params[1]:.5f} {params[2]:.5f} {params[1]/params[2]:.5f} {err:.2e} {success} {message}')\n",
    "        opt_dict[i+1] = {'optimized_alpha':params[0], 'optimized_beta':params[1], 'optimized_gamma':params[2],'beta/gamma':params[1]/params[2],'optimized_err':err, 'success':success,'message':message}\n",
    "\n",
    "    opt_df = pd.DataFrame.from_dict(opt_dict).T\n",
    "    opt_df['month'] = list(df_data3.index)\n",
    "    my_file = \"opt_df.csv\"   \n",
    "    opt_df.to_csv(os.path.join(my_path_, my_file),index=False) \n",
    "model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba933e3-8a5a-48c9-ab8f-0ba6b0a6aab5",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_model(df,ylabel,title,color,log=False):\n",
    "    plt.style.use(\"dark_background\")\n",
    "    fig, ax = plt.subplots(figsize=(15, 5))   \n",
    "    if log:\n",
    "        ax.semilogy(list(df['month']), df[ylabel], \"o-\", color=color, markersize=3)\n",
    "    else:\n",
    "        ax.plot(list(df['month']), df[ylabel], \"o-\", color=color, markersize=3)\n",
    "    ax.axvline(pd.Timestamp(2020, 3, 1),color='r')\n",
    "    plt.grid(True, linewidth=0.5)\n",
    "    ax.ticklabel_format(axis='y')\n",
    "    ax.set_xlabel('month',size=20)\n",
    "    ax.set_title(title,size=30)\n",
    "my_file = \"opt_df.csv\" \n",
    "opt_df = pd.read_csv(os.path.join(my_path_, my_file))\n",
    "opt_df['month'] = opt_df['month'].apply(pd.to_datetime)\n",
    "plot_model(opt_df,'optimized_alpha','Optimized alpha','orange')\n",
    "plot_model(opt_df,'beta/gamma','Optimized beta/gamma','red')\n",
    "plot_model(opt_df,'optimized_err','Optimized error','blue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56bfd0fa-c7c7-49f4-8975-ce47585e4b9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19365019-884a-4dbd-ad79-9a2c5555cd64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b934f506-6368-4320-94e6-765391295b79",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "### Simulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90909ca4-442d-4cba-a015-20eaacf5f90b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def simulation():\n",
    "    c = 100\n",
    "    a = 0.5 \n",
    "    \n",
    "    my_file = \"opt_df.csv\" \n",
    "    opt_df = pd.read_csv(os.path.join(my_path_, my_file))\n",
    "    opt_df['month'] = opt_df['month'].apply(pd.to_datetime)\n",
    "    \n",
    "    my_file = \"dfs_2authors.pickle\"\n",
    "    with open(os.path.join(my_path_, my_file),\"rb\") as fp:\n",
    "        [works_2authors,works,works_authors_aff] = pickle.load(fp)  \n",
    "    works = works.loc['2000-01-01':'2023-12-01'] \n",
    "    works_authors_aff = works_authors_aff.loc['2000-01-01':'2023-12-01']\n",
    "    N_dict = works_authors_aff.groupby('publication_date_1').work_id.nunique().to_frame().to_dict()['work_id']\n",
    "    months_list = list(N_dict.keys())\n",
    "    months_list.sort()\n",
    "    months_list = [i.strftime('%Y-%m-%d') for i in months_list]\n",
    "    start_index = 0\n",
    "    end_index = 120 #180\n",
    "    start_month = months_list[start_index]\n",
    "    end_month = months_list[end_index-1]\n",
    "    my_file = \"df_strengths0.csv\"     \n",
    "    df_0 = pd.read_csv(os.path.join(my_path_, my_file))\n",
    "    my_file = \"inst_set.pickle\"\n",
    "    with open(os.path.join(my_path_, my_file),\"rb\") as fp:\n",
    "        inst_set = pickle.load(fp)       \n",
    "    \n",
    "    #calculate #each month: F_INTRA\n",
    "    my_file = \"df_data1.csv\"    \n",
    "    df_data1 = pd.read_csv(os.path.join(my_path_, my_file))\n",
    "    df_data1 = df_data1.set_index('publication_date_1')\n",
    "    df_data3 = df_data1\n",
    "    df_data3 = df_data3[['total']]\n",
    "    df_data3['total'] = df_data3['total'].astype(int)\n",
    "    df_data3 = df_data3.loc[months_list[end_index]:months_list[-1]]\n",
    "    df_data3_list = list(df_data3['total'])\n",
    "    df_data1 = df_data1[['frac_intra']]\n",
    "    df_data1 = df_data1.rename(columns={'frac_intra':'F_aff'})\n",
    "    df_data1 = df_data1.loc[months_list[end_index]:months_list[-1]] #[months_list[end_index-1]:months_list[-1]]\n",
    "    #each month: average distance\n",
    "    my_file = \"work_edges_dist_mean_monthly.csv\"    \n",
    "    df_data2 = pd.read_csv(os.path.join(my_path_, my_file))\n",
    "    df_data2 = df_data2.set_index('publication_date_1').loc[months_list[end_index]:months_list[-1]]\n",
    "    \n",
    "    my_file = \"I_dist_threshold.csv\"\n",
    "    I_dist = read_parquet(basepath / 'I_dist_threshold')\n",
    "    I_dist = I_dist[['source','target','dist']].reset_index(drop=True)\n",
    "    I_dist = I_dist.query('source.isin(@inst_set) & target.isin(@inst_set)')\n",
    "    \n",
    "    df_intra = I_dist[I_dist.source == I_dist.target].reset_index(drop=True)\n",
    "    df_inter = I_dist[I_dist.source != I_dist.target].reset_index(drop=True)\n",
    "    df_inter.index += len(df_intra)\n",
    "\n",
    "    import random\n",
    "    random.seed(0)\n",
    "    for s in tqdm(range(10)):\n",
    "\n",
    "        df_ = df_0\n",
    "\n",
    "        fra_intra_list = []\n",
    "        mean_dist_list = []\n",
    "        for i in tqdm(range(len(df_data3_list))):\n",
    "\n",
    "            #update \n",
    "            alpha = list(opt_df['optimized_alpha'])[i]\n",
    "            beta = list(opt_df['optimized_beta'])[i]\n",
    "            gamma = list(opt_df['optimized_gamma'])[i]\n",
    "            \n",
    "            inst_str_dict = df_[['institution_id','strength']].set_index('institution_id').to_dict()['strength']\n",
    "\n",
    "            df_intra['m_source'] = df_intra['source'].map(inst_str_dict)\n",
    "            df_inter['m_source'] = df_inter['source'].map(inst_str_dict)\n",
    "            df_inter['m_target'] = df_inter['target'].map(inst_str_dict)\n",
    "            df_inter['m_prod'] = (df_inter['m_source']*df_inter['m_target'])**a         \n",
    "\n",
    "            #edges probabilities\n",
    "            df_intra['p']= gamma*df_intra['m_source'] \n",
    "            df_inter['p'] = df_inter['m_prod'] * ( beta / ((df_inter['dist']+c)**alpha)) \n",
    "\n",
    "            hyperedges_probabilities = list(itertools.chain(df_intra['p'], df_inter['p']))\n",
    "            hyperedges_probabilities = np.array(hyperedges_probabilities)/(df_intra['p'].sum()+df_inter['p'].sum())\n",
    "            W = int(1e5) #df_data3_list[i] \n",
    "            hyperedges_model = random.choices(np.arange(0, len(hyperedges_probabilities)), weights=hyperedges_probabilities, k=W)\n",
    "\n",
    "            #count edges\n",
    "            counter = dict(collections.Counter(hyperedges_model))\n",
    "            df_intra['count'] = df_intra.index.to_series().map(counter)\n",
    "            df_intra['count'] = df_intra['count'].fillna(0)\n",
    "            df_inter['count'] = df_inter.index.to_series().map(counter)\n",
    "            df_inter['count'] = df_inter['count'].fillna(0)           \n",
    "            \n",
    "            #count intra-inter\n",
    "            frac_intra = sum(df_intra['count'])/(df_intra['count'].sum()+df_inter['count'].sum())\n",
    "            fra_intra_list.append(frac_intra)\n",
    "\n",
    "            #mean team distace\n",
    "            df_inter['mean_dist'] = df_inter['dist']*df_inter['count']\n",
    "            mean_dist = df_inter.mean_dist.sum()/W\n",
    "            mean_dist_list.append(mean_dist)\n",
    "\n",
    "            #update strength \n",
    "            df_intra['count'] = df_intra['count']*2 \n",
    "            temp = pd.concat([\n",
    "                df_intra[['source','count']].rename(columns={'source':'institution_id'}),\n",
    "                df_inter[['source','count']].rename(columns={'source':'institution_id'}),\n",
    "                df_inter[['target','count']].rename(columns={'target':'institution_id'})])\n",
    "            temp = temp.groupby('institution_id')['count'].sum().to_frame().reset_index()\n",
    "            df_old = df_\n",
    "            df_ = df_[['institution_id','strength']]\n",
    "            df_ = df_.merge(temp,on='institution_id',how='left')\n",
    "            df_['count'] = df_['count'].fillna(0)\n",
    "            df_['strength'] = df_['strength'] + df_['count']\n",
    "            df_ = df_[['institution_id','strength']]\n",
    "\n",
    "        model_data = pd.DataFrame.from_dict({'fra_intra':fra_intra_list,'mean_dist':mean_dist_list})\n",
    "        model_data['month'] = months_list[end_index:]\n",
    "        my_file = \"simulation_\"+str(s)+\".csv\"  \n",
    "        model_data.to_csv(os.path.join(my_path_, my_file))\n",
    "simulation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7490f6a5-90d0-45f6-b340-cc2c4fffdd42",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_simulation():\n",
    "    \n",
    "    my_file = \"simulation_\"+str(0)+\".csv\"  \n",
    "    model_data = pd.read_csv(os.path.join(my_path_, my_file))\n",
    "    model_data['month'] = pd.to_datetime(model_data['month'])\n",
    "    months_list = list(model_data['month'])\n",
    "    months_list.sort()\n",
    "    months_list = [i.strftime('%Y-%m-%d') for i in months_list]\n",
    "    \n",
    "    #calculate #each month: F_INTRA\n",
    "    my_file = \"df_data1.csv\"    \n",
    "    df_data1 = pd.read_csv(os.path.join(my_path_, my_file))\n",
    "    df_data1 = df_data1.set_index('publication_date_1')\n",
    "    df_data1 = df_data1[['frac_intra']]\n",
    "    df_data1 = df_data1.rename(columns={'frac_intra':'F_aff'})\n",
    "    df_data1 = df_data1.loc[months_list] #[months_list[end_index]:months_list[-1]]\n",
    "    #each month: average distance\n",
    "    my_file = \"work_edges_dist_mean_monthly.csv\"    \n",
    "    work_edges_dist_mean_monthly = pd.read_csv(os.path.join(my_path_, my_file))\n",
    "    work_edges_dist_mean_monthly = work_edges_dist_mean_monthly.set_index('publication_date_1')\n",
    "    work_edges_dist_mean_monthly = work_edges_dist_mean_monthly.loc[months_list]\n",
    "    df_data2 = work_edges_dist_mean_monthly\n",
    "\n",
    "    for s in range(10):\n",
    "        my_file = \"simulation_\"+str(s)+\".csv\"  \n",
    "        model_data = pd.read_csv(os.path.join(my_path_, my_file))\n",
    "        model_data['month'] = pd.to_datetime(model_data['month'])\n",
    "        months_list = list(model_data['month'])\n",
    "        months_list.sort()\n",
    "        months_list = [i.strftime('%Y-%m-%d') for i in months_list]\n",
    "\n",
    "        plt.style.use(\"dark_background\")\n",
    "        fig, ax = plt.subplots(figsize=(15, 5))\n",
    "        x_dates = list(model_data['month'])\n",
    "        x_data = x_dates\n",
    "        y_data1 = list(df_data1.F_aff) \n",
    "        display(model_data)\n",
    "        y_data2 = list(model_data['fra_intra'])\n",
    "        ax.plot(x_data, y_data1, \"o-\", markersize=3,label='data')\n",
    "        ax.plot(x_data, y_data2, \"o-\", markersize=3,label='model')\n",
    "        ax.axvline(pd.Timestamp(2020, 3, 1),color='r')\n",
    "        plt.grid(True, linewidth=0.5)\n",
    "        ax.yaxis.set_major_formatter(formatter)\n",
    "        ax.set_xlabel('month',size=20)\n",
    "        ax.legend() \n",
    "        ax.set_title('Frac intra-insts collabs - with exact params',size=30)\n",
    "        \n",
    "    for s in range(10):\n",
    "        my_file = \"simulation_\"+str(s)+\".csv\"  \n",
    "        model_data = pd.read_csv(os.path.join(my_path_, my_file))\n",
    "        model_data['month'] = pd.to_datetime(model_data['month'])\n",
    "        months_list = list(model_data['month'])\n",
    "        months_list.sort()\n",
    "        months_list = [i.strftime('%Y-%m-%d') for i in months_list]\n",
    "\n",
    "        plt.style.use(\"dark_background\")\n",
    "        fig, ax = plt.subplots(figsize=(15, 5))\n",
    "        x_data = x_dates\n",
    "        y_data1 = list(df_data2.dist)   \n",
    "        y_data2 = list(model_data['mean_dist'])\n",
    "        ax.plot(x_data, y_data1, \"o-\", markersize=3,label='data')\n",
    "        ax.plot(x_data, y_data2, \"o-\", markersize=3,label='model')\n",
    "        ax.axvline(pd.Timestamp(2020, 3, 1),color='r')\n",
    "        plt.grid(True, linewidth=0.5)\n",
    "        ax.yaxis.set_major_formatter(formatter)\n",
    "        ax.set_xlabel('month',size=20)\n",
    "        ax.legend() \n",
    "        ax.set_title('Avg team dist - with exact params',size=30)\n",
    "plot_simulation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c295588-6896-46f0-95c3-3e8f76646fa0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ca3b54c6-e49a-47df-9fce-1d1c41412d20",
   "metadata": {},
   "source": [
    "## Model 1 variable\n",
    "\n",
    "B_{ij} = beta * ((s_i*s_j)^(1/2)/(d_{ij}+c)^alpha) if i!=j </br>\n",
    "B_{ii} = gamma * s_i if i!=j </br>\n",
    "\n",
    "P_{ij} = B_{ij}/N(alpha,beta,gamma)</br>\n",
    "P_{ii} = B_{ii}/N(alpha,beta,gamma)</br>\n",
    "with N(alpha,beta,gamma) = sum_{i} B_{ii} + sum_{(i,j)} B_{ij}</br>\n",
    "\n",
    "Parameters: c=10 </br>\n",
    "Variables: alpha>=0 </br>\n",
    "Fixed: beta>=0, gamma>=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc536122-1ddf-4509-8c7b-327e4ffb1315",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "my_file = \"opt_df.csv\" \n",
    "opt_df = pd.read_csv(os.path.join(my_path_, my_file))\n",
    "opt_df['month'] = opt_df['month'].apply(pd.to_datetime)\n",
    "beta = opt_df['optimized_beta'].mean()\n",
    "gamma = opt_df['optimized_gamma'].mean()\n",
    "print(f'beta {beta}, gamma {gamma}, beta/gamma {beta/gamma}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4ad1a1-c0ef-403d-906e-efe0a9876412",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_collaboration_graph(works_authors_rows):\n",
    "    \n",
    "    authors_id_set = set(works_authors_rows.author_id)\n",
    "                                  \n",
    "    bip_g = nx.from_pandas_edgelist(\n",
    "        works_authors_rows,\n",
    "        source='work_id', target='author_id'\n",
    "    )\n",
    "    \n",
    "    collab_graph = nx.bipartite.weighted_projected_graph(bip_g,nodes=authors_id_set) #bipartite.weighted_projected_graph(bip_g,nodes=authors_id)\n",
    "    return collab_graph\n",
    "\n",
    "def strength_update(df_intra,df_inter,W,a,c,beta,gamma,params):\n",
    "    N = ( beta *((df_inter['m_prod'])/ ((df_inter['dist']+c)**params)) ).sum() + (gamma *df_intra['m_source']).sum() \n",
    "    df_inter2 = df_inter.copy()\n",
    "    df_inter2[['target','source','m_target','m_source']] = df_inter2[['source','target','m_source','m_target']] \n",
    "    df_inter = pd.concat([df_inter,df_inter2])\n",
    "    df_inter['a'] = beta *((df_inter['m_prod'])/ ((df_inter['dist']+c)**params))\n",
    "    df_intra['a'] = 2*gamma *df_intra['m_source']\n",
    "    df = pd.concat([df_inter,df_intra])\n",
    "    df = df.groupby(['source','m_source']).a.sum().to_frame().reset_index()\n",
    "    df['m_source'] = df['m_source'] + (W/N)*df['a']\n",
    "    df = df[['source','m_source']].rename(columns={'source':'institution_id','m_source':'strength'})\n",
    "    inst_str_dict = df.set_index('institution_id').to_dict()['strength']\n",
    "    return inst_str_dict\n",
    "\n",
    "def model_function1(df_intra,df_inter,a,c,beta,gamma,params):    \n",
    "    N = ( beta *((df_inter['m_prod'])/ ((df_inter['dist']+c)**params)) ).sum() + (gamma *df_intra['m_source']).sum() \n",
    "    u = ( (gamma *df_intra['m_source']).sum() )  / N\n",
    "    return u\n",
    "    \n",
    "def model_function2(df_intra,df_inter,a,c,beta,gamma,params):\n",
    "    N = ( beta *((df_inter['m_prod'])/ ((df_inter['dist']+c)**params)) ).sum() + (gamma *df_intra['m_source']).sum()\n",
    "    u = (  ( (df_inter['dist']) * ( ((beta *df_inter['m_prod'])/ ((df_inter['dist']+c)**(params))) ) ).sum() ) / N\n",
    "    return u\n",
    "    \n",
    "def objective_function(params,a,c,beta,gamma, x_data, y_data):\n",
    "    df_intra = x_data[0]\n",
    "    df_inter = x_data[1]\n",
    "\n",
    "    y_pred1 = model_function1(df_intra,df_inter,a,c,beta,gamma,params)\n",
    "    y_pred2 = model_function2(df_intra,df_inter,a,c,beta,gamma,params)\n",
    "    of = ((y_pred1 - y_data[0]) / y_data[0])**2  +  (((y_pred2 - y_data[1]) / y_data[1])**2)  \n",
    "\n",
    "    return of \n",
    "\n",
    "def model_b():\n",
    "    \n",
    "    my_file = \"dfs_2authors.pickle\"\n",
    "    with open(os.path.join(my_path_, my_file),\"rb\") as fp:\n",
    "        [works_2authors,works,works_authors_aff] = pickle.load(fp)  \n",
    "    works = works.loc['2000-01-01':'2023-12-01'] \n",
    "    works_authors_aff = works_authors_aff.loc['2000-01-01':'2023-12-01']\n",
    "    N_dict = works_authors_aff.groupby('publication_date_1').work_id.nunique().to_frame().to_dict()['work_id']\n",
    "    months_list = list(N_dict.keys())\n",
    "    months_list.sort()\n",
    "    months_list = [i.strftime('%Y-%m-%d') for i in months_list]\n",
    "    start_index = 0\n",
    "    end_index = 120 #180\n",
    "    start_month = months_list[start_index]\n",
    "    end_month = months_list[end_index-1]\n",
    "    my_file = \"df_strengths0.csv\"     \n",
    "    df = pd.read_csv(os.path.join(my_path_, my_file))\n",
    "    inst_str_dict = df.set_index('institution_id').to_dict()['strength']\n",
    "    my_file = \"inst_set.pickle\"\n",
    "    with open(os.path.join(my_path_, my_file),\"rb\") as fp:\n",
    "        inst_set = pickle.load(fp)    \n",
    "    # #calculate d_ij \n",
    "    my_file = \"I_dist_threshold.csv\"\n",
    "    I_dist = read_parquet(basepath / 'I_dist_threshold')\n",
    "    I_dist = I_dist[['source','target','dist']].reset_index(drop=True)\n",
    "    I_dist = I_dist.query('source.isin(@inst_set) & target.isin(@inst_set)')\n",
    "\n",
    "    #calculate #each month: F_INTRA\n",
    "\n",
    "    my_file = \"df_data1.csv\"    \n",
    "    df_data1 = pd.read_csv(os.path.join(my_path_, my_file))\n",
    "    df_data1 = df_data1.set_index('publication_date_1')\n",
    "    df_data3 = df_data1\n",
    "    df_data3 = df_data3[['total']]\n",
    "    df_data3['total'] = df_data3['total'].astype(int)\n",
    "    df_data3 = df_data3.loc[months_list[end_index]:months_list[-1]]\n",
    "    df_data3_list = list(df_data3['total'])\n",
    "    df_data1 = df_data1[['frac_intra']]\n",
    "    df_data1 = df_data1.rename(columns={'frac_intra':'F_aff'})\n",
    "    df_data1 = df_data1.loc[months_list[end_index]:months_list[-1]] #[months_list[end_index-1]:months_list[-1]]\n",
    "    #each month: average distance\n",
    "    my_file = \"work_edges_dist_mean_monthly.csv\"    \n",
    "    df_data2 = pd.read_csv(os.path.join(my_path_, my_file))\n",
    "    df_data2 = df_data2.set_index('publication_date_1').loc[months_list[end_index]:months_list[-1]]\n",
    "\n",
    "\n",
    "    c = 100\n",
    "    a = 0.5\n",
    "    \n",
    "    df = I_dist.copy()\n",
    "    df['m_source'] = df['source'].map(inst_str_dict)\n",
    "    df['m_target'] = df['target'].map(inst_str_dict)\n",
    "    df['m_prod'] = (df['m_source']*df['m_target'])**a\n",
    "    df_intra = df[df.source == df.target]\n",
    "    df_inter = df[df.source != df.target]\n",
    "\n",
    "    opt_dict = {}\n",
    "    x_data = [df_intra,df_inter]\n",
    "    y_data = np.array([list(df_data1['F_aff'])[0],list(df_data2['dist'])[0]])\n",
    "\n",
    "    my_file = \"opt_df.csv\" \n",
    "    opt_df = pd.read_csv(os.path.join(my_path_, my_file))\n",
    "    opt_df['month'] = opt_df['month'].apply(pd.to_datetime)\n",
    "    beta = opt_df['optimized_beta'].mean()\n",
    "    gamma = opt_df['optimized_gamma'].mean()\n",
    "    \n",
    "    #randomstart #starting point can be not feasible\n",
    "    np.random.seed(0)\n",
    "    initial_params_list = [2.0] + [np.random.uniform(0, 10, 1)[0] for _ in range(19)]\n",
    "    err_ = +np.inf\n",
    "    initial_params_ = np.nan\n",
    "    result_ = np.nan\n",
    "    for initial_params in tqdm(initial_params_list):\n",
    "        result = minimize(objective_function, initial_params, args=(a,c,beta,gamma,x_data,y_data), bounds=[(0, np.inf)], tol = 1e-10, options={'eps': 1e-10, 'ftol': 1e-15}) #, method='SLSQP'\n",
    "        err = result.fun\n",
    "        success = result.success\n",
    "        print(err,success,result.message)\n",
    "        if err<err_ and success:\n",
    "            initial_params_ = initial_params\n",
    "            result_ = result\n",
    "            err_ = err\n",
    "\n",
    "#     my_file = \"initial_params_randomstart_b.pickle\"\n",
    "#     pickle.dump([initial_params_,result_], open(os.path.join(path_2authors, my_file), 'wb'))\n",
    "\n",
    "#     # #run on the best one\n",
    "#     # my_file = \"initial_params_randomstart_b.pickle\"\n",
    "#     # with open(os.path.join(path_2authors, my_file),\"rb\") as fp:\n",
    "#     #     [initial_params_,result_] = pickle.load(fp)\n",
    "\n",
    "    params = result_.x\n",
    "    err = result_.fun\n",
    "    success = result_.success\n",
    "    message = result_.message\n",
    "    print(f'{0} {list(df_data3.index)[0]} {params[0]:.5f} {err:.2e} {success} {message}')\n",
    "    opt_dict[0] = {'optimized_alpha':params[0], 'optimized_err':err, 'success':success, 'message':message}\n",
    "\n",
    "    #update strenght with parameters\n",
    "    for i in tqdm(range(len(df_data3_list)-1)):\n",
    "        W = int(1e5) #df_data3_list[i]\n",
    "        inst_str_dict = strength_update(df_intra,df_inter,W,a,c,beta,gamma,params)\n",
    "        \n",
    "        df = I_dist.copy()\n",
    "        df['m_source'] = df['source'].map(inst_str_dict)\n",
    "        df['m_target'] = df['target'].map(inst_str_dict)\n",
    "        df['m_prod'] = (df['m_source']*df['m_target'])**a\n",
    "        df_intra = df[df.source == df.target]\n",
    "        df_inter = df[df.source != df.target]\n",
    "\n",
    "        x_data = [df_intra,df_inter]\n",
    "        y_data = np.array([list(df_data1['F_aff'])[i+1],list(df_data2['dist'])[i+1]])\n",
    "\n",
    "        result = minimize(objective_function, params, args=(a,c,beta,gamma,x_data,y_data), bounds=[(0, np.inf)], tol = 1e-10, options={'eps': 1e-10, 'ftol': 1e-15}) \n",
    "        params = result.x\n",
    "        err = result.fun\n",
    "        success = result.success\n",
    "        message = result.message\n",
    "        print(f'{i+1} {list(df_data3.index)[i+1]} {params[0]:.5f} {err:.2e} {success} {message}')\n",
    "        opt_dict[i+1] = {'optimized_alpha':params[0],'optimized_err':err, 'success':success,'message':message}\n",
    "\n",
    "    opt_df = pd.DataFrame.from_dict(opt_dict).T\n",
    "    opt_df['month'] = list(df_data3.index)\n",
    "    my_file = \"opt_df_b.csv\"   \n",
    "    opt_df.to_csv(os.path.join(my_path_, my_file),index=False) \n",
    "model_b()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b4e4c2c-160d-401b-be31-2e9f2d833623",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_model_b(df,ylabel,title,color,log=False):\n",
    "    plt.style.use(\"dark_background\")\n",
    "    fig, ax = plt.subplots(figsize=(15, 5))   \n",
    "    if log:\n",
    "        ax.semilogy(list(df['month']), df[ylabel], \"o-\", color=color, markersize=3)\n",
    "    else:\n",
    "        ax.plot(list(df['month']), df[ylabel], \"o-\", color=color, markersize=3)\n",
    "    ax.axvline(pd.Timestamp(2020, 3, 1),color='r')\n",
    "    plt.grid(True, linewidth=0.5)\n",
    "    ax.ticklabel_format(axis='y')\n",
    "    ax.set_xlabel('month',size=20)\n",
    "    ax.set_title(title,size=30)\n",
    "my_file = \"opt_df_b.csv\" \n",
    "opt_df = pd.read_csv(os.path.join(my_path_, my_file))\n",
    "opt_df['month'] = opt_df['month'].apply(pd.to_datetime)\n",
    "plot_model_b(opt_df,'optimized_alpha','Optimized alpha','orange')\n",
    "plot_model_b(opt_df,'optimized_err','Optimized error','blue')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3bbb501-3f45-4525-8e6d-182e1d6bae9b",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "### Simulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7266dc7-427f-4e25-8b9b-ff1cfb37c64c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def simulation_b():\n",
    "    c = 100 \n",
    "    a = 0.5 \n",
    "    \n",
    "    my_file = \"opt_df.csv\" \n",
    "    opt_df = pd.read_csv(os.path.join(my_path_, my_file))\n",
    "    opt_df['month'] = opt_df['month'].apply(pd.to_datetime)\n",
    "    beta = opt_df['optimized_beta'].mean()\n",
    "    gamma = opt_df['optimized_gamma'].mean()\n",
    "    \n",
    "    my_file = \"opt_df_b.csv\" \n",
    "    opt_df = pd.read_csv(os.path.join(my_path_, my_file))\n",
    "    opt_df['month'] = opt_df['month'].apply(pd.to_datetime)\n",
    "    \n",
    "    my_file = \"dfs_2authors.pickle\"\n",
    "    with open(os.path.join(my_path_, my_file),\"rb\") as fp:\n",
    "        [works_2authors,works,works_authors_aff] = pickle.load(fp)  \n",
    "    works = works.loc['2000-01-01':'2023-12-01'] \n",
    "    works_authors_aff = works_authors_aff.loc['2000-01-01':'2023-12-01']\n",
    "    N_dict = works_authors_aff.groupby('publication_date_1').work_id.nunique().to_frame().to_dict()['work_id']\n",
    "    months_list = list(N_dict.keys())\n",
    "    months_list.sort()\n",
    "    months_list = [i.strftime('%Y-%m-%d') for i in months_list]\n",
    "    start_index = 0\n",
    "    end_index = 120 #180\n",
    "    start_month = months_list[start_index]\n",
    "    end_month = months_list[end_index-1]\n",
    "    my_file = \"df_strengths0.csv\"     \n",
    "    df_0 = pd.read_csv(os.path.join(my_path_, my_file))\n",
    "    my_file = \"inst_set.pickle\"\n",
    "    with open(os.path.join(my_path_, my_file),\"rb\") as fp:\n",
    "        inst_set = pickle.load(fp)       \n",
    "    \n",
    "    #calculate #each month: F_INTRA\n",
    "    my_file = \"df_data1.csv\"    \n",
    "    df_data1 = pd.read_csv(os.path.join(my_path_, my_file))\n",
    "    df_data1 = df_data1.set_index('publication_date_1')\n",
    "    df_data3 = df_data1\n",
    "    df_data3 = df_data3[['total']]\n",
    "    df_data3['total'] = df_data3['total'].astype(int)\n",
    "    df_data3 = df_data3.loc[months_list[end_index]:months_list[-1]]\n",
    "    df_data3_list = list(df_data3['total'])\n",
    "    df_data1 = df_data1[['frac_intra']]\n",
    "    df_data1 = df_data1.rename(columns={'frac_intra':'F_aff'})\n",
    "    df_data1 = df_data1.loc[months_list[end_index]:months_list[-1]] #[months_list[end_index-1]:months_list[-1]]\n",
    "    #each month: average distance\n",
    "    my_file = \"work_edges_dist_mean_monthly.csv\"    \n",
    "    df_data2 = pd.read_csv(os.path.join(my_path_, my_file))\n",
    "    df_data2 = df_data2.set_index('publication_date_1').loc[months_list[end_index]:months_list[-1]]\n",
    "    \n",
    "    my_file = \"I_dist_threshold.csv\"\n",
    "    I_dist = read_parquet(basepath / 'I_dist_threshold')\n",
    "    I_dist = I_dist[['source','target','dist']].reset_index(drop=True)\n",
    "    I_dist = I_dist.query('source.isin(@inst_set) & target.isin(@inst_set)')\n",
    "    \n",
    "    df_intra = I_dist[I_dist.source == I_dist.target].reset_index(drop=True)\n",
    "    df_inter = I_dist[I_dist.source != I_dist.target].reset_index(drop=True)\n",
    "    df_inter.index += len(df_intra)\n",
    "\n",
    "    import random\n",
    "    random.seed(0)\n",
    "    for s in tqdm(range(10)):\n",
    "\n",
    "        df_ = df_0\n",
    "\n",
    "        fra_intra_list = []\n",
    "        mean_dist_list = []\n",
    "        for i in tqdm(range(len(df_data3_list))):\n",
    "\n",
    "            #update \n",
    "            alpha = list(opt_df['optimized_alpha'])[i]\n",
    "            \n",
    "            inst_str_dict = df_[['institution_id','strength']].set_index('institution_id').to_dict()['strength']\n",
    "\n",
    "            df_intra['m_source'] = df_intra['source'].map(inst_str_dict)\n",
    "            df_inter['m_source'] = df_inter['source'].map(inst_str_dict)\n",
    "            df_inter['m_target'] = df_inter['target'].map(inst_str_dict)\n",
    "            df_inter['m_prod'] = (df_inter['m_source']*df_inter['m_target'])**a         \n",
    "\n",
    "            #edges probabilities\n",
    "            df_intra['p']= gamma*df_intra['m_source'] \n",
    "            df_inter['p'] = df_inter['m_prod'] * ( beta / ((df_inter['dist']+c)**alpha)) \n",
    "\n",
    "            hyperedges_probabilities = list(itertools.chain(df_intra['p'], df_inter['p']))\n",
    "            hyperedges_probabilities = np.array(hyperedges_probabilities)/(df_intra['p'].sum()+df_inter['p'].sum())\n",
    "            W = int(1e5) #df_data3_list[i] \n",
    "            hyperedges_model = random.choices(np.arange(0, len(hyperedges_probabilities)), weights=hyperedges_probabilities, k=W)\n",
    "\n",
    "            #count edges\n",
    "            counter = dict(collections.Counter(hyperedges_model))\n",
    "            df_intra['count'] = df_intra.index.to_series().map(counter)\n",
    "            df_intra['count'] = df_intra['count'].fillna(0)\n",
    "            df_inter['count'] = df_inter.index.to_series().map(counter)\n",
    "            df_inter['count'] = df_inter['count'].fillna(0)           \n",
    "            \n",
    "            #count intra-inter\n",
    "            frac_intra = sum(df_intra['count'])/(df_intra['count'].sum()+df_inter['count'].sum())\n",
    "            fra_intra_list.append(frac_intra)\n",
    "\n",
    "            #mean team distace\n",
    "            df_inter['mean_dist'] = df_inter['dist']*df_inter['count']\n",
    "            mean_dist = df_inter.mean_dist.sum()/W\n",
    "            mean_dist_list.append(mean_dist)\n",
    "\n",
    "            #update strength \n",
    "            df_intra['count'] = df_intra['count']*2 \n",
    "            temp = pd.concat([\n",
    "                df_intra[['source','count']].rename(columns={'source':'institution_id'}),\n",
    "                df_inter[['source','count']].rename(columns={'source':'institution_id'}),\n",
    "                df_inter[['target','count']].rename(columns={'target':'institution_id'})])\n",
    "            temp = temp.groupby('institution_id')['count'].sum().to_frame().reset_index()\n",
    "            df_old = df_\n",
    "            df_ = df_[['institution_id','strength']]\n",
    "            df_ = df_.merge(temp,on='institution_id',how='left')\n",
    "            df_['count'] = df_['count'].fillna(0)\n",
    "            df_['strength'] = df_['strength'] + df_['count']\n",
    "            df_ = df_[['institution_id','strength']]\n",
    "\n",
    "        model_data = pd.DataFrame.from_dict({'fra_intra':fra_intra_list,'mean_dist':mean_dist_list})\n",
    "        model_data['month'] = months_list[end_index:]\n",
    "        my_file = \"simulation_\"+str(s)+\"_b.csv\" \n",
    "        model_data.to_csv(os.path.join(my_path_, my_file))\n",
    "simulation_b()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0bf5a70-c543-477f-b6fb-f04a7b7a1999",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_simulation_b():\n",
    "    \n",
    "    my_file = \"simulation_\"+str(0)+\"_b.csv\"\n",
    "    model_data = pd.read_csv(os.path.join(my_path_, my_file))\n",
    "    model_data['month'] = pd.to_datetime(model_data['month'])\n",
    "    months_list = list(model_data['month'])\n",
    "    months_list.sort()\n",
    "    months_list = [i.strftime('%Y-%m-%d') for i in months_list]\n",
    "    \n",
    "    #calculate #each month: F_INTRA\n",
    "    my_file = \"df_data1.csv\"    \n",
    "    df_data1 = pd.read_csv(os.path.join(my_path_, my_file))\n",
    "    df_data1 = df_data1.set_index('publication_date_1')\n",
    "    df_data1 = df_data1[['frac_intra']]\n",
    "    df_data1 = df_data1.rename(columns={'frac_intra':'F_aff'})\n",
    "    df_data1 = df_data1.loc[months_list] #[months_list[end_index]:months_list[-1]]\n",
    "    #each month: average distance\n",
    "    my_file = \"work_edges_dist_mean_monthly.csv\"    \n",
    "    work_edges_dist_mean_monthly = pd.read_csv(os.path.join(my_path_, my_file))\n",
    "    work_edges_dist_mean_monthly = work_edges_dist_mean_monthly.set_index('publication_date_1')\n",
    "    work_edges_dist_mean_monthly = work_edges_dist_mean_monthly.loc[months_list]\n",
    "    df_data2 = work_edges_dist_mean_monthly\n",
    "\n",
    "    for s in range(10):\n",
    "        my_file = \"simulation_\"+str(s)+\"_b.csv\" \n",
    "        model_data = pd.read_csv(os.path.join(my_path_, my_file))\n",
    "        model_data['month'] = pd.to_datetime(model_data['month'])\n",
    "        months_list = list(model_data['month'])\n",
    "        months_list.sort()\n",
    "        months_list = [i.strftime('%Y-%m-%d') for i in months_list]\n",
    "\n",
    "        plt.style.use(\"dark_background\")\n",
    "        fig, ax = plt.subplots(figsize=(15, 5))\n",
    "        x_dates = list(model_data['month'])\n",
    "        x_data = x_dates\n",
    "        y_data1 = list(df_data1.F_aff)   \n",
    "        y_data2 = list(model_data['fra_intra'])\n",
    "        ax.plot(x_data, y_data1, \"o-\", markersize=3,label='data')\n",
    "        ax.plot(x_data, y_data2, \"o-\", markersize=3,label='model')\n",
    "        ax.axvline(pd.Timestamp(2020, 3, 1),color='r')\n",
    "        plt.grid(True, linewidth=0.5)\n",
    "        ax.yaxis.set_major_formatter(formatter)\n",
    "        ax.set_xlabel('month',size=20)\n",
    "        ax.legend() \n",
    "        ax.set_title('Frac intra-insts collabs - with exact params',size=30)\n",
    "        \n",
    "    for s in range(10):\n",
    "        my_file = \"simulation_\"+str(s)+\"_b.csv\" \n",
    "        model_data = pd.read_csv(os.path.join(my_path_, my_file))\n",
    "        model_data['month'] = pd.to_datetime(model_data['month'])\n",
    "        months_list = list(model_data['month'])\n",
    "        months_list.sort()\n",
    "        months_list = [i.strftime('%Y-%m-%d') for i in months_list]\n",
    "\n",
    "        plt.style.use(\"dark_background\")\n",
    "        fig, ax = plt.subplots(figsize=(15, 5))\n",
    "        x_data = x_dates\n",
    "        y_data1 = list(df_data2.dist)   \n",
    "        y_data2 = list(model_data['mean_dist'])\n",
    "        ax.plot(x_data, y_data1, \"o-\", markersize=3,label='data')\n",
    "        ax.plot(x_data, y_data2, \"o-\", markersize=3,label='model')\n",
    "        ax.axvline(pd.Timestamp(2020, 3, 1),color='r')\n",
    "        plt.grid(True, linewidth=0.5)\n",
    "        ax.yaxis.set_major_formatter(formatter)\n",
    "        ax.set_xlabel('month',size=20)\n",
    "        ax.legend() \n",
    "        ax.set_title('Avg team dist - with exact params',size=30)\n",
    "plot_simulation_b()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25bebe4a-8b2f-4ea7-9d75-4f9d5d921f7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259513e1-61c2-4a03-be27-4781cf667dee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daddee1e-eb75-471a-b25b-50b74ae319f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800cf1f0-2502-4cdb-b5b6-db46fafc0051",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4a5084-f65a-4e90-8958-66ec14003538",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
