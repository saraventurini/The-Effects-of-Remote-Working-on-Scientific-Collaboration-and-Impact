{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b47db8b8-d1f3-4a77-8fe0-e60e23ff6774",
   "metadata": {},
   "source": [
    "# Model - info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30397a63-4e1c-43f7-acbb-029c82751e59",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from pathlib import Path\n",
    "import networkx as nx\n",
    "import igraph as ig\n",
    "import pickle\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.spatial import distance\n",
    "import seaborn as sns\n",
    "import time\n",
    "from tqdm.auto import tqdm\n",
    "import random \n",
    "import os\n",
    "from itertools import chain, combinations\n",
    "import itertools\n",
    "import scipy\n",
    "\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "import plotly.io as pio\n",
    "plt.rcParams.update({'font.size': 22})\n",
    "sns.set(style=\"ticks\", context=\"talk\")\n",
    "plt.style.use(\"dark_background\")\n",
    "pd.options.plotting.backend = 'plotly'\n",
    "pio.templates.default = 'plotly_dark+presentation'\n",
    "\n",
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "#pd.options.mode.chained_assignment = None \n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "import matplotlib.dates as dates\n",
    "import matplotlib.dates as mdates\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "from scipy import optimize\n",
    "import numpy.polynomial.polynomial as npoly\n",
    "\n",
    "def form(x,pos):\n",
    "    if x<1e3:\n",
    "        return '%1.3f' % (x)\n",
    "    elif x<1e6:\n",
    "        return '%1.1fK' % (x * 1e-3)\n",
    "    else:\n",
    "        return '%1.1fM' % (x * 1e-6)\n",
    "formatter = FuncFormatter(form)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def read_parquet(name, **args):\n",
    "    path = name\n",
    "    print(f'Reading {name!r}')\n",
    "    tic = time.time()\n",
    "    df = pd.read_parquet(path, engine='fastparquet', **args)\n",
    "    before = len(df)\n",
    "    # df.drop_duplicates(inplace=True)\n",
    "    toc = time.time()\n",
    "    after = len(df)\n",
    "        \n",
    "    print(f'Read {len(df):,} rows from {path.stem!r} in {toc-tic:.2f} sec. {before-after:,} duplicates.')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168738bd-f73f-4c28-b1e0-979fc86d9231",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def my_weight(G, u, v, weight=\"weight\"):\n",
    "    w = 0\n",
    "    for nbr in set(G[u]) & set(G[v]):\n",
    "        w += (G[u][nbr].get(weight, 1) + G[v][nbr].get(weight, 1))/2\n",
    "    return w\n",
    "def make_institution_graph(works_authors_rows):\n",
    "    \n",
    "    institution_id_set = set(works_authors_rows.institution_id)\n",
    "                                  \n",
    "    bip_g = nx.from_pandas_edgelist(\n",
    "        works_authors_rows,\n",
    "        source='work_id', target='institution_id', edge_attr ='weight'\n",
    "    )\n",
    "\n",
    "    inst_graph = nx.bipartite.generic_weighted_projected_graph(bip_g,nodes=institution_id_set,weight_function=my_weight)    \n",
    "    #inst_graph = nx.bipartite.weighted_projected_graph(bip_g,nodes=institution_id_set) \n",
    "\n",
    "    return inst_graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab4a43f-7d48-48ee-ba88-9acbaeb6f007",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "basepath = Path('./Tables_final') \n",
    "my_path_ = Path('./Model_info')\n",
    "if not os.path.exists(my_path_):\n",
    "    os.makedirs(my_path_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f8e97eb-1f45-484d-be22-9c278ad914ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f030651a-6081-4098-8b3d-b9fae9318283",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-12T10:13:49.784563Z",
     "iopub.status.busy": "2024-09-12T10:13:49.784196Z",
     "iopub.status.idle": "2024-09-12T10:13:49.787698Z",
     "shell.execute_reply": "2024-09-12T10:13:49.787422Z",
     "shell.execute_reply.started": "2024-09-12T10:13:49.784547Z"
    }
   },
   "source": [
    "## Perc. works resp. team size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "892e35b3-72c6-493b-8889-c36f167f6571",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "works = read_parquet(basepath / 'works')\n",
    "\n",
    "#TW training window\n",
    "months_list = list(set(works.reset_index().drop_duplicates('publication_date_1').publication_date_1))\n",
    "months_list.sort()\n",
    "months_list = [i.strftime('%Y-%m-%d') for i in months_list]\n",
    "start_index = 0\n",
    "end_index = 120 #180\n",
    "start_month = months_list[start_index]#.strftime('%Y-%m-%d')\n",
    "end_month = months_list[end_index-1]#.strftime('%Y-%m-%d')\n",
    "print(start_month,end_month)\n",
    "works = works[works.num_authors > 1]\n",
    "works_TW = works.loc[start_month:end_month]\n",
    "works_AW = works.loc[months_list[end_index]:]\n",
    "\n",
    "df_perc_works = ((works.groupby('num_authors').work_id.count().to_frame().cumsum()/len(works))*100).rename(columns={'work_id':'perc_works'})\n",
    "display(df_perc_works)\n",
    "df_perc_works = ((works_AW.groupby('num_authors').work_id.count().to_frame().cumsum()/len(works_AW))*100).rename(columns={'work_id':'perc_works'})\n",
    "display(df_perc_works)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916a1428-5cbd-4031-8b12-b502b35a6e34",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fb567e3f-e915-413f-8feb-61ad6c2368c6",
   "metadata": {},
   "source": [
    "## Edges - 2 authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d741f25c-2488-464c-8697-990420b61fcd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "path_2authors = Path('./Model_2authors') \n",
    "if not os.path.exists(path_2authors):\n",
    "    os.makedirs(path_2authors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "887185bf-b3d6-4739-952f-f49ced5bf2fb",
   "metadata": {},
   "source": [
    "### Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15fb3f87-624b-4f76-9351-de88899e315d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "works = read_parquet(basepath / 'works')\n",
    "works_authors_aff = read_parquet(basepath / 'works_authors_aff')\n",
    "\n",
    "works = works.loc['2000-01-01':'2023-12-01'] \n",
    "works = works[works.num_authors>1]\n",
    "\n",
    "works_2authors = set(works[works.num_authors ==2].work_id)\n",
    "print(f'{len(works_2authors)} ({(len(works_2authors)/len(works))*100:.2f}%) works 2 authors')\n",
    "\n",
    "works = works[works.work_id.isin(works_2authors)]\n",
    "works_authors_aff = works_authors_aff[works_authors_aff.work_id.isin(works_2authors)]\n",
    "\n",
    "my_file = \"dfs_2authors.pickle\"\n",
    "pickle.dump([works_2authors,works,works_authors_aff], open(os.path.join(path_2authors, my_file), 'wb')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f6a0891-8661-4eeb-87db-691955e0a9b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2287b237-5415-4fe5-9c37-b941f7f9433e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#INITIALIZATION #preferential attachment  #TW: 2000-2009\n",
    "my_file = \"dfs_2authors.pickle\"\n",
    "with open(os.path.join(path_2authors, my_file),\"rb\") as fp:\n",
    "    [works_2authors,works,works_authors_aff] = pickle.load(fp)  \n",
    "works = works.loc['2000-01-01':'2023-12-01'] \n",
    "works_authors_aff = works_authors_aff.loc['2000-01-01':'2023-12-01']\n",
    "\n",
    "N_dict = works_authors_aff.groupby('publication_date_1').work_id.nunique().to_frame().to_dict()['work_id']\n",
    "months_list = list(N_dict.keys())\n",
    "months_list.sort()\n",
    "months_list = [i.strftime('%Y-%m-%d') for i in months_list]\n",
    "\n",
    "start_index = 0\n",
    "end_index = 120 #180\n",
    "start_month = months_list[start_index]#.strftime('%Y-%m-%d')\n",
    "end_month = months_list[end_index-1]#.strftime('%Y-%m-%d')\n",
    "\n",
    "df_TW = works_authors_aff.loc[start_month:end_month] #df_TW = works_authors_aff.loc[months_list[:120]]\n",
    "inst_set = set(df_TW.institution_id)\n",
    "I = len(inst_set)\n",
    "print(f'TW from {start_month} to {end_month} : {I} institutions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6b463e-7298-4fd4-94ca-68fcdc1210a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#initial strenghts    #consider only institutions in sample in TW\n",
    "#count number (unique) institutions per paper\n",
    "df_TW = df_TW.drop_duplicates(['work_id','institution_id'])\n",
    "df_TW['num_affs'] = df_TW.groupby('work_id')['institution_id'].transform('size')\n",
    "print(f'{min(df_TW.num_affs)}-{max(df_TW.num_affs)} min-max number (unique) affiliations per work')\n",
    "df_TW['weight'] = 2 / ( df_TW['num_affs']*(df_TW['num_affs']-1) ) \n",
    "df_TW.loc[df_TW.num_affs==1,'weight'] = 1 #one affiliation\n",
    "df_TW_noloops = df_TW[df_TW.num_affs>1]\n",
    "df_TW_loops = df_TW[df_TW.num_affs==1]\n",
    "df_TW_loops['institution_id2'] = df_TW_loops['institution_id']\n",
    "df_TW_loops['weight'] = df_TW_loops[['institution_id','institution_id2','weight']].groupby(['institution_id']).weight.transform('sum')\n",
    "df_TW_loops = df_TW_loops.drop_duplicates('institution_id')\n",
    "I_graph = make_institution_graph(df_TW_noloops)\n",
    "I_graph.add_weighted_edges_from([tuple(r) for r in df_TW_loops[['institution_id','institution_id2','weight']].to_numpy()])\n",
    "df_ = pd.DataFrame.from_dict(dict(I_graph.degree(weight='weight')),orient='index').reset_index().rename(columns={'index':'institution_id',0:'strength'})\n",
    "df_['institution_id'] = df_['institution_id'].astype(int)\n",
    "df = df_.sort_values(by='strength',ascending=False)\n",
    "inst_set = set(df.institution_id)\n",
    "I = len(inst_set)\n",
    "print(f'TW from {start_month} to {end_month} : {I} institutions')\n",
    "\n",
    "my_file = \"df_strengths0.csv\"     \n",
    "df.to_csv(os.path.join(path_2authors, my_file),index=False)\n",
    "\n",
    "my_file = \"inst_set.pickle\"\n",
    "pickle.dump(inst_set, open(os.path.join(path_2authors, my_file), 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87247b2d-a449-4fec-bbe9-68e1f180223c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "I_dist = read_parquet(basepath / 'I_dist_threshold')\n",
    "I_dist = I_dist[['source','target','dist']].reset_index(drop=True)\n",
    "I_dist = I_dist.query('source.isin(@inst_set) & target.isin(@inst_set)').reset_index()\n",
    "my_file = \"I_dist_model.csv\" \n",
    "I_dist.to_csv(os.path.join(path_2authors, my_file),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4e1a49-316c-471c-8e78-e42dea60a170",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cdd5ceb0-7142-4286-8e15-ca6e87f3930e",
   "metadata": {},
   "source": [
    "### Data plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86acb54d-9446-42b2-9703-7eaeb54b4ca3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "my_file = \"dfs_2authors.pickle\"\n",
    "with open(os.path.join(path_2authors, my_file),\"rb\") as fp:\n",
    "    [works_2authors,works,works_authors_aff] = pickle.load(fp)  \n",
    "\n",
    "N_dict = works_authors_aff.groupby('publication_date_1').work_id.nunique().to_frame().to_dict()['work_id']\n",
    "\n",
    "months_list = list(N_dict.keys())\n",
    "months_list.sort()\n",
    "months_list = [i.strftime('%Y-%m-%d') for i in months_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b789998-ab46-4857-b620-b71d92017475",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_fit_rolling_breakpoints_2(df,x_column,x_column2,x_label,title,window_size,num_breakpoints):\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(15, 5))\n",
    "    x_dates = list(df['publication_date_1'])\n",
    "    y_data = df[x_column].rolling(window=window_size).mean()[window_size-1:]\n",
    "    y_data2 = df[x_column2].rolling(window=window_size).mean()[window_size-1:]\n",
    "    x_data = x_dates[window_size-1:]\n",
    "    ax.set_ylim([0, 1])\n",
    "    \n",
    "    ax.plot(x_data, y_data, \"o-\", color='orange', markersize=3,label='intra-institution')\n",
    "    ax.plot(x_data, y_data2, \"o-\", color='green', markersize=3,label='inter-institution')\n",
    "\n",
    "    plt.grid(True, linewidth=0.5)\n",
    "    ax.yaxis.set_major_formatter(formatter)\n",
    "\n",
    "    ax.set_xlabel(x_label,size=20)\n",
    "    #ax.set_title(title,size=30)\n",
    "\n",
    "    #ax.xaxis.set_major_locator(mdates.MonthLocator()) # Make ticks on occurrences of each month\n",
    "    ax.xaxis.set_major_formatter(mdates.DateFormatter('%b %y')) # Get only the month to show in the x-axis\n",
    "\n",
    "    ax.axvline(pd.Timestamp(2020, 3, 1),color='r') #ax.axvline(x_dates[84],color='r')\n",
    "\n",
    "    #save for all possible combination of breakpoints the correspondent error \n",
    "    def f(breakpoints, x, y, fcache): \n",
    "        breakpoints = tuple(map(int, sorted(breakpoints)))\n",
    "        if breakpoints not in fcache:\n",
    "            total_error = 0\n",
    "            for f, xi, yi in find_best_piecewise_polynomial(breakpoints, x, y):\n",
    "                total_error += ((f(xi) - yi)**2).sum()\n",
    "            fcache[breakpoints] = total_error \n",
    "        # print('{} --> {}'.format(breakpoints, fcache[breakpoints]))\n",
    "        return fcache[breakpoints]\n",
    "\n",
    "    def find_best_piecewise_polynomial(breakpoints, x, y):\n",
    "        breakpoints = tuple(map(int, sorted(breakpoints)))\n",
    "        xs = np.split(x, breakpoints)\n",
    "        ys = np.split(y, breakpoints)\n",
    "        result = []\n",
    "        for xi, yi in zip(xs, ys):\n",
    "            if len(xi) < 2: continue\n",
    "            coefs = npoly.polyfit(xi, yi, 1)\n",
    "            f = npoly.Polynomial(coefs)\n",
    "            result.append([f, xi, yi])\n",
    "        return result\n",
    "    \n",
    "    x_num = dates.date2num(x_data)\n",
    "    breakpoints = optimize.brute(f, [slice(1, len(x_data), 1)]*num_breakpoints, args=(x_num, y_data, {}), finish=None)\n",
    "    if num_breakpoints==1:\n",
    "        breakpoints = [breakpoints] \n",
    "    for f, xi, yi in find_best_piecewise_polynomial(breakpoints, x_num, y_data):\n",
    "        xi_min = x_data[np.where(x_num == xi.min())[0][0]]\n",
    "        xi_max = x_data[np.where(x_num == xi.max())[0][0]]\n",
    "        x_interval = np.array([xi_min, xi_max]) \n",
    "        #print('y = {:35s}, if x in [{}, {}]'.format(str(f), *x_interval))\n",
    "        x_interval = np.array([xi.min(), xi.max()])\n",
    "        #ax.plot(x_interval, f(x_interval), 'yo-')\n",
    "        coef = f.convert().coef\n",
    "        b = coef[0]\n",
    "        a = coef[1]\n",
    "        if b>0:\n",
    "            ll = 'y = {:.2f} x + {:.0f}'.format(a,b)\n",
    "        else: #b<0\n",
    "            ll = 'y = {:.2f} x - {:.0f}'.format(a,abs(b))    \n",
    "        ax.plot(x_interval, f(x_interval), 'yo-',linewidth=2, markersize=7)       \n",
    "    #save for all possible combination of breakpoints the correspondent error\n",
    "    \n",
    "    def f(breakpoints, x, y, fcache): \n",
    "        breakpoints = tuple(map(int, sorted(breakpoints)))\n",
    "        if breakpoints not in fcache:\n",
    "            total_error = 0\n",
    "            for f, xi, yi in find_best_piecewise_polynomial(breakpoints, x, y):\n",
    "                total_error += ((f(xi) - yi)**2).sum()\n",
    "            fcache[breakpoints] = total_error \n",
    "        # print('{} --> {}'.format(breakpoints, fcache[breakpoints]))\n",
    "        return fcache[breakpoints]\n",
    "\n",
    "    def find_best_piecewise_polynomial(breakpoints, x, y):\n",
    "        breakpoints = tuple(map(int, sorted(breakpoints)))\n",
    "        xs = np.split(x, breakpoints)\n",
    "        ys = np.split(y, breakpoints)\n",
    "        result = []\n",
    "        for xi, yi in zip(xs, ys):\n",
    "            if len(xi) < 2: continue\n",
    "            coefs = npoly.polyfit(xi, yi, 1)\n",
    "            f = npoly.Polynomial(coefs)\n",
    "            result.append([f, xi, yi])\n",
    "        return result\n",
    "    \n",
    "    x_num = dates.date2num(x_data)\n",
    "    breakpoints = optimize.brute(f, [slice(1, len(x_data), 1)]*num_breakpoints, args=(x_num, y_data2, {}), finish=None)\n",
    "    if num_breakpoints==1:\n",
    "        breakpoints = [breakpoints] \n",
    "    for f, xi, yi in find_best_piecewise_polynomial(breakpoints, x_num, y_data2):\n",
    "        xi_min = x_data[np.where(x_num == xi.min())[0][0]]\n",
    "        xi_max = x_data[np.where(x_num == xi.max())[0][0]]\n",
    "        x_interval = np.array([xi_min, xi_max]) \n",
    "        #print('y = {:35s}, if x in [{}, {}]'.format(str(f), *x_interval))\n",
    "        x_interval = np.array([xi.min(), xi.max()])\n",
    "        #ax.plot(x_interval, f(x_interval), 'yo-')\n",
    "        coef = f.convert().coef\n",
    "        b = coef[0]\n",
    "        a = coef[1]\n",
    "        if b>0:\n",
    "            ll = 'y = {:.2f} x + {:.0f}'.format(a,b)\n",
    "        else: #b<0\n",
    "            ll = 'y = {:.2f} x - {:.0f}'.format(a,abs(b))    \n",
    "        ax.plot(x_interval, f(x_interval), 'yo-',linewidth=2, markersize=7) \n",
    "       \n",
    "    ax.legend()        \n",
    "    ax.set_title(title,size=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3dbc24-24f2-4fae-86d4-f16d0b2ac7d3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.dates as dates\n",
    "import matplotlib.dates as mdates\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "from scipy import optimize\n",
    "import numpy.polynomial.polynomial as npoly\n",
    "\n",
    "def form(x,pos):\n",
    "    if x<1e3:\n",
    "        return '%1.3f' % (x)\n",
    "    elif x<1e6:\n",
    "        return '%1.1fK' % (x * 1e-3)\n",
    "    else:\n",
    "        return '%1.1fM' % (x * 1e-6)\n",
    "formatter = FuncFormatter(form)\n",
    "\n",
    "def plot_fit_rolling_breakpoints(df,x_column,x_label,title,window_size,num_breakpoints,ff):\n",
    "    \n",
    "    #save for all possible combination of breakpoints the correspondent error \n",
    "    def f(breakpoints, x, y, fcache): \n",
    "        breakpoints = tuple(map(int, sorted(breakpoints)))\n",
    "        if breakpoints not in fcache:\n",
    "            total_error = 0\n",
    "            for f, xi, yi in find_best_piecewise_polynomial(breakpoints, x, y):\n",
    "                total_error += ((f(xi) - yi)**2).sum()\n",
    "            fcache[breakpoints] = total_error \n",
    "        # print('{} --> {}'.format(breakpoints, fcache[breakpoints]))\n",
    "        return fcache[breakpoints]\n",
    "\n",
    "    def find_best_piecewise_polynomial(breakpoints, x, y):\n",
    "        breakpoints = tuple(map(int, sorted(breakpoints)))\n",
    "        xs = np.split(x, breakpoints)\n",
    "        ys = np.split(y, breakpoints)\n",
    "        result = []\n",
    "        for xi, yi in zip(xs, ys):\n",
    "            if len(xi) < 2: continue\n",
    "            coefs = npoly.polyfit(xi, yi, 1)\n",
    "            f = npoly.Polynomial(coefs)\n",
    "            result.append([f, xi, yi])\n",
    "        return result\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(15, 5))\n",
    "    x_dates = list(df['publication_date_1'])\n",
    "    y_data = df[x_column].rolling(window=window_size).mean()[window_size-1:]\n",
    "    x_data = x_dates[window_size-1:]\n",
    "    \n",
    "    ax.plot(x_data, y_data, \"o-\", color='orange', markersize=3)\n",
    "\n",
    "    plt.grid(True, linewidth=0.5)\n",
    "    ax.yaxis.set_major_formatter(formatter)\n",
    "\n",
    "    ax.set_xlabel(x_label,size=20)\n",
    "    ax.set_title(title,size=30)\n",
    "\n",
    "    ax.xaxis.set_major_formatter(mdates.DateFormatter('%b %y')) # Get only the month to show in the x-axis\n",
    "\n",
    "    ax.axvline(pd.Timestamp(2020, 3, 1),color='r') #ax.axvline(x_dates[84],color='r')\n",
    "\n",
    "    x_num = dates.date2num(x_data)\n",
    "    breakpoints = optimize.brute(f, [slice(1, len(x_data), 1)]*num_breakpoints, args=(x_num, y_data, {}), finish=None)\n",
    "    if num_breakpoints==1:\n",
    "        breakpoints = [breakpoints] \n",
    "\n",
    "    for f, xi, yi in find_best_piecewise_polynomial(breakpoints, x_num, y_data):\n",
    "        xi_min = x_data[np.where(x_num == xi.min())[0][0]]\n",
    "        xi_max = x_data[np.where(x_num == xi.max())[0][0]] \n",
    "        x_interval = np.array([xi.min(), xi.max()])\n",
    "        coef = f.convert().coef\n",
    "        b = coef[0]\n",
    "        a = coef[1]\n",
    "        if ff==1:\n",
    "            if b>0:\n",
    "                ll = 'y = {:.2f} x + {:.2f}'.format(a,b)\n",
    "            else: #b<0\n",
    "                ll = 'y = {:.2f} x - {:.2f}'.format(a,abs(b))    \n",
    "        else:\n",
    "            if b>0:\n",
    "                ll = 'y = {:.5f} x + {:.5f}'.format(a,b)\n",
    "            else: #b<0\n",
    "                ll = 'y = {:.5f} x - {:.5f}'.format(a,abs(b))  \n",
    "        #x_1 = x_dates[window_size-1:][np.where(x_num==x_interval[0])[0][0]]\n",
    "        #x_3 = x_dates[window_size-1:][np.where(x_num==x_interval[1])[0][0]]\n",
    "        ax.plot(x_interval, f(x_interval), 'o-',color='yellow',label=ll, markersize=6)\n",
    "        \n",
    "    ax.legend()\n",
    "    ax.set_title(title,size=30)\n",
    "    #return x_num \n",
    "    \n",
    "x_label='Month'\n",
    "window_size = 6\n",
    "num_breakpoints = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0ff384-c144-4c01-9c7b-99629d85124f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def df_data1_(work_authors_edges_df):\n",
    "    #work_authors_edges_df['intra'] = 0\n",
    "    #work_authors_edges_df.loc[work_authors_edges_df.source_inst == work_authors_edges_df.target_inst,'intra'] = 1\n",
    "    df1 = work_authors_edges_df.groupby('publication_date_1').intra.count().reset_index().rename(columns={'intra':'total'})\n",
    "    df2 = work_authors_edges_df.groupby('publication_date_1').intra.sum().to_frame().reset_index()\n",
    "    intra_inter_df = df1.merge(df2,on='publication_date_1')\n",
    "    intra_inter_df['inter'] = intra_inter_df['total'] - intra_inter_df['intra']\n",
    "    intra_inter_df['frac_intra'] = intra_inter_df['intra'] / intra_inter_df['total']\n",
    "    intra_inter_df['frac_inter'] = intra_inter_df['inter'] / intra_inter_df['total']\n",
    "    df_data1 = intra_inter_df\n",
    "    return df_data1\n",
    "\n",
    "def work_edges_dist_mean_monthly_(works_outside,works_set,path):\n",
    "    my_file = \"work_edges_dist_mean.csv\"    \n",
    "    work_edges_dist_mean = pd.read_csv(os.path.join(path, my_file))\n",
    "    work_edges_dist_mean = work_edges_dist_mean[work_edges_dist_mean.work_id.isin(works_set)]\n",
    "    work_edges_dist_mean = work_edges_dist_mean[~work_edges_dist_mean.work_id.isin(works_outside)]\n",
    "    work_edges_dist_mean['publication_date_1'] = pd.to_datetime(work_edges_dist_mean['publication_date_1'])\n",
    "    work_edges_dist_mean_monthly = work_edges_dist_mean.groupby('publication_date_1').dist.mean().to_frame().reset_index()\n",
    "    return work_edges_dist_mean_monthly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd100669-78ff-47b9-a01f-9d5c409b33c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa7e6fe-6584-405e-9c2e-3511e89b1bff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "work_authors_edges_df_dist = read_parquet(Path('./TeamDistance') / 'work_authors_edges_df_dist')\n",
    "work_authors_edges_df_dist = works[['work_id']].reset_index().merge(work_authors_edges_df_dist,on='work_id')\n",
    "work_authors_edges_df_dist = work_authors_edges_df_dist[work_authors_edges_df_dist.work_id.isin(works_2authors)]\n",
    "work_edges_dist_mean = work_authors_edges_df_dist.groupby('work_id').dist.mean().to_frame().reset_index()\n",
    "work_edges_dist_mean = work_edges_dist_mean.merge(works[['work_id']].reset_index(),on='work_id')\n",
    "work_edges_dist_mean_monthly = work_edges_dist_mean.groupby('publication_date_1').dist.mean().to_frame().reset_index()\n",
    "my_file = \"work_edges_dist_mean.csv\"    \n",
    "work_edges_dist_mean.to_csv(os.path.join(path_2authors, my_file),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a685475-5956-4e4a-b34f-2e00b2d41891",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "my_file = \"work_edges_dist_mean_monthly.csv\"     \n",
    "work_edges_dist_mean_monthly.to_csv(os.path.join(path_2authors, my_file),index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af606fd6-66ab-490d-98e9-f8f32108879c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_data1 = df_data1_(work_authors_edges_df_dist)\n",
    "df_data1['publication_date_1'] = df_data1['publication_date_1'].apply(pd.to_datetime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "754abee2-95c6-455c-83c8-044cdf8fa13f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "be586f30-7af2-4942-9a5e-f272b3c9ae5b",
   "metadata": {},
   "source": [
    "### Power-law"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4ef6b1-019b-481c-b0c4-01ab52eb1284",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def my_weight(G, u, v, weight=\"weight\"):\n",
    "    w = 0\n",
    "    for nbr in set(G[u]) & set(G[v]):\n",
    "        w += (G[u][nbr].get(weight, 1) + G[v][nbr].get(weight, 1))/2\n",
    "    return w\n",
    "\n",
    "def make_institution_graph(works_authors_rows):\n",
    "\n",
    "    institution_id_set = set(works_authors_rows.institution_id)\n",
    "\n",
    "    bip_g = nx.from_pandas_edgelist(\n",
    "        works_authors_rows,\n",
    "        source='work_id', target='institution_id', edge_attr ='weight'\n",
    "    )\n",
    "\n",
    "    inst_graph = nx.bipartite.generic_weighted_projected_graph(bip_g,nodes=institution_id_set,weight_function=my_weight)    \n",
    "    #inst_graph = nx.bipartite.weighted_projected_graph(bip_g,nodes=institution_id_set) \n",
    "\n",
    "    return inst_graph\n",
    "\n",
    "def fit_data(n,n_authors,my_path_):\n",
    "    if n==0:\n",
    "        my_file_add = \"\"\n",
    "    else:\n",
    "        my_file_add = \"_tail\"+str(n)\n",
    "        \n",
    "    my_file = \"inst_set.pickle\"\n",
    "    with open(os.path.join(path_2authors, my_file),\"rb\") as fp:\n",
    "        inst_set = pickle.load(fp)\n",
    "    my_file = \"dfs_\"+str(n_authors)+\"authors.pickle\"\n",
    "    with open(os.path.join(my_path_, my_file),\"rb\") as fp:\n",
    "        [works_2authors,works,works_authors_aff] = pickle.load(fp) \n",
    "    if n>0:\n",
    "        works = works.loc['2000-01-01':'2023-12-01'] \n",
    "        works_authors_aff = works_authors_aff.loc['2000-01-01':'2023-12-01']\n",
    "        inst_set = set(works_authors_aff.institution_id)\n",
    "        N_dict = works_authors_aff.groupby('publication_date_1').work_id.nunique().to_frame().to_dict()['work_id']\n",
    "        months_list = list(N_dict.keys())\n",
    "        months_list.sort()\n",
    "        months_list = [i.strftime('%Y-%m-%d') for i in months_list]\n",
    "        start_index = 0\n",
    "        end_index = 120 #180\n",
    "        start_month = months_list[start_index]#.strftime('%Y-%m-%d')\n",
    "        end_month = months_list[end_index-1]#.strftime('%Y-%m-%d')\n",
    "        df_TW = works_authors_aff.loc[start_month:end_month] #df_TW = works_authors_aff.loc[months_list[:120]]\n",
    "        df_ = df_TW.groupby('institution_id').work_id.count().to_frame().sort_values(by='work_id',ascending=False).reset_index().rename(columns={'work_id':'strength'})\n",
    "        inst_set_s = set((df_[df_.strength>=n]).institution_id)\n",
    "        #restrict to papers not including authors from institutions outside sample\n",
    "        inst_set_s_compl = inst_set - inst_set_s\n",
    "        works_outside = set(works_authors_aff[works_authors_aff.institution_id.isin(inst_set_s_compl)].work_id)\n",
    "        works = works[~works.work_id.isin(works_outside)]\n",
    "        works_authors_aff = works_authors_aff[~works_authors_aff.work_id.isin(works_outside)]\n",
    "\n",
    "    works = works.reset_index()\n",
    "    works['publication_year'] = works['publication_date_1'].dt.year    \n",
    "    works_yearly_count = works.groupby('publication_year').work_id.count().to_frame()#.reset_index()\n",
    "    works = works.set_index('publication_year')\n",
    "    works_authors_aff = works_authors_aff.drop_duplicates(['work_id','institution_id']).reset_index()\n",
    "    works_authors_aff['publication_year'] = works_authors_aff['publication_date_1'].dt.year\n",
    "    works_authors_aff['publication_year'] = works_authors_aff['publication_year'].astype('int64')\n",
    "    works_authors_aff['institution_id'] = works_authors_aff['institution_id'].astype('int64')\n",
    "    works_authors_aff = works_authors_aff.sort_values(by='publication_year')\n",
    "    works_authors_aff['num_affs'] = works_authors_aff.groupby('work_id')['institution_id'].transform('size')\n",
    "    print(f'{min(works_authors_aff.num_affs)}-{max(works_authors_aff.num_affs)} min-max number (unique) affiliations per work')\n",
    "    works_authors_aff['weight'] = 2 / ( works_authors_aff['num_affs']*(works_authors_aff['num_affs']-1) ) \n",
    "    works_authors_aff.loc[works_authors_aff.num_affs==1,'weight'] = 1 #one affiliation\n",
    "\n",
    "    works_authors_aff_noloops = works_authors_aff[works_authors_aff.num_affs>1]\n",
    "    works_authors_aff_loops = works_authors_aff[works_authors_aff.num_affs==1]\n",
    "    works_authors_aff_loops['institution_id2'] = works_authors_aff_loops['institution_id']\n",
    "    works_authors_aff_noloops = works_authors_aff_noloops.set_index('publication_year')\n",
    "    works_authors_aff_loops = works_authors_aff_loops.set_index('publication_year')\n",
    "    \n",
    "    I_dist = read_parquet(basepath / 'I_dist_threshold')\n",
    "    I_dist = I_dist[['source','target','dist']].reset_index(drop=True)\n",
    "\n",
    "    years_list = list(set(works_authors_aff.publication_year))\n",
    "    years_list.sort()\n",
    "    \n",
    "    works_yearly_count = works_yearly_count.loc[2000:2023]\n",
    "    print(f\"Total number preprints (from {min(works_yearly_count.index)} to {max(works_yearly_count.index)}): {works_yearly_count.work_id.sum()}\")\n",
    "    start_year_0 = 2000\n",
    "    end_year_0 = 2009\n",
    "    works_yearly_count_0 = works_yearly_count.loc[start_year_0:end_year_0]\n",
    "    print(f\"Number preprints (from {start_year_0} to {end_year_0}): {works_yearly_count_0.work_id.sum()}\")\n",
    "    start_year_1 = 2010\n",
    "    end_year_1 = 2014\n",
    "    works_yearly_count_1 = works_yearly_count.loc[start_year_1:end_year_1]\n",
    "    print(f\"Number preprints (from {start_year_1} to {end_year_1}): {works_yearly_count_1.work_id.sum()}\")\n",
    "    start_year_2 = 2015\n",
    "    end_year_2 = 2019\n",
    "    works_yearly_count_2 = works_yearly_count.loc[start_year_2:end_year_2]\n",
    "    print(f\"Number preprints (from {start_year_2} to {end_year_2}): {works_yearly_count_2.work_id.sum()}\")\n",
    "    start_year_3 = 2020\n",
    "    end_year_3 = 2021\n",
    "    works_yearly_count_3 = works_yearly_count.loc[start_year_3:end_year_3]\n",
    "    print(f\"Number preprints (from {start_year_3} to {end_year_3}): {works_yearly_count_3.work_id.sum()}\")\n",
    "    start_year_4 = 2022\n",
    "    end_year_4 = 2023\n",
    "    works_yearly_count_4 = works_yearly_count.loc[start_year_4:end_year_4]\n",
    "    print(f\"Number preprints (from {start_year_4} to {end_year_4}): {works_yearly_count_4.work_id.sum()}\")\n",
    "    \n",
    "    works_0 = works.loc[start_year_0:end_year_0]\n",
    "    works_set_0 = set(works_0.work_id)\n",
    "    works_1 = works.loc[start_year_1:end_year_1]\n",
    "    works_set_1 = set(works_1.work_id)\n",
    "    works_2 = works.loc[start_year_2:end_year_2]\n",
    "    works_set_2 = set(works_2.work_id)\n",
    "    works_3 = works.loc[start_year_3:end_year_3]\n",
    "    works_set_3 = set(works_3.work_id)\n",
    "    works_4 = works.loc[start_year_4:end_year_4]\n",
    "    works_set_4 = set(works_4.work_id)\n",
    "\n",
    "    works_authors_aff_noloops_0 = works_authors_aff_noloops.loc[start_year_0:end_year_0]\n",
    "    works_authors_aff_loops_0 = works_authors_aff_loops.loc[start_year_0:end_year_0]\n",
    "    works_authors_aff_noloops_1 = works_authors_aff_noloops.loc[start_year_1:end_year_1]\n",
    "    works_authors_aff_loops_1 = works_authors_aff_loops.loc[start_year_1:end_year_1]\n",
    "    works_authors_aff_noloops_2 = works_authors_aff_noloops.loc[start_year_2:end_year_2]\n",
    "    works_authors_aff_loops_2 = works_authors_aff_loops.loc[start_year_2:end_year_2]\n",
    "    works_authors_aff_noloops_3 = works_authors_aff_noloops.loc[start_year_3:end_year_3]\n",
    "    works_authors_aff_loops_3 = works_authors_aff_loops.loc[start_year_3:end_year_3]\n",
    "    works_authors_aff_noloops_4 = works_authors_aff_noloops.loc[start_year_4:end_year_4]\n",
    "    works_authors_aff_loops_4 = works_authors_aff_loops.loc[start_year_4:end_year_4]\n",
    "\n",
    "    works_authors_aff_noloops_0['period'] = 0\n",
    "    works_authors_aff_noloops_1['period'] = 1\n",
    "    works_authors_aff_noloops_2['period'] = 2\n",
    "    works_authors_aff_noloops_3['period'] = 3\n",
    "    works_authors_aff_noloops_4['period'] = 4\n",
    "    works_authors_aff_loops_0['period'] = 0\n",
    "    works_authors_aff_loops_1['period'] = 1\n",
    "    works_authors_aff_loops_2['period'] = 2\n",
    "    works_authors_aff_loops_3['period'] = 3\n",
    "    works_authors_aff_loops_4['period'] = 4\n",
    "    works_authors_aff_noloops_periods = pd.concat([works_authors_aff_noloops_0,works_authors_aff_noloops_1])\n",
    "    works_authors_aff_noloops_periods = pd.concat([works_authors_aff_noloops_periods,works_authors_aff_noloops_2])\n",
    "    works_authors_aff_noloops_periods = pd.concat([works_authors_aff_noloops_periods,works_authors_aff_noloops_3])\n",
    "    works_authors_aff_noloops_periods = pd.concat([works_authors_aff_noloops_periods,works_authors_aff_noloops_4])\n",
    "    works_authors_aff_loops_periods = pd.concat([works_authors_aff_loops_0,works_authors_aff_loops_1])\n",
    "    works_authors_aff_loops_periods = pd.concat([works_authors_aff_loops_periods,works_authors_aff_loops_2])\n",
    "    works_authors_aff_loops_periods = pd.concat([works_authors_aff_loops_periods,works_authors_aff_loops_3])\n",
    "    works_authors_aff_loops_periods = pd.concat([works_authors_aff_loops_periods,works_authors_aff_loops_4])\n",
    "\n",
    "    for y in tqdm(range(5)):\n",
    "        noloops_df = works_authors_aff_noloops_periods.query(\"period == @y\")\n",
    "        loops_df = works_authors_aff_loops_periods.query(\"period == @y\")\n",
    "        loops_df['weight'] = loops_df[['institution_id','institution_id2','weight']].groupby(['institution_id']).weight.transform('sum')\n",
    "        loops_df = loops_df.drop_duplicates('institution_id')\n",
    "        I = make_institution_graph(noloops_df)\n",
    "        I.add_weighted_edges_from([tuple(r) for r in loops_df[['institution_id','institution_id2','weight']].to_numpy()])\n",
    "        my_file = \"I_period\"+str(y)+my_file_add+\".pickle\"\n",
    "        pickle.dump(I, open(os.path.join(my_path_, my_file), 'wb'))             \n",
    "        #only loops\n",
    "        I_loops = nx.Graph()\n",
    "        I_loops.add_weighted_edges_from([tuple(r) for r in loops_df[['institution_id','institution_id2','weight']].to_numpy()])\n",
    "        my_file = \"Iloops_period\"+str(y)+my_file_add+\".pickle\"\n",
    "        pickle.dump(I_loops, open(os.path.join(my_path_, my_file), 'wb'))\n",
    "\n",
    "    info_df = pd.DataFrame()\n",
    "    degree = {}\n",
    "    for y in tqdm(range(5)):\n",
    "        my_file = \"I_period\"+str(y)+my_file_add+\".pickle\"\n",
    "        with open(os.path.join(my_path_, my_file),\"rb\") as fp:\n",
    "            I_year = pickle.load(fp)\n",
    "        my_file = \"Iloops_period\"+str(y)+my_file_add+\".pickle\"\n",
    "        with open(os.path.join(my_path_, my_file),\"rb\") as fp:\n",
    "            Iloops_year = pickle.load(fp)\n",
    "        info_y = pd.DataFrame.from_dict({'period':[y], 'N': [I_year.number_of_nodes()], 'E': [I_year.size()], 'W' : [I_year.size(weight='weight')], 'Nloops': [Iloops_year.number_of_nodes()], 'Eloops': [Iloops_year.size()], 'Wloops' : [Iloops_year.size(weight='weight')]})\n",
    "        info_df = pd.concat([info_df,info_y])    \n",
    "        degree[y] = {'k': I_year.degree(), 's' : I_year.degree(weight='weight'),'kloops': Iloops_year.degree(), 'sloops' : Iloops_year.degree(weight='weight')}\n",
    "    my_file = \"info_period_df\"+my_file_add+\".csv\"\n",
    "    info_df.to_csv(os.path.join(my_path_, my_file),index=False)    \n",
    "    my_file = \"degree_period\"+my_file_add+\".pickle\"\n",
    "    pickle.dump(degree, open(os.path.join(my_path_, my_file), 'wb'))\n",
    "\n",
    "    degree_df = pd.DataFrame()\n",
    "    for y in tqdm(range(5)):\n",
    "        dict_y = degree[y]\n",
    "        df_k = pd.DataFrame.from_dict(dict_y['k']).rename(columns={0:'institution_id',1:'k'})\n",
    "        df_s = pd.DataFrame.from_dict(dict_y['s']).rename(columns={0:'institution_id',1:'s'})\n",
    "        df_kloops = pd.DataFrame.from_dict(dict_y['kloops']).rename(columns={0:'institution_id',1:'kloops'})\n",
    "        df_sloops = pd.DataFrame.from_dict(dict_y['sloops']).rename(columns={0:'institution_id',1:'sloops'})\n",
    "        if len(df_kloops)!=0:\n",
    "            df_year = (df_k.merge(df_s,on='institution_id')).merge(df_kloops.merge(df_sloops,on='institution_id'),on='institution_id',how='left')\n",
    "        else:\n",
    "            df_year = df_k.merge(df_s,on='institution_id')\n",
    "            df_year[\"kloops\"] = np.nan\n",
    "            df_year[\"sloops\"] = np.nan\n",
    "        df_year.insert(0, 'period', y)\n",
    "        degree_df = pd.concat([degree_df,df_year])\n",
    "    degree_df = degree_df.fillna(0)\n",
    "    my_file = \"degree_df\"+my_file_add+\".csv\"    \n",
    "    degree_df.to_csv(os.path.join(my_path_, my_file),index=False) \n",
    "\n",
    "    edges_df = pd.DataFrame()\n",
    "    for y in tqdm(range(5)):\n",
    "        my_file = \"I_period\"+str(y)+my_file_add+\".pickle\"\n",
    "        with open(os.path.join(my_path_, my_file),\"rb\") as fp:\n",
    "            I_year = pickle.load(fp)\n",
    "        I_year_df = nx.to_pandas_edgelist(I_year)\n",
    "        I_year_df = I_year_df.merge(I_dist,on=['source','target'],how='left')\n",
    "        I_year_df.insert(0, 'period', y)\n",
    "        edges_df = pd.concat([edges_df,I_year_df])\n",
    "    my_file = \"edges_df\"+my_file_add+\".csv\"    \n",
    "    edges_df.to_csv(os.path.join(my_path_, my_file),index=False) \n",
    "\n",
    "    edges_comp_df = pd.DataFrame()\n",
    "    for y in tqdm(range(5)):\n",
    "        my_file = \"I_period\"+str(y)+my_file_add+\".pickle\" \n",
    "        with open(os.path.join(my_path_, my_file),\"rb\") as fp:\n",
    "            I = pickle.load(fp)\n",
    "        I_comp = nx.complement(I) #20 mins\n",
    "        # my_file = \"I_period\"+str(y)+\"_comp.pickle\"\n",
    "        # pickle.dump(I_comp, open(os.path.join(my_path_, my_file), 'wb')) \n",
    "        I_comp_df = nx.to_pandas_edgelist(I_comp)\n",
    "        I_comp_df = I_comp_df.merge(I_dist,on=['source','target'])\n",
    "        I_comp_df.insert(0, 'period', y)\n",
    "        edges_comp_df = pd.concat([edges_comp_df,I_comp_df])    \n",
    "    my_file = \"edges_comp_df\"+my_file_add+\".csv\"    \n",
    "    edges_comp_df.to_csv(os.path.join(my_path_, my_file),index=False)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06662591-210b-4b9c-a4bc-5178f74617bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fit_data(0,2,path_2authors)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eade90f0-1a91-4e4a-814d-e0f46c0debcc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fit_data(20,2,path_2authors)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ffbc024-660f-49c8-ab59-e850471c2245",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fit_data(50,2,path_2authors)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4916f09-7ee6-4a4b-956e-5452f89213ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7362e3b-ac37-4712-a52e-577ab8de5594",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plots(n,my_path_):\n",
    "    if n==0:\n",
    "        my_file_add = \"\"\n",
    "    else:\n",
    "        my_file_add = \"_tail\"+str(n)    \n",
    "    \n",
    "    periods_list = [0,1,2,3,4]\n",
    "    periods_labels = ['[2000,2009]','[2010,2014]','[2015,2019]','[2020,2021]','[2022,2023]']\n",
    "    color_dict = {0: 'yellow', 1: 'magenta', 2: 'mediumseagreen', 3: 'cyan', 4: 'orange'}\n",
    "    \n",
    "    my_file = \"edges_df\"+my_file_add+\".csv\"\n",
    "    edges_df = pd.read_csv(my_path_ / my_file) \n",
    "    my_file = \"degree_df\"+my_file_add+\".csv\"\n",
    "    degree_df = pd.read_csv(my_path_ / my_file)  \n",
    "    my_file = \"edges_comp_df\"+my_file_add+\".csv\"\n",
    "    edges_comp_df = pd.read_csv(my_path_ / my_file) \n",
    "    edges_comp_df['weight'] = 0\n",
    "    edges_df_tot = pd.concat([edges_df,edges_comp_df])\n",
    "    \n",
    "    def mean_scatter_plot_multi(periods_list,periods_labels,color_dict,func,edges_df,degree_df,col,columnx,columny,labelx,labely,title,logx=False,logy=False,limity=False,geomspace=False):\n",
    "        fig, ax = plt.subplots(figsize=(8,5))\n",
    "        for y in periods_list[0:]:\n",
    "            df1 = func(edges_df,degree_df,col,y)\n",
    "            x1 = np.array(df1[columnx]) \n",
    "            y1 = np.array(df1[columny])  \n",
    "            # Define the grid\n",
    "            gridsize = 10**2\n",
    "            if geomspace:\n",
    "                xbins1 = np.geomspace(x1.min(), x1.max(), gridsize)\n",
    "                #xbins2 = np.geomspace(x2.min(), x2.max(), gridsize)\n",
    "            else:\n",
    "                xbins1 = np.linspace(x1.min(), x1.max(), gridsize)\n",
    "                #xbins2 = np.linspace(x2.min(), x2.max(), gridsize)\n",
    "            # Calculate the mean values within each column\n",
    "            mean_values1 = []\n",
    "            for i in range(len(xbins1) - 1):\n",
    "                mask = (x1 >= xbins1[i]) & (x1 < xbins1[i + 1])\n",
    "                mean_y1 = np.mean(y1[mask])\n",
    "                mean_values1.append(mean_y1) \n",
    "            ax.plot((xbins1[:-1] + xbins1[1:]) / 2, mean_values1, '.',color=color_dict[y],markersize=5, label=periods_labels[y])          \n",
    "        ax.set_xlabel(labelx,size=20)\n",
    "        ax.set_ylabel(labely,size=20)\n",
    "        ax.set_title(title,size=30)\n",
    "        if logx==True:\n",
    "            ax.set_xscale('log')  \n",
    "        if logy==True:\n",
    "            ax.set_yscale('log')\n",
    "        if limity==True:\n",
    "            ax.set_ylim([0, 1])  \n",
    "        ax.legend(bbox_to_anchor=(1.0, 0.9), prop={'size': 15}, markerscale=3)\n",
    "        plt.show() \n",
    "    def scipy_fun_multi(periods_list,periods_labels,color_dict,func,edges_df,degree_df,col,x_max,labelx,labely,title):\n",
    "        fig, ax = plt.subplots(figsize=(8,5))  \n",
    "        for y in periods_list[0:]:\n",
    "            x1,y1 = func(edges_df,degree_df,col,y)\n",
    "            y1 = np.array(y1)\n",
    "            popt1, pcov1 = scipy.optimize.curve_fit(linear, x1[x1<=x_max], y1[x1<=x_max])\n",
    "            perr1 = np.sqrt(np.diag(pcov1))\n",
    "            print(f'{periods_labels[y]}: gamma {popt1[0]} (perr {perr1[0]:.4f})') \n",
    "            ax.scatter(x1[x1<=x_max], y1[x1<=x_max],marker='.',color=color_dict[y],linewidths=0.01, label=periods_labels[y]) \n",
    "            ax.plot(x1[x1<=x_max], linear(x1[x1<=x_max], *popt1),color=color_dict[y],linewidth=2.0)   \n",
    "        ax.set_xlabel(labelx,size=20)\n",
    "        ax.set_ylabel(labely,size=20)\n",
    "        ax.legend(bbox_to_anchor=(1.0, 0.9), prop={'size': 15}, markerscale=3)\n",
    "        ax.set_title(title+\" - linear\",size=30)\n",
    "        plt.show()\n",
    "    def linear(x, b):\n",
    "        return  b * x \n",
    "    def linear_fit(edges_df,degree_df,col,yy):\n",
    "        df1 = fig_2B(edges_df,degree_df,col,yy)\n",
    "        x1 = np.array(df1[\"s\"]) \n",
    "        y1 = np.array(df1[col])  \n",
    "        gridsize = 10**2\n",
    "        xbins1 = np.linspace(x1.min(), x1.max(), gridsize)    \n",
    "        mean_values1 = []\n",
    "        for i in range(len(xbins1) - 1):\n",
    "            mask = (x1 >= xbins1[i]) & (x1 < xbins1[i + 1])\n",
    "            mean_y1 = np.mean(y1[mask])\n",
    "            mean_values1.append(mean_y1)     \n",
    "        x = (xbins1[:-1] + xbins1[1:]) / 2\n",
    "        y = np.array(mean_values1)\n",
    "        x = x[~np.isnan(y)]\n",
    "        y = y[~np.isnan(y)]\n",
    "        return x,y\n",
    "    \n",
    "    def fig_2B(edges_df,degree_df,col,y):\n",
    "        edges_year_df = edges_df.query('period == @y')\n",
    "        loops_w_count = edges_year_df[edges_year_df.source == edges_year_df.target][['source',col]].rename(columns={'source':'institution_id'})\n",
    "        #degree_year_df = (degree_df[degree_df.period<=y][['institution_id','s']]).groupby(['institution_id']).s.sum().to_frame().reset_index()\n",
    "        degree_year_df = degree_df.query('period == @y')\n",
    "        loops_w_count = loops_w_count.merge(degree_year_df,on='institution_id')\n",
    "        return loops_w_count\n",
    "    \n",
    "    mean_scatter_plot_multi(periods_list,periods_labels,color_dict,fig_2B,edges_df_tot,degree_df,\"weight\",\"s\",\"weight\",\"s_i\",\"w_ii\",'Fig. 2B',False,False,False,False)\n",
    "    scipy_fun_multi(periods_list,periods_labels,color_dict,linear_fit,edges_df_tot,degree_df,\"weight\",2*10**4,\"s_i\",\"w_ii\",\"Fig. 2B\")\n",
    "\n",
    "    def fig_4B(df1,df2,col,y):\n",
    "        df1 = df1.query('period == @y')[['source','target',col,'dist']]\n",
    "        df2 = df2.query('period == @y')[['institution_id','s']]\n",
    "        df2_dict = df2.set_index('institution_id').to_dict()['s']\n",
    "        df1['source_s'] = df1['source'].map(df2_dict)\n",
    "        df1['target_s'] = df1['target'].map(df2_dict)\n",
    "        df1['ratio'] = df1[col] / (df1['source_s']*df1['target_s'])**(1/2)\n",
    "        df1 = df1[[\"dist\", \"ratio\"]]\n",
    "        #delate zero\n",
    "        df1 = df1[df1.dist>0]\n",
    "        return df1\n",
    "    def scipy_fun_multi(periods_list,periods_labels,color_dict,func,edges_df,degree_df,col,x_min,labelx,labely,title):\n",
    "        fig, ax = plt.subplots(figsize=(8,5))  \n",
    "        for y in periods_list[0:]:\n",
    "            x1,y1 = func(edges_df,degree_df,col,y)\n",
    "            y1 = np.array(y1)\n",
    "            popt1, pcov1 = scipy.optimize.curve_fit(power2, x1[x1>=x_min], y1[x1>=x_min])\n",
    "            perr1 = np.sqrt(np.diag(pcov1))\n",
    "            print(f'{periods_labels[y]}: alpha {(-1)*popt1[1]:.3f} (perr {perr1[1]:.4f}), beta {popt1[0]:.6f} (perr {perr1[0]:.4f})') \n",
    "            ax.scatter(x1[x1>=x_min], y1[x1>=x_min],marker='.',color=color_dict[y],linewidths=0.01, label=periods_labels[y]) \n",
    "            ax.plot(x1[x1>=x_min], power2(x1[x1>=x_min], *popt1),color=color_dict[y],linewidth=2.0)   \n",
    "        ax.set_xscale('log')  \n",
    "        ax.set_yscale('log')\n",
    "        ax.set_xlabel(labelx,size=20)\n",
    "        ax.set_ylabel(labely,size=20)\n",
    "        ax.legend(bbox_to_anchor=(1.0, 0.9), prop={'size': 15}, markerscale=3)\n",
    "        ax.set_title(title+\" - power law\",size=30)\n",
    "        plt.show()\n",
    "    def power2(x, b, c):\n",
    "        return  b * x ** c \n",
    "    def powerlaw_fit(edges_df,degree_df,col,yy):\n",
    "        df = fig_4B(edges_df,degree_df,col,yy)\n",
    "        x = np.array(df[\"dist\"]) \n",
    "        y = np.array(df[\"ratio\"])\n",
    "        gridsize = 10**2\n",
    "        xbins = np.geomspace(x.min(), x.max(), gridsize)\n",
    "        # Calculate the mean values within each column\n",
    "        mean_values = []\n",
    "        for i in range(len(xbins) - 1):\n",
    "            mask = (x >= xbins[i]) & (x < xbins[i + 1])\n",
    "            mean_y = np.mean(y[mask])\n",
    "            mean_values.append(mean_y) \n",
    "        x = (xbins[:-1] + xbins[1:]) / 2\n",
    "        y = np.array(mean_values)\n",
    "        return x,y\n",
    "    \n",
    "    mean_scatter_plot_multi(periods_list,periods_labels,color_dict,fig_4B,edges_df_tot,degree_df,\"weight\",\"dist\", \"ratio\",\"d_ij\",\"w_ij / (s_i*s_j)^(1/2) \",'Fig. 4B',True,True,False,True)\n",
    "    scipy_fun_multi(periods_list,periods_labels,color_dict,powerlaw_fit,edges_df_tot,degree_df,\"weight\",10**1,\"d_ij\",\"w_ij / (s_i*s_j)^(1/2) \",\"Fig. 4B - power law\")\n",
    "    scipy_fun_multi(periods_list,periods_labels,color_dict,powerlaw_fit,edges_df_tot,degree_df,\"weight\",10**2,\"d_ij\",\"w_ij / (s_i*s_j)^(1/2) \",\"Fig. 4B - power law\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a1ab83f-4848-40ce-87b3-802e63e9aaab",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "plots(0,path_2authors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82432b3b-caef-484d-b128-1bb84a34eb5f",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "plots(20,path_2authors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96794402-f91a-46c6-9e65-4f9855a72456",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "plots(50,path_2authors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0750bd7a-3b14-4360-962f-a73df2785185",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcae9e76-917f-48fd-8bb5-b8c0517a9d8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c637e60c-003f-41a3-bf24-fdfeebfa32ac",
   "metadata": {},
   "source": [
    "## Edges - 3 authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "266af36d-7bb4-4907-9872-1dc093280668",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "path_3authors = Path('./Model_3authors') \n",
    "if not os.path.exists(path_3authors):\n",
    "    os.makedirs(path_3authors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c525c24-3515-44fc-a32d-2421a8ec8805",
   "metadata": {},
   "source": [
    "### Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2e7fb7-6a4f-4c0f-8d26-582231b49202",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "works = read_parquet(basepath / 'works')\n",
    "works_authors_aff = read_parquet(basepath / 'works_authors_aff')\n",
    "\n",
    "works = works.loc['2000-01-01':'2023-12-01'] \n",
    "works = works[works.num_authors>1]\n",
    "\n",
    "works_3authors = set(works[works.num_authors ==3].work_id)\n",
    "print(f'{len(works_3authors)} ({(len(works_3authors)/len(works))*100:.2f}%) works 3 authors')\n",
    "\n",
    "works = works[works.work_id.isin(works_3authors)]\n",
    "works_authors_aff = works_authors_aff[works_authors_aff.work_id.isin(works_3authors)]\n",
    "\n",
    "my_file = \"dfs_3authors.pickle\"\n",
    "pickle.dump([works_3authors,works,works_authors_aff], open(os.path.join(path_3authors, my_file), 'wb')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308f957e-9ca4-40a5-b842-bdc0b0c082f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#INITIALIZATION #preferential attachment  #TW: 2000-2009\n",
    "my_file = \"dfs_3authors.pickle\"\n",
    "with open(os.path.join(path_3authors, my_file),\"rb\") as fp:\n",
    "    [works_3authors,works,works_authors_aff] = pickle.load(fp)  \n",
    "works = works.loc['2000-01-01':'2023-12-01'] \n",
    "works_authors_aff = works_authors_aff.loc['2000-01-01':'2023-12-01']\n",
    "\n",
    "N_dict = works_authors_aff.groupby('publication_date_1').work_id.nunique().to_frame().to_dict()['work_id']\n",
    "months_list = list(N_dict.keys())\n",
    "months_list.sort()\n",
    "months_list = [i.strftime('%Y-%m-%d') for i in months_list]\n",
    "\n",
    "start_index = 0\n",
    "end_index = 120 #180\n",
    "start_month = months_list[start_index]#.strftime('%Y-%m-%d')\n",
    "end_month = months_list[end_index-1]#.strftime('%Y-%m-%d')\n",
    "\n",
    "df_TW = works_authors_aff.loc[start_month:end_month] #df_TW = works_authors_aff.loc[months_list[:120]]\n",
    "inst_set = set(df_TW.institution_id)\n",
    "I = len(inst_set)\n",
    "print(f'TW from {start_month} to {end_month} : {I} institutions')\n",
    "# my_file = \"inst_set.pickle\"\n",
    "# pickle.dump(inst_set, open(os.path.join(path_3authors, my_file), 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77a8b3b-82d6-4cba-8e16-576f7002cc47",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#initial strenghts    #consider only institutions in sample in TW\n",
    "#count number (unique) institutions per paper\n",
    "df_TW = df_TW.drop_duplicates(['work_id','institution_id'])\n",
    "df_TW['num_affs'] = df_TW.groupby('work_id')['institution_id'].transform('size')\n",
    "print(f'{min(df_TW.num_affs)}-{max(df_TW.num_affs)} min-max number (unique) affiliations per work')\n",
    "df_TW['weight'] = 2 / ( df_TW['num_affs']*(df_TW['num_affs']-1) ) \n",
    "df_TW.loc[df_TW.num_affs==1,'weight'] = 1 #one affiliation\n",
    "df_TW_noloops = df_TW[df_TW.num_affs>1]\n",
    "df_TW_loops = df_TW[df_TW.num_affs==1]\n",
    "df_TW_loops['institution_id2'] = df_TW_loops['institution_id']\n",
    "df_TW_loops['weight'] = df_TW_loops[['institution_id','institution_id2','weight']].groupby(['institution_id']).weight.transform('sum')\n",
    "df_TW_loops = df_TW_loops.drop_duplicates('institution_id')\n",
    "I_graph = make_institution_graph(df_TW_noloops)\n",
    "I_graph.add_weighted_edges_from([tuple(r) for r in df_TW_loops[['institution_id','institution_id2','weight']].to_numpy()])\n",
    "df_ = pd.DataFrame.from_dict(dict(I_graph.degree(weight='weight')),orient='index').reset_index().rename(columns={'index':'institution_id',0:'strength'})\n",
    "df_['institution_id'] = df_['institution_id'].astype(int)\n",
    "df = df_.sort_values(by='strength',ascending=False)\n",
    "inst_set = set(df.institution_id)\n",
    "I = len(inst_set)\n",
    "print(f'TW from {start_month} to {end_month} : {I} institutions')\n",
    "\n",
    "my_file = \"df_strengths0.csv\"     \n",
    "df.to_csv(os.path.join(path_3authors, my_file),index=False)\n",
    "\n",
    "my_file = \"inst_set.pickle\"\n",
    "pickle.dump(inst_set, open(os.path.join(path_3authors, my_file), 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa3be0cb-595d-4eed-b478-5f09c2a096dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "I_dist = read_parquet(basepath / 'I_dist_threshold')\n",
    "I_dist = I_dist[['source','target','dist']].reset_index(drop=True)\n",
    "I_dist_sort = [tuple(row) for row in I_dist.itertuples(index=False)]\n",
    "I_dist_sort = [ sorted(list(x)[:2])+[list(x)[2]] for x in I_dist_sort]\n",
    "I_dist_sort = pd.DataFrame(I_dist_sort, columns = ['source', 'target', 'dist'])\n",
    "my_file = \"I_dist_model.csv\" \n",
    "I_dist_sort.to_csv(os.path.join(path_3authors, my_file),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bec08cd-ec31-45dd-9ad4-48d6b60ae171",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "90bfcd94-fa87-4ab7-bb97-bab7bbf4d86e",
   "metadata": {},
   "source": [
    "### Data plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4253af72-1e31-49be-8c13-1376f0e0f103",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "my_file = \"dfs_3authors.pickle\"\n",
    "with open(os.path.join(path_3authors, my_file),\"rb\") as fp:\n",
    "    [works_3authors,works,works_authors_aff] = pickle.load(fp)  \n",
    "\n",
    "N_dict = works_authors_aff.groupby('publication_date_1').work_id.nunique().to_frame().to_dict()['work_id']\n",
    "\n",
    "months_list = list(N_dict.keys())\n",
    "months_list.sort()\n",
    "months_list = [i.strftime('%Y-%m-%d') for i in months_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0d838c-ddc5-4765-9548-d69404ad2f27",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_fit_rolling_breakpoints_2(df,x_column,x_column2,x_label,title,window_size,num_breakpoints):\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(15, 5))\n",
    "    x_dates = list(df['publication_date_1'])\n",
    "    y_data = df[x_column].rolling(window=window_size).mean()[window_size-1:]\n",
    "    y_data2 = df[x_column2].rolling(window=window_size).mean()[window_size-1:]\n",
    "    x_data = x_dates[window_size-1:]\n",
    "    ax.set_ylim([0, 1])\n",
    "    \n",
    "    ax.plot(x_data, y_data, \"o-\", color='orange', markersize=3,label='intra-institution')\n",
    "    ax.plot(x_data, y_data2, \"o-\", color='green', markersize=3,label='inter-institution')\n",
    "\n",
    "    plt.grid(True, linewidth=0.5)\n",
    "    ax.yaxis.set_major_formatter(formatter)\n",
    "\n",
    "    ax.set_xlabel(x_label,size=20)\n",
    "    #ax.set_title(title,size=30)\n",
    "\n",
    "    #ax.xaxis.set_major_locator(mdates.MonthLocator()) # Make ticks on occurrences of each month\n",
    "    ax.xaxis.set_major_formatter(mdates.DateFormatter('%b %y')) # Get only the month to show in the x-axis\n",
    "\n",
    "    ax.axvline(pd.Timestamp(2020, 3, 1),color='r') #ax.axvline(x_dates[84],color='r')\n",
    "\n",
    "    #save for all possible combination of breakpoints the correspondent error \n",
    "    def f(breakpoints, x, y, fcache): \n",
    "        breakpoints = tuple(map(int, sorted(breakpoints)))\n",
    "        if breakpoints not in fcache:\n",
    "            total_error = 0\n",
    "            for f, xi, yi in find_best_piecewise_polynomial(breakpoints, x, y):\n",
    "                total_error += ((f(xi) - yi)**2).sum()\n",
    "            fcache[breakpoints] = total_error \n",
    "        # print('{} --> {}'.format(breakpoints, fcache[breakpoints]))\n",
    "        return fcache[breakpoints]\n",
    "\n",
    "    def find_best_piecewise_polynomial(breakpoints, x, y):\n",
    "        breakpoints = tuple(map(int, sorted(breakpoints)))\n",
    "        xs = np.split(x, breakpoints)\n",
    "        ys = np.split(y, breakpoints)\n",
    "        result = []\n",
    "        for xi, yi in zip(xs, ys):\n",
    "            if len(xi) < 2: continue\n",
    "            coefs = npoly.polyfit(xi, yi, 1)\n",
    "            f = npoly.Polynomial(coefs)\n",
    "            result.append([f, xi, yi])\n",
    "        return result\n",
    "    \n",
    "    x_num = dates.date2num(x_data)\n",
    "    breakpoints = optimize.brute(f, [slice(1, len(x_data), 1)]*num_breakpoints, args=(x_num, y_data, {}), finish=None)\n",
    "    if num_breakpoints==1:\n",
    "        breakpoints = [breakpoints] \n",
    "    for f, xi, yi in find_best_piecewise_polynomial(breakpoints, x_num, y_data):\n",
    "        xi_min = x_data[np.where(x_num == xi.min())[0][0]]\n",
    "        xi_max = x_data[np.where(x_num == xi.max())[0][0]]\n",
    "        x_interval = np.array([xi_min, xi_max]) \n",
    "        #print('y = {:35s}, if x in [{}, {}]'.format(str(f), *x_interval))\n",
    "        x_interval = np.array([xi.min(), xi.max()])\n",
    "        #ax.plot(x_interval, f(x_interval), 'yo-')\n",
    "        coef = f.convert().coef\n",
    "        b = coef[0]\n",
    "        a = coef[1]\n",
    "        if b>0:\n",
    "            ll = 'y = {:.2f} x + {:.0f}'.format(a,b)\n",
    "        else: #b<0\n",
    "            ll = 'y = {:.2f} x - {:.0f}'.format(a,abs(b))    \n",
    "        ax.plot(x_interval, f(x_interval), 'yo-',linewidth=2, markersize=7)       \n",
    "    #save for all possible combination of breakpoints the correspondent error\n",
    "    \n",
    "    def f(breakpoints, x, y, fcache): \n",
    "        breakpoints = tuple(map(int, sorted(breakpoints)))\n",
    "        if breakpoints not in fcache:\n",
    "            total_error = 0\n",
    "            for f, xi, yi in find_best_piecewise_polynomial(breakpoints, x, y):\n",
    "                total_error += ((f(xi) - yi)**2).sum()\n",
    "            fcache[breakpoints] = total_error \n",
    "        # print('{} --> {}'.format(breakpoints, fcache[breakpoints]))\n",
    "        return fcache[breakpoints]\n",
    "\n",
    "    def find_best_piecewise_polynomial(breakpoints, x, y):\n",
    "        breakpoints = tuple(map(int, sorted(breakpoints)))\n",
    "        xs = np.split(x, breakpoints)\n",
    "        ys = np.split(y, breakpoints)\n",
    "        result = []\n",
    "        for xi, yi in zip(xs, ys):\n",
    "            if len(xi) < 2: continue\n",
    "            coefs = npoly.polyfit(xi, yi, 1)\n",
    "            f = npoly.Polynomial(coefs)\n",
    "            result.append([f, xi, yi])\n",
    "        return result\n",
    "    \n",
    "    x_num = dates.date2num(x_data)\n",
    "    breakpoints = optimize.brute(f, [slice(1, len(x_data), 1)]*num_breakpoints, args=(x_num, y_data2, {}), finish=None)\n",
    "    if num_breakpoints==1:\n",
    "        breakpoints = [breakpoints] \n",
    "    for f, xi, yi in find_best_piecewise_polynomial(breakpoints, x_num, y_data2):\n",
    "        xi_min = x_data[np.where(x_num == xi.min())[0][0]]\n",
    "        xi_max = x_data[np.where(x_num == xi.max())[0][0]]\n",
    "        x_interval = np.array([xi_min, xi_max]) \n",
    "        #print('y = {:35s}, if x in [{}, {}]'.format(str(f), *x_interval))\n",
    "        x_interval = np.array([xi.min(), xi.max()])\n",
    "        #ax.plot(x_interval, f(x_interval), 'yo-')\n",
    "        coef = f.convert().coef\n",
    "        b = coef[0]\n",
    "        a = coef[1]\n",
    "        if b>0:\n",
    "            ll = 'y = {:.2f} x + {:.0f}'.format(a,b)\n",
    "        else: #b<0\n",
    "            ll = 'y = {:.2f} x - {:.0f}'.format(a,abs(b))    \n",
    "        ax.plot(x_interval, f(x_interval), 'yo-',linewidth=2, markersize=7) \n",
    "       \n",
    "    ax.legend()        \n",
    "    ax.set_title(title,size=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db94191-efb1-4c54-ae78-eb88e9c4e61a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.dates as dates\n",
    "import matplotlib.dates as mdates\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "from scipy import optimize\n",
    "import numpy.polynomial.polynomial as npoly\n",
    "\n",
    "def form(x,pos):\n",
    "    if x<1e3:\n",
    "        return '%1.3f' % (x)\n",
    "    elif x<1e6:\n",
    "        return '%1.1fK' % (x * 1e-3)\n",
    "    else:\n",
    "        return '%1.1fM' % (x * 1e-6)\n",
    "formatter = FuncFormatter(form)\n",
    "\n",
    "def plot_fit_rolling_breakpoints(df,x_column,x_label,title,window_size,num_breakpoints,ff):\n",
    "    \n",
    "    #save for all possible combination of breakpoints the correspondent error \n",
    "    def f(breakpoints, x, y, fcache): \n",
    "        breakpoints = tuple(map(int, sorted(breakpoints)))\n",
    "        if breakpoints not in fcache:\n",
    "            total_error = 0\n",
    "            for f, xi, yi in find_best_piecewise_polynomial(breakpoints, x, y):\n",
    "                total_error += ((f(xi) - yi)**2).sum()\n",
    "            fcache[breakpoints] = total_error \n",
    "        # print('{} --> {}'.format(breakpoints, fcache[breakpoints]))\n",
    "        return fcache[breakpoints]\n",
    "\n",
    "    def find_best_piecewise_polynomial(breakpoints, x, y):\n",
    "        breakpoints = tuple(map(int, sorted(breakpoints)))\n",
    "        xs = np.split(x, breakpoints)\n",
    "        ys = np.split(y, breakpoints)\n",
    "        result = []\n",
    "        for xi, yi in zip(xs, ys):\n",
    "            if len(xi) < 2: continue\n",
    "            coefs = npoly.polyfit(xi, yi, 1)\n",
    "            f = npoly.Polynomial(coefs)\n",
    "            result.append([f, xi, yi])\n",
    "        return result\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(15, 5))\n",
    "    x_dates = list(df['publication_date_1'])\n",
    "    y_data = df[x_column].rolling(window=window_size).mean()[window_size-1:]\n",
    "    x_data = x_dates[window_size-1:]\n",
    "    \n",
    "    ax.plot(x_data, y_data, \"o-\", color='orange', markersize=3)\n",
    "\n",
    "    plt.grid(True, linewidth=0.5)\n",
    "    ax.yaxis.set_major_formatter(formatter)\n",
    "\n",
    "    ax.set_xlabel(x_label,size=20)\n",
    "    ax.set_title(title,size=30)\n",
    "\n",
    "    ax.xaxis.set_major_formatter(mdates.DateFormatter('%b %y')) # Get only the month to show in the x-axis\n",
    "\n",
    "    ax.axvline(pd.Timestamp(2020, 3, 1),color='r') #ax.axvline(x_dates[84],color='r')\n",
    "\n",
    "    x_num = dates.date2num(x_data)\n",
    "    breakpoints = optimize.brute(f, [slice(1, len(x_data), 1)]*num_breakpoints, args=(x_num, y_data, {}), finish=None)\n",
    "    if num_breakpoints==1:\n",
    "        breakpoints = [breakpoints] \n",
    "\n",
    "    for f, xi, yi in find_best_piecewise_polynomial(breakpoints, x_num, y_data):\n",
    "        xi_min = x_data[np.where(x_num == xi.min())[0][0]]\n",
    "        xi_max = x_data[np.where(x_num == xi.max())[0][0]] \n",
    "        x_interval = np.array([xi.min(), xi.max()])\n",
    "        coef = f.convert().coef\n",
    "        b = coef[0]\n",
    "        a = coef[1]\n",
    "        if ff==1:\n",
    "            if b>0:\n",
    "                ll = 'y = {:.2f} x + {:.2f}'.format(a,b)\n",
    "            else: #b<0\n",
    "                ll = 'y = {:.2f} x - {:.2f}'.format(a,abs(b))    \n",
    "        else:\n",
    "            if b>0:\n",
    "                ll = 'y = {:.5f} x + {:.5f}'.format(a,b)\n",
    "            else: #b<0\n",
    "                ll = 'y = {:.5f} x - {:.5f}'.format(a,abs(b))  \n",
    "        #x_1 = x_dates[window_size-1:][np.where(x_num==x_interval[0])[0][0]]\n",
    "        #x_3 = x_dates[window_size-1:][np.where(x_num==x_interval[1])[0][0]]\n",
    "        ax.plot(x_interval, f(x_interval), 'o-',color='yellow',label=ll, markersize=6)\n",
    "        \n",
    "    ax.legend()\n",
    "    ax.set_title(title,size=30)\n",
    "    #return x_num \n",
    "    \n",
    "x_label='Month'\n",
    "window_size = 6\n",
    "num_breakpoints = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9155d37-53e6-4a8b-809f-4bade8a1b0d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def work_edges_dist_mean_monthly_(works_outside,works_set,path):\n",
    "    my_file = \"work_edges_dist_mean.csv\"    \n",
    "    work_edges_dist_mean = pd.read_csv(os.path.join(path, my_file))\n",
    "    work_edges_dist_mean = work_edges_dist_mean[work_edges_dist_mean.work_id.isin(works_set)]\n",
    "    work_edges_dist_mean = work_edges_dist_mean[~work_edges_dist_mean.work_id.isin(works_outside)]\n",
    "    work_edges_dist_mean['publication_date_1'] = pd.to_datetime(work_edges_dist_mean['publication_date_1'])\n",
    "    work_edges_dist_mean_monthly = work_edges_dist_mean.groupby('publication_date_1').dist.mean().to_frame().reset_index()\n",
    "    return work_edges_dist_mean_monthly\n",
    "\n",
    "def work_edges_dist_max_monthly_(works_outside,works_set,path):\n",
    "    my_file = \"work_edges_dist_max.csv\"    \n",
    "    work_edges_dist_mean = pd.read_csv(os.path.join(path, my_file))\n",
    "    work_edges_dist_mean = work_edges_dist_mean[work_edges_dist_mean.work_id.isin(works_set)]\n",
    "    work_edges_dist_mean = work_edges_dist_mean[~work_edges_dist_mean.work_id.isin(works_outside)]\n",
    "    work_edges_dist_mean['publication_date_1'] = pd.to_datetime(work_edges_dist_mean['publication_date_1'])\n",
    "    work_edges_dist_mean_monthly = work_edges_dist_mean.groupby('publication_date_1').dist.mean().to_frame().reset_index()\n",
    "    return work_edges_dist_mean_monthly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c7b919-636f-45ce-bc07-e864cdd98173",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "work_authors_edges_df_dist = read_parquet(Path('./TeamDistance') / 'work_authors_edges_df_dist')\n",
    "work_authors_edges_df_dist = works[['work_id']].reset_index().merge(work_authors_edges_df_dist,on='work_id')\n",
    "work_authors_edges_df_dist = work_authors_edges_df_dist[work_authors_edges_df_dist.work_id.isin(works_3authors)]\n",
    "work_edges_dist_mean = work_authors_edges_df_dist.groupby('work_id').dist.mean().to_frame().reset_index()\n",
    "work_edges_dist_mean = work_edges_dist_mean.merge(works[['work_id']].reset_index(),on='work_id')\n",
    "work_edges_dist_mean_monthly = work_edges_dist_mean.groupby('publication_date_1').dist.mean().to_frame().reset_index()\n",
    "my_file = \"work_edges_dist_mean.csv\"    \n",
    "work_edges_dist_mean.to_csv(os.path.join(path_3authors, my_file),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4454b97-4198-49d0-b706-ada0a6936e5c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "my_file = \"work_edges_dist_mean_monthly.csv\"     \n",
    "work_edges_dist_mean_monthly.to_csv(os.path.join(path_3authors, my_file),index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ccc404-3a6f-43f1-b48d-035d4ac84529",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67613ba9-f320-4c1d-b6b6-aea3e105f044",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "my_file = \"dfs_3authors.pickle\"\n",
    "with open(os.path.join(path_3authors, my_file),\"rb\") as fp:\n",
    "    [works_3authors,works_3authors,works_authors_aff_3authors] = pickle.load(fp)  \n",
    "inst_set_whole_3authors = set((works_authors_aff_3authors.drop_duplicates('institution_id')).institution_id)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4427c6f-b1d4-43c7-893f-e8a0b2230ef1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "N_dict = works_authors_aff_3authors.groupby('publication_date_1').work_id.nunique().to_frame().to_dict()['work_id']\n",
    "months_list = list(N_dict.keys())\n",
    "months_list.sort()\n",
    "months_list = [i.strftime('%Y-%m-%d') for i in months_list]\n",
    "start_index = 0\n",
    "end_index = 120 \n",
    "start_month = months_list[start_index]\n",
    "end_month = months_list[end_index-1]\n",
    "works_authors_aff_3authors_TW = works_authors_aff_3authors.loc[start_month:end_month]\n",
    "\n",
    "inst_set_TW_3authors = set((works_authors_aff_3authors_TW .drop_duplicates('institution_id')).institution_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ed1f6d-415c-4262-bc4d-ff5e04d2621b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#restrict to papers not including authors from institutions not in TW\n",
    "inst_set_TW_3authors_compl = inst_set_whole_3authors - inst_set_TW_3authors\n",
    "works_outside = set(works_authors_aff_3authors[works_authors_aff_3authors.institution_id.isin(inst_set_TW_3authors_compl)].work_id)\n",
    "works_3authors = works_3authors[~works_3authors.work_id.isin(works_outside)]\n",
    "works_authors_aff_3authors_new = works_authors_aff_3authors[~works_authors_aff_3authors.work_id.isin(works_outside)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5604f3-7847-483d-b4a6-8b8e95882b0e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f'3 authors. [2000,2023] {len(inst_set_whole_3authors)} institutions, [2000,2009] {len(inst_set_TW_3authors)} institutions ({(len(inst_set_TW_3authors)/len(inst_set_whole_3authors))*100:.2f}%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7754e0f2-c54a-47dd-8d32-f41ec789eed1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f'3 authors. All institutions: {len(set(works_authors_aff_3authors.work_id))} works, restrict to institutions in TW {len(set(works_authors_aff_3authors_new.work_id))} works ({(len(set(works_authors_aff_3authors_new.work_id))/len(set(works_authors_aff_3authors.work_id)))*100:.2f}%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf990f7-fb24-4db5-b601-b0ee4026fe25",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#delate tail\n",
    "df_strengths_3authors_TW = works_authors_aff_3authors_TW.groupby('institution_id').work_id.count().to_frame().sort_values(by='work_id',ascending=False).reset_index().rename(columns={'work_id':'strength'})\n",
    "df_strengths_3authors_TW['institution_id'] = df_strengths_3authors_TW['institution_id'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4dc7403-c2f9-465a-801b-6bb95a20b8f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for n in [20,50,70,100]:\n",
    "    my_file = \"dfs_3authors.pickle\"\n",
    "    with open(os.path.join(path_3authors, my_file),\"rb\") as fp:\n",
    "        [works_3authors,works_3authors,works_authors_aff_3authors] = pickle.load(fp) \n",
    "    I = set((df_strengths_3authors_TW[df_strengths_3authors_TW.strength>=n]).institution_id)\n",
    "    I_compl = inst_set_whole_3authors - I\n",
    "    works_outside = set(works_authors_aff_3authors[works_authors_aff_3authors.institution_id.isin(I_compl)].work_id)\n",
    "    works_3authors = works_3authors[~works_3authors.work_id.isin(works_outside)]\n",
    "    works_authors_aff_3authors_new = works_authors_aff_3authors[~works_authors_aff_3authors.work_id.isin(works_outside)]\n",
    "    print(f'3 authors - tail {n}. TW {len(I)} institutions ({(len(I)/len(inst_set_TW_3authors))*100:.2f}%), {len(set(works_authors_aff_3authors_new.work_id))} works ({(len(set(works_authors_aff_3authors_new.work_id))/len(set(works_authors_aff_3authors.work_id)))*100:.2f}%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765763f8-ed34-49b4-b5f0-3f9b04f4557a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116881c6-660a-4117-af62-4310c988e515",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "393e5643-8c58-489d-8145-12343b380952",
   "metadata": {},
   "source": [
    "### Power-law"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e10ad8-a851-4074-a5dd-7846275d938a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fit_data(0,3,path_3authors) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66163563-7f7a-437a-af18-e0e82532e022",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fit_data(20,3,path_3authors) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a306cf0-6a16-4e82-b9cb-8b4cf3fb4d36",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fit_data(50,3,path_3authors) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c0f9ad-b4eb-4b48-84da-6b1838baf2d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "991dad12-4dfd-41cc-945f-82c6c5eb114d",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "plots(0,path_3authors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3084fb46-cb09-4d54-b3a9-c79348d045bc",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "plots(20,path_3authors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a720ff9f-8fda-43c7-81b0-1a39a161dabf",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "plots(50,path_3authors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c5d180-b4b4-4a06-86d1-efd3ecb2ad66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc2c53e-f60f-414e-a1a5-97058874c090",
   "metadata": {},
   "outputs": [],
   "source": [
    "## triangles ## different distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34721e4-c8a6-4e23-bdc9-ff9fb1ba4778",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n = 50 \n",
    "my_file = \"insts_tail\"+str(n)+\"_TW.pickle\"\n",
    "with open(os.path.join(path_3authors, my_file),\"rb\") as fp:\n",
    "    inst_set_s = pickle.load(fp)\n",
    "len(inst_set_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0584ca32-1602-40fe-8f14-f2d4d72386ff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lll = range(800)\n",
    "all_triangles = set(itertools.combinations(list(lll), 3))#all possible triangles i<j<k\n",
    "all_triangles = pd.DataFrame(all_triangles, columns =['i', 'j', 'k'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b64056aa-ba3c-494c-ac16-11afd32f4150",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_triangles.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4351434b-7994-4824-9a0d-c1ecda8b3b0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374871c6-371d-4806-80fb-3fb8fa9dd13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#800\n",
    "#1000 #2mins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d62a20-4342-4319-a85b-a42f5aadc158",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ab95a0-4b73-4c63-b1e4-d7865a61c46b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#TW\n",
    "my_file = \"dfs_3authors.pickle\"\n",
    "with open(os.path.join(path_3authors, my_file),\"rb\") as fp:\n",
    "    [works_3authors,works,works_authors_aff] = pickle.load(fp)  \n",
    "works = works.loc['2000-01-01':'2023-12-01'] \n",
    "works_authors_aff = works_authors_aff.loc['2000-01-01':'2023-12-01']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628a69b3-16c8-4583-a797-f2b8f36ef9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(works_authors_aff.institution_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9209c7db-e1dd-4999-b977-2818782c57c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_dict = works_authors_aff.groupby('publication_date_1').work_id.nunique().to_frame().to_dict()['work_id']\n",
    "months_list = list(N_dict.keys())\n",
    "months_list.sort()\n",
    "months_list = [i.strftime('%Y-%m-%d') for i in months_list]\n",
    "\n",
    "#INITIALIZATION #preferential attachment  #TW: 2000-2009\n",
    "start_index = 0\n",
    "end_index = 120 #180\n",
    "start_month = months_list[start_index]#.strftime('%Y-%m-%d')\n",
    "end_month = months_list[end_index-1]#.strftime('%Y-%m-%d')\n",
    "df_TW = works_authors_aff.loc[start_month:end_month] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ca1761-edf9-4abd-9bd3-417c42302751",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(df_TW.institution_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f932961e-04c0-466b-befd-829e84fd87da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040a328c-4c40-47de-8571-749d20a2b411",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_file = \"I_dist_model.csv\"  \n",
    "I_dist = pd.read_csv(os.path.join(path_3authors, my_file))\n",
    "I_dist = I_dist.query('source.isin(@inst_set_s) & target.isin(@inst_set_s)')\n",
    "\n",
    "all_triangles = set(itertools.combinations(list(inst_set_s), 3))#all possible triangles i<j<k\n",
    "all_triangles = [sorted(list(x)) for x in all_triangles]\n",
    "df_inter3 = pd.DataFrame(all_triangles, columns =['i', 'j', 'k'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "814a1666-a0d6-44d7-a714-2fa147944eb2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646c4372-19c6-462f-a641-1fb45115f374",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def power_law_threshold_triangles(n):\n",
    "    my_file = \"insts_tail\"+str(n)+\"_TW.pickle\"\n",
    "    with open(os.path.join(path_3authors, my_file),\"rb\") as fp:\n",
    "        inst_set_s = pickle.load(fp)\n",
    "\n",
    "    my_file = \"I_dist_model.csv\"  \n",
    "    I_dist = pd.read_csv(os.path.join(path_3authors, my_file))\n",
    "    I_dist = I_dist[['source','target','dist']]\n",
    "    I_dist = I_dist.query('source.isin(@inst_set_s) & target.isin(@inst_set_s)')\n",
    "    I_dist['source'] = I_dist['source'].astype(int)\n",
    "    I_dist['target'] = I_dist['target'].astype(int)\n",
    "    df_inter2 = I_dist[I_dist.source!=I_dist.target]\n",
    "    df_inter2['dist_mean'] = df_inter2['dist']*(2/3)\n",
    "    df_inter2['dist_max'] = df_inter2['dist']\n",
    "    df_inter2 = df_inter2.drop(columns='dist').reset_index(drop=True)\n",
    "    df_inter2 = df_inter2.rename(columns={'source':'i','target':'j'})\n",
    "    df_inter2_copy = df_inter2.copy()\n",
    "    df_inter2['k'] = df_inter2['i']\n",
    "    df_inter2_copy['k'] = df_inter2_copy['j']\n",
    "    df_inter2 = pd.concat([df_inter2,df_inter2_copy])\n",
    "    df_intra = I_dist[I_dist.source==I_dist.target]\n",
    "    df_intra['dist_mean'] = 0\n",
    "    df_intra['dist_max'] = 0\n",
    "    df_intra['dist_RMS'] = 0\n",
    "    df_intra = df_intra.drop(columns='dist').reset_index(drop=True)\n",
    "    df_intra = df_intra.rename(columns={'source':'i','target':'j'})\n",
    "    df_intra['k']=df_intra['i']\n",
    "    df_intra = df_intra[['i','j','k','dist_mean','dist_max','dist_RMS']]\n",
    "\n",
    "    my_file = \"df_inter3_tail\"+str(n)+\".csv\" \n",
    "    df_inter3 = pd.read_csv(os.path.join(path_3authors, my_file))\n",
    "    df_inter3['i'] = df_inter3['i'].astype(int)\n",
    "    df_inter3['j'] = df_inter3['j'].astype(int)\n",
    "    df_inter3['k'] = df_inter3['k'].astype(int)\n",
    "    df_inter3 = df_inter3.query('i.isin(@inst_set_s) and j.isin(@inst_set_s) and k.isin(@inst_set_s)')\n",
    "    df_inter3 = df_inter3.drop(columns='dist')\n",
    "    df_inter3['dist_mean'] = (df_inter3['d_ij']+df_inter3['d_jk']+df_inter3['d_ik'])/3\n",
    "    df_inter3['dist_max'] = df_inter3[['d_ij','d_jk','d_ik']].max(axis=1)\n",
    "\n",
    "    ## Center of mass\n",
    "    basepath4 = Path('/N/project/openalex/ssikdar/processed-snapshots/csv-files/aug-2023')\n",
    "    institutions_geo_df = pd.read_csv(basepath4 / 'institutions_geo.csv.gz')\n",
    "    institutions_geo_df = institutions_geo_df[['institution_id','latitude','longitude']]\n",
    "    institutions_geo_df = pd.concat([institutions_geo_df,pd.DataFrame.from_dict({'institution_id':4210144721,'latitude':42.48948,'longitude':-83.14465},orient='index').T])\n",
    "    institutions_geo_df = institutions_geo_df.query('institution_id.isin(@inst_set_s)')\n",
    "    institutions_geo_df['latitude_rad'] = np.radians(institutions_geo_df['latitude']) # Convert lat/lng from degrees to radians\n",
    "    institutions_geo_df['longitude_rad'] = np.radians(institutions_geo_df['longitude'])\n",
    "    R = 6371 # Radius of Earth in kilometers\n",
    "    # Calculate Cartesian coordinates\n",
    "    institutions_geo_df['x'] = R * np.cos(institutions_geo_df['latitude_rad']) * np.cos(institutions_geo_df['longitude_rad'])\n",
    "    institutions_geo_df['y'] = R * np.cos(institutions_geo_df['latitude_rad']) * np.sin(institutions_geo_df['longitude_rad'])\n",
    "    institutions_geo_df['z'] = R * np.sin(institutions_geo_df['latitude_rad'])\n",
    "    institutions_geo_mapx = dict(zip(institutions_geo_df['institution_id'],institutions_geo_df['x']))\n",
    "    institutions_geo_mapy = dict(zip(institutions_geo_df['institution_id'],institutions_geo_df['y']))\n",
    "    institutions_geo_mapz = dict(zip(institutions_geo_df['institution_id'],institutions_geo_df['z']))\n",
    "\n",
    "    df_inter2['i_x'] = df_inter2['i'].map(institutions_geo_mapx)\n",
    "    df_inter2['i_y'] = df_inter2['i'].map(institutions_geo_mapy)\n",
    "    df_inter2['i_z'] = df_inter2['i'].map(institutions_geo_mapz)\n",
    "    df_inter2['j_x'] = df_inter2['j'].map(institutions_geo_mapx)\n",
    "    df_inter2['j_y'] = df_inter2['j'].map(institutions_geo_mapy)\n",
    "    df_inter2['j_z'] = df_inter2['j'].map(institutions_geo_mapz)\n",
    "    df_inter2['k_x'] = df_inter2['k'].map(institutions_geo_mapx)\n",
    "    df_inter2['k_y'] = df_inter2['k'].map(institutions_geo_mapy)\n",
    "    df_inter2['k_z'] = df_inter2['k'].map(institutions_geo_mapz)\n",
    "    #center of mass\n",
    "    df_inter2['C_x'] = (df_inter2[['i_x','j_x','k_x']]).mean(axis=1)\n",
    "    df_inter2['C_y'] = (df_inter2[['i_y','j_y','k_y']]).mean(axis=1)\n",
    "    df_inter2['C_z'] = (df_inter2[['i_z','j_z','k_z']]).mean(axis=1)\n",
    "    #Root Mean Square Radius, RMS radius\n",
    "    df_inter2['i_dist'] = np.sqrt((df_inter2['i_x'] - df_inter2['C_x'])**2 + (df_inter2['i_y'] - df_inter2['C_y'])**2 + (df_inter2['i_z'] - df_inter2['C_z'])**2)\n",
    "    df_inter2['j_dist'] = np.sqrt((df_inter2['j_x'] - df_inter2['C_x'])**2 + (df_inter2['j_y'] - df_inter2['C_y'])**2 + (df_inter2['j_z'] - df_inter2['C_z'])**2)\n",
    "    df_inter2['k_dist'] = np.sqrt((df_inter2['k_x'] - df_inter2['C_x'])**2 + (df_inter2['k_y'] - df_inter2['C_y'])**2 + (df_inter2['k_z'] - df_inter2['C_z'])**2)\n",
    "    df_inter2['dist_RMS'] =  np.sqrt((df_inter2[['i_dist','j_dist','k_dist']]**2).mean(axis=1))\n",
    "    df_inter2 = df_inter2[['i','j','k','dist_mean','dist_max','dist_RMS']]\n",
    "    df_inter2[['i', 'j', 'k']] = df_inter2[['i', 'j', 'k']].apply(lambda row: sorted(row), axis=1, result_type='expand')\n",
    "\n",
    "    df_inter3['i_x'] = df_inter3['i'].map(institutions_geo_mapx)\n",
    "    df_inter3['i_y'] = df_inter3['i'].map(institutions_geo_mapy)\n",
    "    df_inter3['i_z'] = df_inter3['i'].map(institutions_geo_mapz)\n",
    "    df_inter3['j_x'] = df_inter3['j'].map(institutions_geo_mapx)\n",
    "    df_inter3['j_y'] = df_inter3['j'].map(institutions_geo_mapy)\n",
    "    df_inter3['j_z'] = df_inter3['j'].map(institutions_geo_mapz)\n",
    "    df_inter3['k_x'] = df_inter3['k'].map(institutions_geo_mapx)\n",
    "    df_inter3['k_y'] = df_inter3['k'].map(institutions_geo_mapy)\n",
    "    df_inter3['k_z'] = df_inter3['k'].map(institutions_geo_mapz)\n",
    "    #center of mass\n",
    "    df_inter3['C_x'] = (df_inter3[['i_x','j_x','k_x']]).mean(axis=1)\n",
    "    df_inter3['C_y'] = (df_inter3[['i_y','j_y','k_y']]).mean(axis=1)\n",
    "    df_inter3['C_z'] = (df_inter3[['i_z','j_z','k_z']]).mean(axis=1)\n",
    "    #Root Mean Square Radius, RMS radius\n",
    "    df_inter3['i_dist'] = np.sqrt((df_inter3['i_x'] - df_inter3['C_x'])**2 + (df_inter3['i_y'] - df_inter3['C_y'])**2 + (df_inter3['i_z'] - df_inter3['C_z'])**2)\n",
    "    df_inter3['j_dist'] = np.sqrt((df_inter3['j_x'] - df_inter3['C_x'])**2 + (df_inter3['j_y'] - df_inter3['C_y'])**2 + (df_inter3['j_z'] - df_inter3['C_z'])**2)\n",
    "    df_inter3['k_dist'] = np.sqrt((df_inter3['k_x'] - df_inter3['C_x'])**2 + (df_inter3['k_y'] - df_inter3['C_y'])**2 + (df_inter3['k_z'] - df_inter3['C_z'])**2)\n",
    "    df_inter3['dist_RMS'] =  np.sqrt((df_inter3[['i_dist','j_dist','k_dist']]**2).mean(axis=1))\n",
    "    df_inter3 = df_inter3[['i','j','k','dist_mean','dist_max','dist_RMS']]\n",
    "\n",
    "    df_concat = pd.concat([df_intra,df_inter2,df_inter3])\n",
    "\n",
    "    my_file = \"inst_set.pickle\"\n",
    "    with open(os.path.join(path_3authors, my_file),\"rb\") as fp:\n",
    "        inst_set = pickle.load(fp)\n",
    "    my_file = \"dfs_3authors.pickle\"\n",
    "    with open(os.path.join(path_3authors, my_file),\"rb\") as fp:\n",
    "        [works_3authors,works,works_authors_aff] = pickle.load(fp) \n",
    "\n",
    "    inst_set_s_compl = inst_set - inst_set_s\n",
    "    inst_set_s_compl = inst_set_s_compl.union(set(works_authors_aff.institution_id)-inst_set)\n",
    "    works_outside = set(works_authors_aff[works_authors_aff.institution_id.isin(inst_set_s_compl)].work_id)    \n",
    "    works = works[~works.work_id.isin(works_outside)]\n",
    "    works_authors_aff = works_authors_aff[~works_authors_aff.work_id.isin(works_outside)]    \n",
    "    works_authors_aff = works_authors_aff[['work_id','institution_id']].reset_index()\n",
    "    works_authors_aff = works_authors_aff.sort_values(by=['publication_date_1','work_id','institution_id'])\n",
    "    works_authors_aff = works_authors_aff.groupby(['publication_date_1','work_id']).institution_id.apply(list).to_frame()\n",
    "    works_authors_aff['i'] = works_authors_aff['institution_id'].apply(lambda x: x[0])\n",
    "    works_authors_aff['j'] = works_authors_aff['institution_id'].apply(lambda x: x[1])\n",
    "    works_authors_aff['k'] = works_authors_aff['institution_id'].apply(lambda x: x[2])\n",
    "    works_authors_aff['i'] = works_authors_aff['i'].astype(int)\n",
    "    works_authors_aff['j'] = works_authors_aff['j'].astype(int)\n",
    "    works_authors_aff['k'] = works_authors_aff['k'].astype(int)\n",
    "    works_authors_aff = works_authors_aff[['i','j','k']].reset_index()\n",
    "    works_authors_aff = works_authors_aff.groupby(['publication_date_1','i','j','k']).work_id.count().to_frame().reset_index().rename(columns={'work_id':'count'})\n",
    "    #works_authors_aff = works_authors_aff[works_authors_aff.publication_date_1<'2023']\n",
    "\n",
    "    works_authors_aff = works_authors_aff.set_index('publication_date_1')\n",
    "    start_year_0 = 2000\n",
    "    end_year_0 = 2009\n",
    "    start_year_1 = 2010\n",
    "    end_year_1 = 2014\n",
    "    start_year_2 = 2015\n",
    "    end_year_2 = 2019\n",
    "    start_year_3 = 2020\n",
    "    end_year_3 = 2021\n",
    "    start_year_4 = 2022\n",
    "    end_year_4 = 2023\n",
    "    works_authors_aff.loc[str(start_year_0):str(end_year_0),'period'] = 0\n",
    "    works_authors_aff.loc[str(start_year_1):str(end_year_1),'period'] = 1\n",
    "    works_authors_aff.loc[str(start_year_2):str(end_year_2),'period'] = 2\n",
    "    works_authors_aff.loc[str(start_year_3):str(end_year_3),'period'] = 3\n",
    "    works_authors_aff.loc[str(start_year_4):str(end_year_4),'period'] = 4\n",
    "    works_authors_aff['period'] = works_authors_aff['period'].astype(int)\n",
    "\n",
    "    df = pd.DataFrame()\n",
    "    for y in range(5):\n",
    "        df_y = works_authors_aff[works_authors_aff.period == y]\n",
    "        df_y = df_y.merge(df_concat,on=['i','j','k'],how='right')#[['i','j','k','dist','count']]\n",
    "        df_y['count'] = df_y['count'].fillna(0)\n",
    "        df_y = df_y.groupby(['i','j','k','dist_mean','dist_max','dist_RMS'])['count'].sum().to_frame().reset_index()\n",
    "        df_y['prob'] = df_y['count']/df_y['count'].sum()\n",
    "        df_y['period'] = y\n",
    "        df = pd.concat([df,df_y])\n",
    "    my_file = \"triangles_df_tail\"+str(n)+\".csv\"    \n",
    "    df.to_csv(os.path.join(path_3authors, my_file),index=False)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc4fc3f-ad45-4df6-9a95-680f402f3caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "power_law_threshold_triangles(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f77b825-d226-4cfd-b3ee-2a9b023ba528",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n = 50\n",
    "my_file = \"triangles_df_tail\"+str(n)+\".csv\" \n",
    "triangles_df = pd.read_csv(os.path.join(path_3authors, my_file))  \n",
    "my_file = \"degree_df_tail\"+str(n)+\".csv\"    \n",
    "degree_df = pd.read_csv(os.path.join(path_3authors, my_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b54ad3-a942-416b-bdfa-8d82ad4150f3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plots(50,path_3authors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6741c7bf-6068-4af7-b30d-d70e27cdac8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a713f7-adca-4013-a6ed-e1a977062096",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plots(n,my_path_):\n",
    "    if n==0:\n",
    "        my_file_add = \"\"\n",
    "    else:\n",
    "        my_file_add = \"_tail\"+str(n)    \n",
    "    \n",
    "    periods_list = [0,1,2,3,4]\n",
    "    periods_labels = ['[2000,2009]','[2010,2014]','[2015,2019]','[2020,2021]','[2022,2023]']\n",
    "    color_dict = {0: 'yellow', 1: 'magenta', 2: 'mediumseagreen', 3: 'cyan', 4: 'orange'}\n",
    "    \n",
    "    my_file = \"triangles_df\"+my_file_add+\".csv\"\n",
    "    edges_df_tot = pd.read_csv(my_path_ / my_file) \n",
    "    my_file = \"degree_df\"+my_file_add+\".csv\"\n",
    "    degree_df = pd.read_csv(my_path_ / my_file)  \n",
    "    \n",
    "    def mean_scatter_plot_multi(periods_list,periods_labels,color_dict,func,edges_df,degree_df,col,columnx,columny,labelx,labely,title,logx=False,logy=False,limity=False,geomspace=False):\n",
    "        fig, ax = plt.subplots(figsize=(8,5))\n",
    "        for y in periods_list[1:]:\n",
    "            df1 = func(edges_df,degree_df,col,y)\n",
    "            x1 = np.array(df1[columnx]) \n",
    "            y1 = np.array(df1[columny])  \n",
    "            # Define the grid\n",
    "            gridsize = 10**2\n",
    "            if geomspace:\n",
    "                xbins1 = np.geomspace(x1.min(), x1.max(), gridsize)\n",
    "                #xbins2 = np.geomspace(x2.min(), x2.max(), gridsize)\n",
    "            else:\n",
    "                xbins1 = np.linspace(x1.min(), x1.max(), gridsize)\n",
    "                #xbins2 = np.linspace(x2.min(), x2.max(), gridsize)\n",
    "            # Calculate the mean values within each column\n",
    "            mean_values1 = []\n",
    "            for i in range(len(xbins1) - 1):\n",
    "                mask = (x1 >= xbins1[i]) & (x1 < xbins1[i + 1])\n",
    "                mean_y1 = np.mean(y1[mask])\n",
    "                mean_values1.append(mean_y1) \n",
    "            ax.plot((xbins1[:-1] + xbins1[1:]) / 2, mean_values1, '.',color=color_dict[y],markersize=5, label=periods_labels[y])          \n",
    "        ax.set_xlabel(labelx,size=20)\n",
    "        ax.set_ylabel(labely,size=20)\n",
    "        ax.set_title(title,size=30)\n",
    "        if logx==True:\n",
    "            ax.set_xscale('log')  \n",
    "        if logy==True:\n",
    "            ax.set_yscale('log')\n",
    "        if limity==True:\n",
    "            ax.set_ylim([0, 1])  \n",
    "        ax.legend(bbox_to_anchor=(1.0, 0.9), prop={'size': 15}, markerscale=3)\n",
    "        plt.show() \n",
    "    def scipy_fun_multi(periods_list,periods_labels,color_dict,func,edges_df,degree_df,col,x_max,labelx,labely,title):\n",
    "        fig, ax = plt.subplots(figsize=(8,5))  \n",
    "        for y in periods_list[1:]:\n",
    "            x1,y1 = func(edges_df,degree_df,col,y)\n",
    "            y1 = np.array(y1)\n",
    "            popt1, pcov1 = scipy.optimize.curve_fit(linear, x1[x1<=x_max], y1[x1<=x_max])\n",
    "            perr1 = np.sqrt(np.diag(pcov1))\n",
    "            print(f'{periods_labels[y]}: gamma {popt1[0]} (perr {perr1[0]:.4f})') \n",
    "            ax.scatter(x1[x1<=x_max], y1[x1<=x_max],marker='.',color=color_dict[y],linewidths=0.01, label=periods_labels[y]) \n",
    "            ax.plot(x1[x1<=x_max], linear(x1[x1<=x_max], *popt1),color=color_dict[y],linewidth=2.0)   \n",
    "        ax.set_xlabel(labelx,size=20)\n",
    "        ax.set_ylabel(labely,size=20)\n",
    "        ax.legend(bbox_to_anchor=(1.0, 0.9), prop={'size': 15}, markerscale=3)\n",
    "        ax.set_title(title+\" - linear\",size=30)\n",
    "        plt.show()\n",
    "    def linear(x, b):\n",
    "        return  b * x \n",
    "    def linear_fit(edges_df,degree_df,col,yy):\n",
    "        df1 = fig_2B(edges_df,degree_df,col,yy)\n",
    "        x1 = np.array(df1[\"s\"]) \n",
    "        y1 = np.array(df1[col])  \n",
    "        gridsize = 10**2\n",
    "        xbins1 = np.linspace(x1.min(), x1.max(), gridsize)    \n",
    "        mean_values1 = []\n",
    "        for i in range(len(xbins1) - 1):\n",
    "            mask = (x1 >= xbins1[i]) & (x1 < xbins1[i + 1])\n",
    "            mean_y1 = np.mean(y1[mask])\n",
    "            mean_values1.append(mean_y1)     \n",
    "        x = (xbins1[:-1] + xbins1[1:]) / 2\n",
    "        y = np.array(mean_values1)\n",
    "        x = x[~np.isnan(y)]\n",
    "        y = y[~np.isnan(y)]\n",
    "        return x,y\n",
    "    \n",
    "    def fig_2B(edges_df,degree_df,col,y):\n",
    "        edges_year_df = edges_df.query('period == @y')\n",
    "        loops_w_count = edges_year_df.query('i==j and j==k')[['i',col]].rename(columns={'i':'institution_id'})\n",
    "        degree_year_df = (degree_df[degree_df.period<=y][['institution_id','s']]).groupby(['institution_id']).s.sum().to_frame().reset_index()\n",
    "        loops_w_count = loops_w_count.merge(degree_year_df,on='institution_id')\n",
    "        return loops_w_count\n",
    "    \n",
    "    mean_scatter_plot_multi(periods_list,periods_labels,color_dict,fig_2B,edges_df_tot,degree_df,\"weight\",\"s\",\"weight\",\"s_i\",\"w_ii\",'Fig. 2B',False,False,False,False)\n",
    "    scipy_fun_multi(periods_list,periods_labels,color_dict,linear_fit,edges_df_tot,degree_df,\"weight\",2*10**4,\"s_i\",\"w_ii\",\"Fig. 2B\")\n",
    "    #mean_scatter_plot_multi(periods_list,periods_labels,color_dict,fig_2B,edges_df_tot,degree_df,\"prob\",\"s\",\"prob\",\"s_i\",\"p_ii\",'Fig. 2B',False,False,False,False)\n",
    "    #scipy_fun_multi(periods_list,periods_labels,color_dict,linear_fit,edges_df_tot,degree_df,\"prob\",2*10**4,\"s_i\",\"p_ii\",\"Fig. 2B\")\n",
    "    \n",
    "    def fig_4B(df1,df2,col,y):\n",
    "        df1 = df1.query('period == @y')[['i','j','k',col,'dist_mean']]\n",
    "        df2 = df2.query('period == @y')[['institution_id','s']]\n",
    "        df2_dict = df2.set_index('institution_id').to_dict()['s']\n",
    "        df1['source_s'] = df1['source'].map(df2_dict)\n",
    "        df1['target_s'] = df1['target'].map(df2_dict)\n",
    "        df1['ratio'] = df1[col] / (df1['source_s']*df1['target_s'])**(1/2)\n",
    "        df1 = df1[[\"dist\", \"ratio\"]]\n",
    "        #delate zero\n",
    "        df1 = df1[df1.dist>0]\n",
    "        return df1\n",
    "    def scipy_fun_multi(periods_list,periods_labels,color_dict,func,edges_df,degree_df,col,x_min,labelx,labely,title):\n",
    "        fig, ax = plt.subplots(figsize=(8,5))  \n",
    "        for y in periods_list[1:]:\n",
    "            x1,y1 = func(edges_df,degree_df,col,y)\n",
    "            y1 = np.array(y1)\n",
    "            popt1, pcov1 = scipy.optimize.curve_fit(power2, x1[x1>=x_min], y1[x1>=x_min])\n",
    "            perr1 = np.sqrt(np.diag(pcov1))\n",
    "            print(f'{periods_labels[y]}: alpha {(-1)*popt1[1]:.3f} (perr {perr1[1]:.4f}), beta {popt1[0]:.6f} (perr {perr1[0]:.4f})') \n",
    "            ax.scatter(x1[x1>=x_min], y1[x1>=x_min],marker='.',color=color_dict[y],linewidths=0.01, label=periods_labels[y]) \n",
    "            ax.plot(x1[x1>=x_min], power2(x1[x1>=x_min], *popt1),color=color_dict[y],linewidth=2.0)   \n",
    "        ax.set_xscale('log')  \n",
    "        ax.set_yscale('log')\n",
    "        ax.set_xlabel(labelx,size=20)\n",
    "        ax.set_ylabel(labely,size=20)\n",
    "        ax.legend(bbox_to_anchor=(1.0, 0.9), prop={'size': 15}, markerscale=3)\n",
    "        ax.set_title(title+\" - power law\",size=30)\n",
    "        plt.show()\n",
    "    def power2(x, b, c):\n",
    "        return  b * x ** c \n",
    "    def powerlaw_fit(edges_df,degree_df,col,yy):\n",
    "        df = fig_4B(edges_df,degree_df,col,yy)\n",
    "        x = np.array(df[\"dist\"]) \n",
    "        y = np.array(df[\"ratio\"])\n",
    "        gridsize = 10**2\n",
    "        xbins = np.geomspace(x.min(), x.max(), gridsize)\n",
    "        # Calculate the mean values within each column\n",
    "        mean_values = []\n",
    "        for i in range(len(xbins) - 1):\n",
    "            mask = (x >= xbins[i]) & (x < xbins[i + 1])\n",
    "            mean_y = np.mean(y[mask])\n",
    "            mean_values.append(mean_y) \n",
    "        x = (xbins[:-1] + xbins[1:]) / 2\n",
    "        y = np.array(mean_values)\n",
    "        return x,y\n",
    "    \n",
    "    mean_scatter_plot_multi(periods_list,periods_labels,color_dict,fig_4B,edges_df_tot,degree_df,\"weight\",\"dist\", \"ratio\",\"d_ij\",\"w_ij / (s_i*s_j)^(1/2) \",'Fig. 4B',True,True,False,True)\n",
    "    scipy_fun_multi(periods_list,periods_labels,color_dict,powerlaw_fit,edges_df_tot,degree_df,\"weight\",10**1,\"d_ij\",\"w_ij / (s_i*s_j)^(1/2) \",\"Fig. 4B - power law\")\n",
    "    scipy_fun_multi(periods_list,periods_labels,color_dict,powerlaw_fit,edges_df_tot,degree_df,\"weight\",10**2,\"d_ij\",\"w_ij / (s_i*s_j)^(1/2) \",\"Fig. 4B - power law\")\n",
    "    #mean_scatter_plot_multi(periods_list,periods_labels,color_dict,fig_4B,edges_df_tot,degree_df,\"prob\",\"dist\", \"ratio\",\"d_ij\",\"p_ij / (s_i*s_j)^(1/2) \",'Fig. 4B',True,True,False,True)\n",
    "    #scipy_fun_multi(periods_list,periods_labels,color_dict,powerlaw_fit,edges_df_tot,degree_df,\"prob\",10**1,\"d_ij\",\"p_ij / (s_i*s_j)^(1/2) \",\"Fig. 4B - power law\")\n",
    "    #scipy_fun_multi(periods_list,periods_labels,color_dict,powerlaw_fit,edges_df_tot,degree_df,\"prob\",10**2,\"d_ij\",\"p_ij / (s_i*s_j)^(1/2) \",\"Fig. 4B - power law\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9aef1a0-3745-4010-a242-527a08382b6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c9b66795-f478-4149-8b3e-e0f7158de9c2",
   "metadata": {},
   "source": [
    "## Edges - all works "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b0819c-a0f2-4843-92db-b00f54e90a0d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "path_allauthors = Path('./Model_allworks')\n",
    "if not os.path.exists(path_allauthors):\n",
    "    os.makedirs(path_allauthors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ffda7d-7f9f-40f7-adbf-93b9ece1f3ed",
   "metadata": {},
   "source": [
    "### Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1cd6ae-0bbc-4ebf-82e3-2c6d45316691",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "works = read_parquet(basepath / 'works')\n",
    "works_authors_aff = read_parquet(basepath / 'works_authors_aff')\n",
    "\n",
    "works = works.loc['2000-01-01':'2023-12-01'] \n",
    "works = works[works.num_authors>1]\n",
    "\n",
    "works_allauthors = set(works.work_id)\n",
    "print(f'{len(works_allauthors)} ({(len(works_allauthors)/len(works))*100:.2f}%) works all authors')\n",
    "\n",
    "works = works[works.work_id.isin(works_allauthors)]\n",
    "works_authors_aff = works_authors_aff[works_authors_aff.work_id.isin(works_allauthors)]\n",
    "\n",
    "my_file = \"dfs_allauthors.pickle\"\n",
    "pickle.dump([works_allauthors,works,works_authors_aff], open(os.path.join(path_allauthors, my_file), 'wb')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210eb673-0f01-45e6-9726-c6be2c414a93",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#INITIALIZATION #preferential attachment  #TW: 2000-2009\n",
    "my_file = \"dfs_allauthors.pickle\"\n",
    "with open(os.path.join(path_allauthors, my_file),\"rb\") as fp:\n",
    "    [works_allauthors,works,works_authors_aff] = pickle.load(fp)  \n",
    "works = works.loc['2000-01-01':'2023-12-01'] \n",
    "works_authors_aff = works_authors_aff.loc['2000-01-01':'2023-12-01']\n",
    "\n",
    "N_dict = works_authors_aff.groupby('publication_date_1').work_id.nunique().to_frame().to_dict()['work_id']\n",
    "months_list = list(N_dict.keys())\n",
    "months_list.sort()\n",
    "months_list = [i.strftime('%Y-%m-%d') for i in months_list]\n",
    "\n",
    "start_index = 0\n",
    "end_index = 120 #180\n",
    "start_month = months_list[start_index]#.strftime('%Y-%m-%d')\n",
    "end_month = months_list[end_index-1]#.strftime('%Y-%m-%d')\n",
    "\n",
    "df_TW = works_authors_aff.loc[start_month:end_month] #df_TW = works_authors_aff.loc[months_list[:120]]\n",
    "inst_set = set(df_TW.institution_id)\n",
    "I = len(inst_set)\n",
    "print(f'TW from {start_month} to {end_month} : {I} institutions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b0271c-44b1-4081-baa3-c581d6c9c4df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#initial strenghts    #consider only institutions in sample in TW\n",
    "#count number (unique) institutions per paper\n",
    "df_TW = df_TW.drop_duplicates(['work_id','institution_id'])\n",
    "df_TW['num_affs'] = df_TW.groupby('work_id')['institution_id'].transform('size')\n",
    "print(f'{min(df_TW.num_affs)}-{max(df_TW.num_affs)} min-max number (unique) affiliations per work')\n",
    "df_TW['weight'] = 2 / ( df_TW['num_affs']*(df_TW['num_affs']-1) ) \n",
    "df_TW.loc[df_TW.num_affs==1,'weight'] = 1 #one affiliation\n",
    "df_TW_noloops = df_TW[df_TW.num_affs>1]\n",
    "df_TW_loops = df_TW[df_TW.num_affs==1]\n",
    "df_TW_loops['institution_id2'] = df_TW_loops['institution_id']\n",
    "df_TW_loops['weight'] = df_TW_loops[['institution_id','institution_id2','weight']].groupby(['institution_id']).weight.transform('sum')\n",
    "df_TW_loops = df_TW_loops.drop_duplicates('institution_id')\n",
    "I_graph = make_institution_graph(df_TW_noloops)\n",
    "I_graph.add_weighted_edges_from([tuple(r) for r in df_TW_loops[['institution_id','institution_id2','weight']].to_numpy()])\n",
    "df_ = pd.DataFrame.from_dict(dict(I_graph.degree(weight='weight')),orient='index').reset_index().rename(columns={'index':'institution_id',0:'strength'})\n",
    "df_['institution_id'] = df_['institution_id'].astype(int)\n",
    "df = df_.sort_values(by='strength',ascending=False)\n",
    "inst_set = set(df.institution_id)\n",
    "I = len(inst_set)\n",
    "print(f'TW from {start_month} to {end_month} : {I} institutions')\n",
    "\n",
    "my_file = \"df_strengths0.csv\"     \n",
    "df.to_csv(os.path.join(path_allauthors, my_file),index=False)\n",
    "\n",
    "my_file = \"inst_set.pickle\"\n",
    "pickle.dump(inst_set, open(os.path.join(path_allauthors, my_file), 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f006eb6a-8ac9-4e29-af2c-326c3831945b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "I_dist = read_parquet(basepath / 'I_dist_threshold')\n",
    "I_dist = I_dist[['source','target','dist']].reset_index(drop=True)\n",
    "I_dist = I_dist.query('source.isin(@inst_set) & target.isin(@inst_set)').reset_index()\n",
    "my_file = \"I_dist_model.csv\" \n",
    "I_dist.to_csv(os.path.join(path_allauthors, my_file),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b80c992-561a-4e7b-ac06-481bb14a1bfb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3274f6-9dc4-4f2f-ad93-468a50c1a37d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "work_authors_edges_df_dist = read_parquet(Path('./TeamDistance') / 'work_authors_edges_df_dist')\n",
    "work_authors_edges_df_dist = works[['work_id']].reset_index().merge(work_authors_edges_df_dist,on='work_id')\n",
    "work_authors_edges_df_dist = work_authors_edges_df_dist[work_authors_edges_df_dist.work_id.isin(works_allauthors)]\n",
    "work_edges_dist_mean = work_authors_edges_df_dist.groupby('work_id').dist.mean().to_frame().reset_index()\n",
    "work_edges_dist_mean = work_edges_dist_mean.merge(works[['work_id']].reset_index(),on='work_id')\n",
    "work_edges_dist_mean_monthly = work_edges_dist_mean.groupby('publication_date_1').dist.mean().to_frame().reset_index()\n",
    "my_file = \"work_edges_dist_mean.csv\"    \n",
    "work_edges_dist_mean.to_csv(os.path.join(path_allauthors, my_file),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1fcc18e-2a7a-4fe9-9483-5d2a4c87ef67",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "my_file = \"work_edges_dist_mean_monthly.csv\"     \n",
    "work_edges_dist_mean_monthly.to_csv(os.path.join(path_allauthors, my_file),index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b470a33c-9798-4bd1-8f1f-60eb9c8de670",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_data1 = df_data1_(work_authors_edges_df_dist)\n",
    "df_data1['publication_date_1'] = df_data1['publication_date_1'].apply(pd.to_datetime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15af6d8-1015-463a-82d7-338155d2da8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a42f43-1dbf-49f1-b568-c963800df402",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
