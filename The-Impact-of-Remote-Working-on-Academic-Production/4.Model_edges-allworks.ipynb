{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8db45cb-e0de-4e7d-990b-cc349c470790",
   "metadata": {},
   "source": [
    "# Model - edges - all works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d496c73-ba21-4c98-af68-ace1d7856b94",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from pathlib import Path\n",
    "import networkx as nx\n",
    "import igraph as ig\n",
    "import pickle\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.spatial import distance\n",
    "import seaborn as sns\n",
    "from time import time\n",
    "from tqdm.auto import tqdm\n",
    "import random \n",
    "import os\n",
    "from itertools import chain, combinations\n",
    "import itertools\n",
    "import collections\n",
    "\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "import plotly.io as pio\n",
    "plt.rcParams.update({'font.size': 22})\n",
    "sns.set(style=\"ticks\", context=\"talk\")\n",
    "plt.style.use(\"dark_background\")\n",
    "pd.options.plotting.backend = 'plotly'\n",
    "pio.templates.default = 'plotly_dark+presentation'\n",
    "                    \n",
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "import matplotlib.dates as dates\n",
    "import matplotlib.dates as mdates\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "from scipy import optimize\n",
    "import numpy.polynomial.polynomial as npoly\n",
    "\n",
    "def form(x,pos):\n",
    "    if x<1e3:\n",
    "        return '%1.3f' % (x)\n",
    "    elif x<1e6:\n",
    "        return '%1.1fK' % (x * 1e-3)\n",
    "    else:\n",
    "        return '%1.1fM' % (x * 1e-6)\n",
    "formatter = FuncFormatter(form)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def read_parquet(name, **args):\n",
    "    path = name\n",
    "    print(f'Reading {name!r}')\n",
    "    tic = time()\n",
    "    df = pd.read_parquet(path, engine='fastparquet', **args)\n",
    "    before = len(df)\n",
    "    # df.drop_duplicates(inplace=True)\n",
    "    toc = time()\n",
    "    after = len(df)\n",
    "        \n",
    "    print(f'Read {len(df):,} rows from {path.stem!r} in {toc-tic:.2f} sec. {before-after:,} duplicates.')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf80ccc-9d63-43a8-9bad-06972a2ef9a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "basepath = Path('./Tables_final') \n",
    "my_path_ = Path('./Model_allworks')\n",
    "if not os.path.exists(my_path_):\n",
    "    os.makedirs(my_path_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "279dcc41-548f-490a-882a-47b25bc7a0b7",
   "metadata": {},
   "source": [
    "## Model 2 variables\n",
    "\n",
    "B_{ij} = beta * ((s_i*s_j)^a/(d_{ij}+c)^alpha) if i!=j </br>\n",
    "B_{ii} = gamma * s_i if i!=j </br>\n",
    "\n",
    "P_{ij} = B_{ij}/N(alpha,beta,gamma)</br>\n",
    "P_{ii} = B_{ii}/N(alpha,beta,gamma)</br>\n",
    "with N(alpha,beta,gamma) = sum_{i} B_{ii} + sum_{(i,j)} B_{ij}</br>\n",
    "\n",
    "Parameters: a=1/2, c=10 </br>\n",
    "Variables: alpha>=0, beta>=0, gamma>=0 </br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fde8cdc-3cfc-49d6-8a2e-f6c6dd1e1eb8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_collaboration_graph(works_authors_rows):\n",
    "    \n",
    "    authors_id_set = set(works_authors_rows.author_id)\n",
    "                                  \n",
    "    bip_g = nx.from_pandas_edgelist(\n",
    "        works_authors_rows,\n",
    "        source='work_id', target='author_id'\n",
    "    )\n",
    "    \n",
    "    collab_graph = nx.bipartite.weighted_projected_graph(bip_g,nodes=authors_id_set) #bipartite.weighted_projected_graph(bip_g,nodes=authors_id)\n",
    "    return collab_graph\n",
    "\n",
    "def strength_update(df_intra,df_inter,W,a,c,params):\n",
    "    N = ( params[1] *((df_inter['m_prod'])/ ((df_inter['dist']+c)**params[0])) ).sum() + (params[2] *df_intra['m_source']).sum() \n",
    "    df_inter2 = df_inter.copy()\n",
    "    df_inter2[['target','source','m_target','m_source']] = df_inter2[['source','target','m_source','m_target']] \n",
    "    df_inter = pd.concat([df_inter,df_inter2])\n",
    "    df_inter['a'] = params[1] *((df_inter['m_prod'])/ ((df_inter['dist']+c)**params[0]))\n",
    "    df_intra['a'] = 2*params[2] *df_intra['m_source']\n",
    "    df = pd.concat([df_inter,df_intra])\n",
    "    df = df.groupby(['source','m_source']).a.sum().to_frame().reset_index()\n",
    "    df['m_source'] = df['m_source'] + (W/N)*df['a']\n",
    "    df = df[['source','m_source']].rename(columns={'source':'institution_id','m_source':'strength'})\n",
    "    inst_str_dict = df.set_index('institution_id').to_dict()['strength']\n",
    "    return inst_str_dict\n",
    "\n",
    "def model_function1(df_intra,df_inter,a,c,params):    \n",
    "    N = ( params[1] *((df_inter['m_prod'])/ ((df_inter['dist']+c)**params[0])) ).sum() + (params[2] *df_intra['m_source']).sum() \n",
    "    u = ( (params[2] *df_intra['m_source']).sum() )  / N\n",
    "    return u\n",
    "    \n",
    "def model_function2(df_intra,df_inter,a,c,params):\n",
    "    N = ( params[1] *((df_inter['m_prod'])/ ((df_inter['dist']+c)**params[0])) ).sum() + (params[2] *df_intra['m_source']).sum()\n",
    "    u = (  ( (df_inter['dist']) * ( ((params[1] *df_inter['m_prod'])/ ((df_inter['dist']+c)**(params[0]))) ) ).sum() ) / N\n",
    "    return u\n",
    "    \n",
    "def objective_function(params,a,c, x_data, y_data):\n",
    "    df_intra = x_data[0]\n",
    "    df_inter = x_data[1]\n",
    "\n",
    "    y_pred1 = model_function1(df_intra,df_inter,a,c,params)\n",
    "    y_pred2 = model_function2(df_intra,df_inter,a,c,params)\n",
    "    of = ((y_pred1 - y_data[0]) / y_data[0])**2  +  (((y_pred2 - y_data[1]) / y_data[1])**2)  \n",
    "\n",
    "    return of \n",
    "\n",
    "def model():\n",
    "    works = read_parquet(basepath / 'works')\n",
    "    works_authors_aff = read_parquet(basepath / 'works_authors_aff')\n",
    "    works_all = set(works.work_id)\n",
    "    N_dict = works_authors_aff.groupby('publication_date_1').work_id.nunique().to_frame().to_dict()['work_id']\n",
    "    months_list = list(N_dict.keys())\n",
    "    months_list.sort()\n",
    "    months_list = [i.strftime('%Y-%m-%d') for i in months_list]\n",
    "    start_index = 0\n",
    "    end_index = 120 #180\n",
    "    start_month = months_list[start_index]\n",
    "    end_month = months_list[end_index-1]\n",
    "    my_file = \"df_strengths0.csv\"     \n",
    "    df = pd.read_csv(os.path.join(my_path_, my_file))\n",
    "    inst_str_dict = df.set_index('institution_id').to_dict()['strength']\n",
    "    my_file = \"inst_set.pickle\"\n",
    "    with open(os.path.join(my_path_, my_file),\"rb\") as fp:\n",
    "        inst_set = pickle.load(fp)    \n",
    "    # #calculate d_ij \n",
    "    my_file = \"I_dist_threshold.csv\"\n",
    "    I_dist = read_parquet(basepath / 'I_dist_threshold')\n",
    "    I_dist = I_dist[['source','target','dist']].reset_index(drop=True)\n",
    "    I_dist = I_dist.query('source.isin(@inst_set) & target.isin(@inst_set)')\n",
    "\n",
    "    #calculate #each month: F_INTRA\n",
    "\n",
    "    my_file = \"df_data1.csv\"    \n",
    "    df_data1 = pd.read_csv(os.path.join(my_path_, my_file))\n",
    "    df_data1 = df_data1.set_index('publication_date_1')\n",
    "    df_data3 = df_data1\n",
    "    df_data3 = df_data3[['total']]\n",
    "    df_data3['total'] = df_data3['total'].astype(int)\n",
    "    df_data3 = df_data3.loc[months_list[end_index]:months_list[-1]]\n",
    "    df_data3_list = list(df_data3['total'])\n",
    "    df_data1 = df_data1[['frac_intra']]\n",
    "    df_data1 = df_data1.rename(columns={'frac_intra':'F_aff'})\n",
    "    df_data1 = df_data1.loc[months_list[end_index]:months_list[-1]] #[months_list[end_index-1]:months_list[-1]]\n",
    "    #each month: average distance\n",
    "    my_file = \"work_edges_dist_mean_monthly.csv\"    \n",
    "    df_data2 = pd.read_csv(os.path.join(my_path_, my_file))\n",
    "    df_data2 = df_data2.set_index('publication_date_1').loc[months_list[end_index]:months_list[-1]]\n",
    "\n",
    "    I_dist = I_dist.query('source.isin(@inst_set) & target.isin(@inst_set)')\n",
    "\n",
    "    c = 100\n",
    "    a = 0.5\n",
    "    \n",
    "    df = I_dist.copy()\n",
    "    df['m_source'] = df['source'].map(inst_str_dict)\n",
    "    df['m_target'] = df['target'].map(inst_str_dict)\n",
    "    df['m_prod'] = (df['m_source']*df['m_target'])**a\n",
    "    df_intra = df[df.source == df.target]\n",
    "    df_inter = df[df.source != df.target]\n",
    "\n",
    "    opt_dict = {}\n",
    "    x_data = [df_intra,df_inter]\n",
    "    y_data = np.array([list(df_data1['F_aff'])[0],list(df_data2['dist'])[0]])\n",
    "\n",
    "    #randomstart #starting point can be not feasible\n",
    "    np.random.seed(0)\n",
    "    initial_params_list = [[2.0,1.0,1.0]] + [list(np.concatenate([np.random.uniform(0, 5, 1),np.random.uniform(0, 1e3, 2)])) for _ in range(9)]\n",
    "    err_ = +np.inf\n",
    "    initial_params_ = np.nan\n",
    "    result_ = np.nan\n",
    "    for initial_params in tqdm(initial_params_list):\n",
    "        result = minimize(objective_function, initial_params, args=(a,c,x_data,y_data), bounds=((0, np.inf), (0, np.inf), (0, np.inf)), tol = 1e-10, options={'eps': 1e-10, 'ftol': 1e-15}) #, method='SLSQP'\n",
    "        err = result.fun\n",
    "        success = result.success\n",
    "        print(err,success,result.message)\n",
    "        if err<err_ and success:\n",
    "            initial_params_ = initial_params\n",
    "            result_ = result\n",
    "            err_ = err\n",
    "\n",
    "#     my_file = \"initial_params_randomstart.pickle\"\n",
    "#     pickle.dump([initial_params_,result_], open(os.path.join(my_path_, my_file), 'wb'))\n",
    "\n",
    "#     # #run on the best one\n",
    "#     # my_file = \"initial_params_randomstart.pickle\"\n",
    "#     # with open(os.path.join(my_path_, my_file),\"rb\") as fp:\n",
    "#     #     [initial_params_,result_] = pickle.load(fp)\n",
    "\n",
    "    params = result_.x\n",
    "    err = result_.fun\n",
    "    success = result_.success\n",
    "    message = result_.message\n",
    "    print(f'{0} {list(df_data3.index)[0]} {params[0]:.5f} {params[1]:.5f} {params[2]:.5f} {params[1]/params[2]:.5f}  {err:.2e} {success} {message}')\n",
    "    opt_dict[0] = {'optimized_alpha':params[0], 'optimized_beta':params[1], 'optimized_gamma':params[2],'beta/gamma':params[1]/params[2],'optimized_err':err, 'success':success, 'message':message}\n",
    "\n",
    "    #update strenght with parameters\n",
    "    for i in tqdm(range(len(df_data3_list)-1)):\n",
    "        W = int(1e5) #df_data3_list[i]\n",
    "        inst_str_dict = strength_update(df_intra,df_inter,W,a,c,params)\n",
    "\n",
    "        df = I_dist.copy()\n",
    "        df['m_source'] = df['source'].map(inst_str_dict)\n",
    "        df['m_target'] = df['target'].map(inst_str_dict)\n",
    "        df['m_prod'] = (df['m_source']*df['m_target'])**a\n",
    "        #df_intra = df[df.dist==0]\n",
    "        #df_intra = df[df.dist>0]\n",
    "        df_intra = df[df.source == df.target]\n",
    "        df_inter = df[df.source != df.target]\n",
    "\n",
    "        x_data = [df_intra,df_inter]\n",
    "        y_data = np.array([list(df_data1['F_aff'])[i+1],list(df_data2['dist'])[i+1]])\n",
    "\n",
    "        result = minimize(objective_function, params, args=(a,c,x_data,y_data), bounds=((0, np.inf), (0, np.inf), (0, np.inf)), tol = 1e-10, options={'eps': 1e-10, 'ftol': 1e-15}) \n",
    "        params = result.x\n",
    "        err = result.fun\n",
    "        success = result.success\n",
    "        message = result.message\n",
    "        print(f'{i+1} {list(df_data3.index)[i+1]} {params[0]:.5f} {params[1]:.5f} {params[2]:.5f} {params[1]/params[2]:.5f} {err:.2e} {success} {message}')\n",
    "        opt_dict[i+1] = {'optimized_alpha':params[0], 'optimized_beta':params[1], 'optimized_gamma':params[2],'beta/gamma':params[1]/params[2],'optimized_err':err, 'success':success,'message':message}\n",
    "\n",
    "    opt_df = pd.DataFrame.from_dict(opt_dict).T\n",
    "    opt_df['month'] = list(df_data3.index)\n",
    "    my_file = \"opt_df.csv\"   \n",
    "    opt_df.to_csv(os.path.join(my_path_, my_file),index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab2e5b8-37a2-4dd6-91bb-7bfc26530675",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c048ac08-687b-4b1a-b6c6-2fe4e7c88094",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7a20a2-5589-43f7-a976-caac352b9a9d",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_model(df,ylabel,title,color,log=False):\n",
    "    plt.style.use(\"dark_background\")\n",
    "    fig, ax = plt.subplots(figsize=(15, 5))   \n",
    "    if log:\n",
    "        ax.semilogy(list(df['month']), df[ylabel], \"o-\", color=color, markersize=3)\n",
    "    else:\n",
    "        ax.plot(list(df['month']), df[ylabel], \"o-\", color=color, markersize=3)\n",
    "    ax.axvline(pd.Timestamp(2020, 3, 1),color='r')\n",
    "    plt.grid(True, linewidth=0.5)\n",
    "    ax.ticklabel_format(axis='y')\n",
    "    ax.set_xlabel('month',size=20)\n",
    "    ax.set_title(title,size=30)\n",
    "my_file = \"opt_df.csv\" \n",
    "opt_df = pd.read_csv(os.path.join(my_path_, my_file))\n",
    "opt_df['month'] = opt_df['month'].apply(pd.to_datetime)\n",
    "plot_model(opt_df,'optimized_alpha','Optimized alpha','orange')\n",
    "plot_model(opt_df,'beta/gamma','Optimized beta/gamma','red')\n",
    "plot_model(opt_df,'optimized_err','Optimized error','blue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da193348-6ebd-4b6d-b13b-d0677e6bfe14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0d1ebdd0-53df-4195-b93d-b0e9cb261f3a",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "### Simulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248852cc-a1c9-4785-b5c6-972338bca5c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def simulation():\n",
    "    c = 100\n",
    "    a = 0.5 \n",
    "    \n",
    "    my_file = \"opt_df.csv\" \n",
    "    opt_df = pd.read_csv(os.path.join(my_path_, my_file))\n",
    "    opt_df['month'] = opt_df['month'].apply(pd.to_datetime)\n",
    "    \n",
    "    works = read_parquet(basepath / 'works')\n",
    "    works_authors_aff = read_parquet(basepath / 'works_authors_aff')\n",
    "    works_all = set(works.work_id)\n",
    "    N_dict = works_authors_aff.groupby('publication_date_1').work_id.nunique().to_frame().to_dict()['work_id']\n",
    "    months_list = list(N_dict.keys())\n",
    "    months_list.sort()\n",
    "    months_list = [i.strftime('%Y-%m-%d') for i in months_list]\n",
    "    start_index = 0\n",
    "    end_index = 120 #180\n",
    "    start_month = months_list[start_index]\n",
    "    end_month = months_list[end_index-1]\n",
    "    my_file = \"df_strengths0.csv\"     \n",
    "    df_0 = pd.read_csv(os.path.join(my_path_, my_file))\n",
    "    my_file = \"inst_set.pickle\"\n",
    "    with open(os.path.join(my_path_, my_file),\"rb\") as fp:\n",
    "        inst_set = pickle.load(fp)       \n",
    "    \n",
    "    #calculate #each month: F_INTRA\n",
    "    my_file = \"df_data1.csv\"    \n",
    "    df_data1 = pd.read_csv(os.path.join(my_path_, my_file))\n",
    "    df_data1 = df_data1.set_index('publication_date_1')\n",
    "    df_data3 = df_data1\n",
    "    df_data3 = df_data3[['total']]\n",
    "    df_data3['total'] = df_data3['total'].astype(int)\n",
    "    df_data3 = df_data3.loc[months_list[end_index]:months_list[-1]]\n",
    "    df_data3_list = list(df_data3['total'])\n",
    "    df_data1 = df_data1[['frac_intra']]\n",
    "    df_data1 = df_data1.rename(columns={'frac_intra':'F_aff'})\n",
    "    df_data1 = df_data1.loc[months_list[end_index]:months_list[-1]] #[months_list[end_index-1]:months_list[-1]]\n",
    "    #each month: average distance\n",
    "    my_file = \"work_edges_dist_mean_monthly.csv\"    \n",
    "    df_data2 = pd.read_csv(os.path.join(my_path_, my_file))\n",
    "    df_data2 = df_data2.set_index('publication_date_1').loc[months_list[end_index]:months_list[-1]]\n",
    "    \n",
    "    my_file = \"I_dist_threshold.csv\"\n",
    "    I_dist = read_parquet(basepath / 'I_dist_threshold')\n",
    "    I_dist = I_dist[['source','target','dist']].reset_index(drop=True)\n",
    "    I_dist = I_dist.query('source.isin(@inst_set) & target.isin(@inst_set)')\n",
    "    \n",
    "    df_intra = I_dist[I_dist.source == I_dist.target].reset_index(drop=True)\n",
    "    df_inter = I_dist[I_dist.source != I_dist.target].reset_index(drop=True)\n",
    "    df_inter.index += len(df_intra)\n",
    "\n",
    "    import random\n",
    "    random.seed(0)\n",
    "    for s in tqdm(range(10)):\n",
    "\n",
    "        df_ = df_0\n",
    "\n",
    "        fra_intra_list = []\n",
    "        mean_dist_list = []\n",
    "        for i in tqdm(range(len(df_data3_list))):\n",
    "\n",
    "            #update \n",
    "            alpha = list(opt_df['optimized_alpha'])[i]\n",
    "            beta = list(opt_df['optimized_beta'])[i]\n",
    "            gamma = list(opt_df['optimized_gamma'])[i]\n",
    "            \n",
    "            inst_str_dict = df_[['institution_id','strength']].set_index('institution_id').to_dict()['strength']\n",
    "\n",
    "            df_intra['m_source'] = df_intra['source'].map(inst_str_dict)\n",
    "            df_inter['m_source'] = df_inter['source'].map(inst_str_dict)\n",
    "            df_inter['m_target'] = df_inter['target'].map(inst_str_dict)\n",
    "            df_inter['m_prod'] = (df_inter['m_source']*df_inter['m_target'])**a         \n",
    "\n",
    "            #edges probabilities\n",
    "            df_intra['p']= gamma*df_intra['m_source'] \n",
    "            df_inter['p'] = df_inter['m_prod'] * ( beta / ((df_inter['dist']+c)**alpha)) \n",
    "\n",
    "            hyperedges_probabilities = list(itertools.chain(df_intra['p'], df_inter['p']))\n",
    "            hyperedges_probabilities = np.array(hyperedges_probabilities)/(df_intra['p'].sum()+df_inter['p'].sum())\n",
    "            W = int(1e5) #df_data3_list[i] \n",
    "            hyperedges_model = random.choices(np.arange(0, len(hyperedges_probabilities)), weights=hyperedges_probabilities, k=W)\n",
    "\n",
    "            #count edges\n",
    "            counter = dict(collections.Counter(hyperedges_model))\n",
    "            df_intra['count'] = df_intra.index.to_series().map(counter)\n",
    "            df_intra['count'] = df_intra['count'].fillna(0)\n",
    "            df_inter['count'] = df_inter.index.to_series().map(counter)\n",
    "            df_inter['count'] = df_inter['count'].fillna(0)           \n",
    "            \n",
    "            #count intra-inter\n",
    "            frac_intra = sum(df_intra['count'])/(df_intra['count'].sum()+df_inter['count'].sum())\n",
    "            fra_intra_list.append(frac_intra)\n",
    "\n",
    "            #mean team distace\n",
    "            df_inter['mean_dist'] = df_inter['dist']*df_inter['count']\n",
    "            mean_dist = df_inter.mean_dist.sum()/W\n",
    "            mean_dist_list.append(mean_dist)\n",
    "\n",
    "            #update strength \n",
    "            df_intra['count'] = df_intra['count']*2 \n",
    "            temp = pd.concat([\n",
    "                df_intra[['source','count']].rename(columns={'source':'institution_id'}),\n",
    "                df_inter[['source','count']].rename(columns={'source':'institution_id'}),\n",
    "                df_inter[['target','count']].rename(columns={'target':'institution_id'})])\n",
    "            temp = temp.groupby('institution_id')['count'].sum().to_frame().reset_index()\n",
    "            df_old = df_\n",
    "            df_ = df_[['institution_id','strength']]\n",
    "            df_ = df_.merge(temp,on='institution_id',how='left')\n",
    "            df_['count'] = df_['count'].fillna(0)\n",
    "            df_['strength'] = df_['strength'] + df_['count']\n",
    "            df_ = df_[['institution_id','strength']]\n",
    "\n",
    "        model_data = pd.DataFrame.from_dict({'fra_intra':fra_intra_list,'mean_dist':mean_dist_list})\n",
    "        model_data['month'] = list(df_data3.index)\n",
    "        my_file = \"simulation_\"+str(s)+\".csv\"  \n",
    "        model_data.to_csv(os.path.join(my_path_, my_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0362d8af-e2db-4f2a-ab90-bbb4a9e6b50b",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "simulation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f996445-d5df-4a65-9dc0-48b798ebbb03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca77cd6-034f-4231-82db-76675b431fe9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_simulation():\n",
    "    \n",
    "    my_file = \"simulation_\"+str(0)+\".csv\"  \n",
    "    model_data = pd.read_csv(os.path.join(my_path_, my_file))\n",
    "    model_data['month'] = pd.to_datetime(model_data['month'])\n",
    "    months_list = list(model_data['month'])\n",
    "    months_list.sort()\n",
    "    months_list = [i.strftime('%Y-%m-%d') for i in months_list]\n",
    "    \n",
    "    #calculate #each month: F_INTRA\n",
    "    my_file = \"df_data1.csv\"    \n",
    "    df_data1 = pd.read_csv(os.path.join(my_path_, my_file))\n",
    "    df_data1 = df_data1.set_index('publication_date_1')\n",
    "    df_data1 = df_data1[['frac_intra']]\n",
    "    df_data1 = df_data1.rename(columns={'frac_intra':'F_aff'})\n",
    "    df_data1 = df_data1.loc[months_list] #[months_list[end_index]:months_list[-1]]\n",
    "    #each month: average distance\n",
    "    my_file = \"work_edges_dist_mean_monthly.csv\"    \n",
    "    work_edges_dist_mean_monthly = pd.read_csv(os.path.join(my_path_, my_file))\n",
    "    work_edges_dist_mean_monthly = work_edges_dist_mean_monthly.set_index('publication_date_1')\n",
    "    work_edges_dist_mean_monthly = work_edges_dist_mean_monthly.loc[months_list]\n",
    "    df_data2 = work_edges_dist_mean_monthly\n",
    "\n",
    "    for s in range(10):\n",
    "        my_file = \"simulation_\"+str(s)+\".csv\"  \n",
    "        model_data = pd.read_csv(os.path.join(my_path_, my_file))\n",
    "        model_data['month'] = pd.to_datetime(model_data['month'])\n",
    "        months_list = list(model_data['month'])\n",
    "        months_list.sort()\n",
    "        months_list = [i.strftime('%Y-%m-%d') for i in months_list]\n",
    "\n",
    "        plt.style.use(\"dark_background\")\n",
    "        fig, ax = plt.subplots(figsize=(15, 5))\n",
    "        x_dates = list(model_data['month'])\n",
    "        x_data = x_dates\n",
    "        y_data1 = list(df_data1.F_aff)   \n",
    "        y_data2 = list(model_data['fra_intra'])\n",
    "        ax.plot(x_data, y_data1, \"o-\", markersize=3,label='data')\n",
    "        ax.plot(x_data, y_data2, \"o-\", markersize=3,label='model')\n",
    "        ax.axvline(pd.Timestamp(2020, 3, 1),color='r')\n",
    "        plt.grid(True, linewidth=0.5)\n",
    "        ax.yaxis.set_major_formatter(formatter)\n",
    "        ax.set_xlabel('month',size=20)\n",
    "        ax.legend() \n",
    "        ax.set_title('Frac intra-insts collabs - with exact params',size=30)\n",
    "        \n",
    "    for s in range(10):\n",
    "        my_file = \"simulation_\"+str(s)+\".csv\"  \n",
    "        model_data = pd.read_csv(os.path.join(my_path_, my_file))\n",
    "        model_data['month'] = pd.to_datetime(model_data['month'])\n",
    "        months_list = list(model_data['month'])\n",
    "        months_list.sort()\n",
    "        months_list = [i.strftime('%Y-%m-%d') for i in months_list]\n",
    "\n",
    "        plt.style.use(\"dark_background\")\n",
    "        fig, ax = plt.subplots(figsize=(15, 5))\n",
    "        x_data = x_dates\n",
    "        y_data1 = list(df_data2.dist)   \n",
    "        y_data2 = list(model_data['mean_dist'])\n",
    "        ax.plot(x_data, y_data1, \"o-\", markersize=3,label='data')\n",
    "        ax.plot(x_data, y_data2, \"o-\", markersize=3,label='model')\n",
    "        ax.axvline(pd.Timestamp(2020, 3, 1),color='r')\n",
    "        plt.grid(True, linewidth=0.5)\n",
    "        ax.yaxis.set_major_formatter(formatter)\n",
    "        ax.set_xlabel('month',size=20)\n",
    "        ax.legend() \n",
    "        ax.set_title('Avg team dist - with exact params',size=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc33f80-4c7c-4a31-be75-11d28bd6c1c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_simulation()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b1d875-0b29-4a61-8d43-a248c0230fb4",
   "metadata": {},
   "source": [
    "## Model 1 variable\n",
    "\n",
    "B_{ij} = beta * ((s_i*s_j)^(1/2)/(d_{ij}+c)^alpha) if i!=j </br>\n",
    "B_{ii} = gamma * s_i if i!=j </br>\n",
    "\n",
    "P_{ij} = B_{ij}/N(alpha,beta,gamma)</br>\n",
    "P_{ii} = B_{ii}/N(alpha,beta,gamma)</br>\n",
    "with N(alpha,beta,gamma) = sum_{i} B_{ii} + sum_{(i,j)} B_{ij}</br>\n",
    "\n",
    "Parameters: c=10 </br>\n",
    "Variables: alpha>=0 </br>\n",
    "Fixed: beta>=0, gamma>=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b745c601-505f-419c-9f9b-68d51ea56cdc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "my_file = \"opt_df.csv\" \n",
    "opt_df = pd.read_csv(os.path.join(my_path_, my_file))\n",
    "opt_df['month'] = opt_df['month'].apply(pd.to_datetime)\n",
    "beta = opt_df['optimized_beta'].mean()\n",
    "gamma = opt_df['optimized_gamma'].mean()\n",
    "print(f'beta {beta}, gamma {gamma}, beta/gamma {beta/gamma}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0362fc69-7015-42bc-8c34-b3f8ce57f3a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_collaboration_graph(works_authors_rows):\n",
    "    \n",
    "    authors_id_set = set(works_authors_rows.author_id)\n",
    "                                  \n",
    "    bip_g = nx.from_pandas_edgelist(\n",
    "        works_authors_rows,\n",
    "        source='work_id', target='author_id'\n",
    "    )\n",
    "    \n",
    "    collab_graph = nx.bipartite.weighted_projected_graph(bip_g,nodes=authors_id_set) #bipartite.weighted_projected_graph(bip_g,nodes=authors_id)\n",
    "    return collab_graph\n",
    "\n",
    "def strength_update(df_intra,df_inter,W,a,c,beta,gamma,params):\n",
    "    N = ( beta *((df_inter['m_prod'])/ ((df_inter['dist']+c)**params)) ).sum() + (gamma *df_intra['m_source']).sum() \n",
    "    df_inter2 = df_inter.copy()\n",
    "    df_inter2[['target','source','m_target','m_source']] = df_inter2[['source','target','m_source','m_target']] \n",
    "    df_inter = pd.concat([df_inter,df_inter2])\n",
    "    df_inter['a'] = beta *((df_inter['m_prod'])/ ((df_inter['dist']+c)**params))\n",
    "    df_intra['a'] = 2*gamma *df_intra['m_source']\n",
    "    df = pd.concat([df_inter,df_intra])\n",
    "    df = df.groupby(['source','m_source']).a.sum().to_frame().reset_index()\n",
    "    df['m_source'] = df['m_source'] + (W/N)*df['a']\n",
    "    df = df[['source','m_source']].rename(columns={'source':'institution_id','m_source':'strength'})\n",
    "    inst_str_dict = df.set_index('institution_id').to_dict()['strength']\n",
    "    return inst_str_dict\n",
    "\n",
    "def model_function1(df_intra,df_inter,a,c,beta,gamma,params):    \n",
    "    N = ( beta *((df_inter['m_prod'])/ ((df_inter['dist']+c)**params)) ).sum() + (gamma *df_intra['m_source']).sum() \n",
    "    u = ( (gamma *df_intra['m_source']).sum() )  / N\n",
    "    return u\n",
    "    \n",
    "def model_function2(df_intra,df_inter,a,c,beta,gamma,params):\n",
    "    N = ( beta *((df_inter['m_prod'])/ ((df_inter['dist']+c)**params)) ).sum() + (gamma *df_intra['m_source']).sum()\n",
    "    u = (  ( (df_inter['dist']) * ( ((beta *df_inter['m_prod'])/ ((df_inter['dist']+c)**(params))) ) ).sum() ) / N\n",
    "    return u\n",
    "    \n",
    "def objective_function(params,a,c,beta,gamma, x_data, y_data):\n",
    "    df_intra = x_data[0]\n",
    "    df_inter = x_data[1]\n",
    "\n",
    "    y_pred1 = model_function1(df_intra,df_inter,a,c,beta,gamma,params)\n",
    "    y_pred2 = model_function2(df_intra,df_inter,a,c,beta,gamma,params)\n",
    "    of = ((y_pred1 - y_data[0]) / y_data[0])**2  +  (((y_pred2 - y_data[1]) / y_data[1])**2)  \n",
    "\n",
    "    return of \n",
    "\n",
    "def model_b():\n",
    "    \n",
    "    works = read_parquet(basepath / 'works')\n",
    "    works_authors_aff = read_parquet(basepath / 'works_authors_aff')\n",
    "    works_all = set(works.work_id)\n",
    "    N_dict = works_authors_aff.groupby('publication_date_1').work_id.nunique().to_frame().to_dict()['work_id']\n",
    "    months_list = list(N_dict.keys())\n",
    "    months_list.sort()\n",
    "    months_list = [i.strftime('%Y-%m-%d') for i in months_list]\n",
    "    start_index = 0\n",
    "    end_index = 120 #180\n",
    "    start_month = months_list[start_index]\n",
    "    end_month = months_list[end_index-1]\n",
    "    my_file = \"df_strengths0.csv\"     \n",
    "    df = pd.read_csv(os.path.join(my_path_, my_file))\n",
    "    inst_str_dict = df.set_index('institution_id').to_dict()['strength']\n",
    "    my_file = \"inst_set.pickle\"\n",
    "    with open(os.path.join(my_path_, my_file),\"rb\") as fp:\n",
    "        inst_set = pickle.load(fp)    \n",
    "    # #calculate d_ij \n",
    "    my_file = \"I_dist_threshold.csv\"\n",
    "    I_dist = read_parquet(basepath / 'I_dist_threshold')\n",
    "    I_dist = I_dist[['source','target','dist']].reset_index(drop=True)\n",
    "    I_dist = I_dist.query('source.isin(@inst_set) & target.isin(@inst_set)')\n",
    "\n",
    "    #calculate #each month: F_INTRA\n",
    "\n",
    "    my_file = \"df_data1.csv\"    \n",
    "    df_data1 = pd.read_csv(os.path.join(my_path_, my_file))\n",
    "    df_data1 = df_data1.set_index('publication_date_1')\n",
    "    df_data3 = df_data1\n",
    "    df_data3 = df_data3[['total']]\n",
    "    df_data3['total'] = df_data3['total'].astype(int)\n",
    "    df_data3 = df_data3.loc[months_list[end_index]:months_list[-1]]\n",
    "    df_data3_list = list(df_data3['total'])\n",
    "    df_data1 = df_data1[['frac_intra']]\n",
    "    df_data1 = df_data1.rename(columns={'frac_intra':'F_aff'})\n",
    "    df_data1 = df_data1.loc[months_list[end_index]:months_list[-1]] #[months_list[end_index-1]:months_list[-1]]\n",
    "    #each month: average distance\n",
    "    my_file = \"work_edges_dist_mean_monthly.csv\"    \n",
    "    df_data2 = pd.read_csv(os.path.join(my_path_, my_file))\n",
    "    df_data2 = df_data2.set_index('publication_date_1').loc[months_list[end_index]:months_list[-1]]\n",
    "\n",
    "    I_dist = I_dist.query('source.isin(@inst_set) & target.isin(@inst_set)')\n",
    "\n",
    "    c = 100\n",
    "    a = 0.5\n",
    "    \n",
    "    df = I_dist.copy()\n",
    "    df['m_source'] = df['source'].map(inst_str_dict)\n",
    "    df['m_target'] = df['target'].map(inst_str_dict)\n",
    "    df['m_prod'] = (df['m_source']*df['m_target'])**a\n",
    "    df_intra = df[df.source == df.target]\n",
    "    df_inter = df[df.source != df.target]\n",
    "\n",
    "    opt_dict = {}\n",
    "    x_data = [df_intra,df_inter]\n",
    "    y_data = np.array([list(df_data1['F_aff'])[0],list(df_data2['dist'])[0]])\n",
    "\n",
    "    my_file = \"opt_df.csv\" \n",
    "    opt_df = pd.read_csv(os.path.join(my_path_, my_file))\n",
    "    opt_df['month'] = opt_df['month'].apply(pd.to_datetime)\n",
    "    beta = opt_df['optimized_beta'].mean()\n",
    "    gamma = opt_df['optimized_gamma'].mean()\n",
    "    \n",
    "    #randomstart #starting point can be not feasible\n",
    "    np.random.seed(0)\n",
    "    initial_params_list = [2.0] + [np.random.uniform(0, 10, 1)[0] for _ in range(19)]\n",
    "    err_ = +np.inf\n",
    "    initial_params_ = np.nan\n",
    "    result_ = np.nan\n",
    "    for initial_params in tqdm(initial_params_list):\n",
    "        result = minimize(objective_function, initial_params, args=(a,c,beta,gamma,x_data,y_data), bounds=[(0, np.inf)], tol = 1e-10, options={'eps': 1e-10, 'ftol': 1e-15}) #, method='SLSQP'\n",
    "        err = result.fun\n",
    "        success = result.success\n",
    "        print(err,success,result.message)\n",
    "        if err<err_ and success:\n",
    "            initial_params_ = initial_params\n",
    "            result_ = result\n",
    "            err_ = err\n",
    "\n",
    "    params = result_.x\n",
    "    err = result_.fun\n",
    "    success = result_.success\n",
    "    message = result_.message\n",
    "    print(f'{0} {list(df_data3.index)[0]} {params[0]:.5f} {err:.2e} {success} {message}')\n",
    "    opt_dict[0] = {'optimized_alpha':params[0], 'optimized_err':err, 'success':success, 'message':message}\n",
    "\n",
    "    #update strenght with parameters\n",
    "    for i in tqdm(range(len(df_data3_list)-1)):\n",
    "        W = int(1e5) #df_data3_list[i]\n",
    "        inst_str_dict = strength_update(df_intra,df_inter,W,a,c,beta,gamma,params)\n",
    "\n",
    "        df = I_dist.copy()\n",
    "        df['m_source'] = df['source'].map(inst_str_dict)\n",
    "        df['m_target'] = df['target'].map(inst_str_dict)\n",
    "        df['m_prod'] = (df['m_source']*df['m_target'])**a\n",
    "        df_intra = df[df.source == df.target]\n",
    "        df_inter = df[df.source != df.target]\n",
    "\n",
    "        x_data = [df_intra,df_inter]\n",
    "        y_data = np.array([list(df_data1['F_aff'])[i+1],list(df_data2['dist'])[i+1]])\n",
    "\n",
    "        result = minimize(objective_function, params, args=(a,c,beta,gamma,x_data,y_data), bounds=[(0, np.inf)], tol = 1e-10, options={'eps': 1e-10, 'ftol': 1e-15}) \n",
    "        params = result.x\n",
    "        err = result.fun\n",
    "        success = result.success\n",
    "        message = result.message\n",
    "        print(f'{i+1} {list(df_data3.index)[i+1]} {params[0]:.5f} {err:.2e} {success} {message}')\n",
    "        opt_dict[i+1] = {'optimized_alpha':params[0],'optimized_err':err, 'success':success,'message':message}\n",
    "\n",
    "    opt_df = pd.DataFrame.from_dict(opt_dict).T\n",
    "    opt_df['month'] = list(df_data3.index)\n",
    "    my_file = \"opt_df_b.csv\"   \n",
    "    opt_df.to_csv(os.path.join(my_path_, my_file),index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9256f6-02a1-4bde-998b-b77697ac3a9b",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_b()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0017218f-a4ac-4178-8478-19d0a629bbf2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_model_b(df,ylabel,title,color,log=False):\n",
    "    plt.style.use(\"dark_background\")\n",
    "    fig, ax = plt.subplots(figsize=(15, 5))   \n",
    "    if log:\n",
    "        ax.semilogy(list(df['month']), df[ylabel], \"o-\", color=color, markersize=3)\n",
    "    else:\n",
    "        ax.plot(list(df['month']), df[ylabel], \"o-\", color=color, markersize=3)\n",
    "    ax.axvline(pd.Timestamp(2020, 3, 1),color='r')\n",
    "    plt.grid(True, linewidth=0.5)\n",
    "    ax.ticklabel_format(axis='y')\n",
    "    ax.set_xlabel('month',size=20)\n",
    "    ax.set_title(title,size=30)\n",
    "my_file = \"opt_df_b.csv\" \n",
    "opt_df = pd.read_csv(os.path.join(my_path_, my_file))\n",
    "opt_df['month'] = opt_df['month'].apply(pd.to_datetime)\n",
    "plot_model_b(opt_df,'optimized_alpha','Optimized alpha','orange')\n",
    "plot_model_b(opt_df,'optimized_err','Optimized error','blue')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d8c0324-fbee-4d45-851b-a4075f6bdd46",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "### Simulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb8e1f7-318e-43e5-af93-b406f1e0df8b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def simulation_b():\n",
    "    c = 100 \n",
    "    a = 0.5 \n",
    "    \n",
    "    my_file = \"opt_df.csv\" \n",
    "    opt_df = pd.read_csv(os.path.join(my_path_, my_file))\n",
    "    opt_df['month'] = opt_df['month'].apply(pd.to_datetime)\n",
    "    beta = opt_df['optimized_beta'].mean()\n",
    "    gamma = opt_df['optimized_gamma'].mean()\n",
    "    \n",
    "    my_file = \"opt_df_b.csv\" \n",
    "    opt_df = pd.read_csv(os.path.join(my_path_, my_file))\n",
    "    opt_df['month'] = opt_df['month'].apply(pd.to_datetime)\n",
    "    \n",
    "    works = read_parquet(basepath / 'works')\n",
    "    works_authors_aff = read_parquet(basepath / 'works_authors_aff')\n",
    "    works_all = set(works.work_id)\n",
    "    works_authors_aff = works_authors_aff[works_authors_aff.work_id.isin(works_all)]\n",
    "    N_dict = works_authors_aff.groupby('publication_date_1').work_id.nunique().to_frame().to_dict()['work_id']\n",
    "    months_list = list(N_dict.keys())\n",
    "    months_list.sort()\n",
    "    months_list = [i.strftime('%Y-%m-%d') for i in months_list]\n",
    "    start_index = 0\n",
    "    end_index = 120 #180\n",
    "    start_month = months_list[start_index]\n",
    "    end_month = months_list[end_index-1]\n",
    "    my_file = \"df_strengths0.csv\"     \n",
    "    df_0 = pd.read_csv(os.path.join(my_path_, my_file))\n",
    "    my_file = \"inst_set.pickle\"\n",
    "    with open(os.path.join(my_path_, my_file),\"rb\") as fp:\n",
    "        inst_set = pickle.load(fp)       \n",
    "    \n",
    "    #calculate #each month: F_INTRA\n",
    "    my_file = \"df_data1.csv\"    \n",
    "    df_data1 = pd.read_csv(os.path.join(my_path_, my_file))\n",
    "    df_data1 = df_data1.set_index('publication_date_1')\n",
    "    df_data3 = df_data1\n",
    "    df_data3 = df_data3[['total']]\n",
    "    df_data3['total'] = df_data3['total'].astype(int)\n",
    "    df_data3 = df_data3.loc[months_list[end_index]:months_list[-1]]\n",
    "    df_data3_list = list(df_data3['total'])\n",
    "    df_data1 = df_data1[['frac_intra']]\n",
    "    df_data1 = df_data1.rename(columns={'frac_intra':'F_aff'})\n",
    "    df_data1 = df_data1.loc[months_list[end_index]:months_list[-1]] #[months_list[end_index-1]:months_list[-1]]\n",
    "    #each month: average distance\n",
    "    my_file = \"work_edges_dist_mean_monthly.csv\"    \n",
    "    df_data2 = pd.read_csv(os.path.join(my_path_, my_file))\n",
    "    df_data2 = df_data2.set_index('publication_date_1').loc[months_list[end_index]:months_list[-1]]\n",
    "    \n",
    "    my_file = \"I_dist_threshold.csv\"\n",
    "    I_dist = read_parquet(basepath / 'I_dist_threshold')\n",
    "    I_dist = I_dist[['source','target','dist']].reset_index(drop=True)\n",
    "    I_dist = I_dist.query('source.isin(@inst_set) & target.isin(@inst_set)')\n",
    "    \n",
    "    df_intra = I_dist[I_dist.source == I_dist.target].reset_index(drop=True)\n",
    "    df_inter = I_dist[I_dist.source != I_dist.target].reset_index(drop=True)\n",
    "    df_inter.index += len(df_intra)\n",
    "\n",
    "    import random\n",
    "    random.seed(0)\n",
    "    for s in tqdm(range(10)):\n",
    "\n",
    "        df_ = df_0\n",
    "\n",
    "        fra_intra_list = []\n",
    "        mean_dist_list = []\n",
    "        for i in tqdm(range(len(df_data3_list))):\n",
    "\n",
    "            #update \n",
    "            alpha = list(opt_df['optimized_alpha'])[i]\n",
    "            \n",
    "            inst_str_dict = df_[['institution_id','strength']].set_index('institution_id').to_dict()['strength']\n",
    "\n",
    "            df_intra['m_source'] = df_intra['source'].map(inst_str_dict)\n",
    "            df_inter['m_source'] = df_inter['source'].map(inst_str_dict)\n",
    "            df_inter['m_target'] = df_inter['target'].map(inst_str_dict)\n",
    "            df_inter['m_prod'] = (df_inter['m_source']*df_inter['m_target'])**a         \n",
    "\n",
    "            #edges probabilities\n",
    "            df_intra['p']= gamma*df_intra['m_source'] \n",
    "            df_inter['p'] = df_inter['m_prod'] * ( beta / ((df_inter['dist']+c)**alpha)) \n",
    "\n",
    "            hyperedges_probabilities = list(itertools.chain(df_intra['p'], df_inter['p']))\n",
    "            hyperedges_probabilities = np.array(hyperedges_probabilities)/(df_intra['p'].sum()+df_inter['p'].sum())\n",
    "            W = int(1e5) #df_data3_list[i] \n",
    "            hyperedges_model = random.choices(np.arange(0, len(hyperedges_probabilities)), weights=hyperedges_probabilities, k=W)\n",
    "\n",
    "            #count edges\n",
    "            counter = dict(collections.Counter(hyperedges_model))\n",
    "            df_intra['count'] = df_intra.index.to_series().map(counter)\n",
    "            df_intra['count'] = df_intra['count'].fillna(0)\n",
    "            df_inter['count'] = df_inter.index.to_series().map(counter)\n",
    "            df_inter['count'] = df_inter['count'].fillna(0)           \n",
    "            \n",
    "            #count intra-inter\n",
    "            frac_intra = sum(df_intra['count'])/(df_intra['count'].sum()+df_inter['count'].sum())\n",
    "            fra_intra_list.append(frac_intra)\n",
    "\n",
    "            #mean team distace\n",
    "            df_inter['mean_dist'] = df_inter['dist']*df_inter['count']\n",
    "            mean_dist = df_inter.mean_dist.sum()/W\n",
    "            mean_dist_list.append(mean_dist)\n",
    "\n",
    "            #update strength \n",
    "            df_intra['count'] = df_intra['count']*2 \n",
    "            temp = pd.concat([\n",
    "                df_intra[['source','count']].rename(columns={'source':'institution_id'}),\n",
    "                df_inter[['source','count']].rename(columns={'source':'institution_id'}),\n",
    "                df_inter[['target','count']].rename(columns={'target':'institution_id'})])\n",
    "            temp = temp.groupby('institution_id')['count'].sum().to_frame().reset_index()\n",
    "            df_old = df_\n",
    "            df_ = df_[['institution_id','strength']]\n",
    "            df_ = df_.merge(temp,on='institution_id',how='left')\n",
    "            df_['count'] = df_['count'].fillna(0)\n",
    "            df_['strength'] = df_['strength'] + df_['count']\n",
    "            df_ = df_[['institution_id','strength']]\n",
    "\n",
    "        model_data = pd.DataFrame.from_dict({'fra_intra':fra_intra_list,'mean_dist':mean_dist_list})\n",
    "        model_data['month'] = list(df_data3.index)\n",
    "        my_file = \"simulation_\"+str(s)+\"_b.csv\" \n",
    "        model_data.to_csv(os.path.join(my_path_, my_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70aaa5f3-b643-43fb-be54-1fc9ef3655c0",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "simulation_b()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281ccd06-f5f6-4a9e-af81-ffa2bfb7b851",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae60eb52-c9f6-4411-813c-2f098d7d7950",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_simulation_b():\n",
    "    \n",
    "    my_file = \"simulation_\"+str(0)+\"_b.csv\"\n",
    "    model_data = pd.read_csv(os.path.join(my_path_, my_file))\n",
    "    model_data['month'] = pd.to_datetime(model_data['month'])\n",
    "    months_list = list(model_data['month'])\n",
    "    months_list.sort()\n",
    "    months_list = [i.strftime('%Y-%m-%d') for i in months_list]\n",
    "    \n",
    "    #calculate #each month: F_INTRA\n",
    "    my_file = \"df_data1.csv\"    \n",
    "    df_data1 = pd.read_csv(os.path.join(my_path_, my_file))\n",
    "    df_data1 = df_data1.set_index('publication_date_1')\n",
    "    df_data1 = df_data1[['frac_intra']]\n",
    "    df_data1 = df_data1.rename(columns={'frac_intra':'F_aff'})\n",
    "    df_data1 = df_data1.loc[months_list] #[months_list[end_index]:months_list[-1]]\n",
    "    #each month: average distance\n",
    "    my_file = \"work_edges_dist_mean_monthly.csv\"    \n",
    "    work_edges_dist_mean_monthly = pd.read_csv(os.path.join(my_path_, my_file))\n",
    "    work_edges_dist_mean_monthly = work_edges_dist_mean_monthly.set_index('publication_date_1')\n",
    "    work_edges_dist_mean_monthly = work_edges_dist_mean_monthly.loc[months_list]\n",
    "    df_data2 = work_edges_dist_mean_monthly\n",
    "\n",
    "    for s in range(10):\n",
    "        my_file = \"simulation_\"+str(s)+\"_b.csv\" \n",
    "        model_data = pd.read_csv(os.path.join(my_path_, my_file))\n",
    "        model_data['month'] = pd.to_datetime(model_data['month'])\n",
    "        months_list = list(model_data['month'])\n",
    "        months_list.sort()\n",
    "        months_list = [i.strftime('%Y-%m-%d') for i in months_list]\n",
    "\n",
    "        plt.style.use(\"dark_background\")\n",
    "        fig, ax = plt.subplots(figsize=(15, 5))\n",
    "        x_dates = list(model_data['month'])\n",
    "        x_data = x_dates\n",
    "        y_data1 = list(df_data1.F_aff)   \n",
    "        y_data2 = list(model_data['fra_intra'])\n",
    "        ax.plot(x_data, y_data1, \"o-\", markersize=3,label='data')\n",
    "        ax.plot(x_data, y_data2, \"o-\", markersize=3,label='model')\n",
    "        ax.axvline(pd.Timestamp(2020, 3, 1),color='r')\n",
    "        plt.grid(True, linewidth=0.5)\n",
    "        ax.yaxis.set_major_formatter(formatter)\n",
    "        ax.set_xlabel('month',size=20)\n",
    "        ax.legend() \n",
    "        ax.set_title('Frac intra-insts collabs - with exact params',size=30)\n",
    "        \n",
    "    for s in range(10):\n",
    "        my_file = \"simulation_\"+str(s)+\"_b.csv\" \n",
    "        model_data = pd.read_csv(os.path.join(my_path_, my_file))\n",
    "        model_data['month'] = pd.to_datetime(model_data['month'])\n",
    "        months_list = list(model_data['month'])\n",
    "        months_list.sort()\n",
    "        months_list = [i.strftime('%Y-%m-%d') for i in months_list]\n",
    "\n",
    "        plt.style.use(\"dark_background\")\n",
    "        fig, ax = plt.subplots(figsize=(15, 5))\n",
    "        x_data = x_dates\n",
    "        y_data1 = list(df_data2.dist)   \n",
    "        y_data2 = list(model_data['mean_dist'])\n",
    "        ax.plot(x_data, y_data1, \"o-\", markersize=3,label='data')\n",
    "        ax.plot(x_data, y_data2, \"o-\", markersize=3,label='model')\n",
    "        ax.axvline(pd.Timestamp(2020, 3, 1),color='r')\n",
    "        plt.grid(True, linewidth=0.5)\n",
    "        ax.yaxis.set_major_formatter(formatter)\n",
    "        ax.set_xlabel('month',size=20)\n",
    "        ax.legend() \n",
    "        ax.set_title('Avg team dist - with exact params',size=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e983f6a-f0b7-4150-a3d9-0bc08e2bc578",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_simulation_b()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979e39fd-f2bc-4f42-8e2f-79d11acf1d49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8edb07ce-3cae-4753-a1fe-1de562d49e1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "226cb8d6-3253-4b4f-b22d-93a5ecda82fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0b44ca-663f-4361-90de-3960720feece",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
