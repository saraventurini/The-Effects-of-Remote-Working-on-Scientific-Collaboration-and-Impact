{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b47db8b8-d1f3-4a77-8fe0-e60e23ff6774",
   "metadata": {},
   "source": [
    "# Model - info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30397a63-4e1c-43f7-acbb-029c82751e59",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from pathlib import Path\n",
    "import networkx as nx\n",
    "import igraph as ig\n",
    "import pickle\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.spatial import distance\n",
    "import seaborn as sns\n",
    "import time\n",
    "from tqdm.auto import tqdm\n",
    "import random \n",
    "import os\n",
    "from itertools import chain, combinations\n",
    "import itertools\n",
    "import scipy\n",
    "\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "import plotly.io as pio\n",
    "plt.rcParams.update({'font.size': 22})\n",
    "sns.set(style=\"ticks\", context=\"talk\")\n",
    "plt.style.use(\"dark_background\")\n",
    "pd.options.plotting.backend = 'plotly'\n",
    "pio.templates.default = 'plotly_dark+presentation'\n",
    "\n",
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "#pd.options.mode.chained_assignment = None \n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "import matplotlib.dates as dates\n",
    "import matplotlib.dates as mdates\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "from scipy import optimize\n",
    "import numpy.polynomial.polynomial as npoly\n",
    "\n",
    "def form(x,pos):\n",
    "    if x<1e3:\n",
    "        return '%1.3f' % (x)\n",
    "    elif x<1e6:\n",
    "        return '%1.1fK' % (x * 1e-3)\n",
    "    else:\n",
    "        return '%1.1fM' % (x * 1e-6)\n",
    "formatter = FuncFormatter(form)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def read_parquet(name, **args):\n",
    "    path = name\n",
    "    print(f'Reading {name!r}')\n",
    "    tic = time.time()\n",
    "    df = pd.read_parquet(path, engine='fastparquet', **args)\n",
    "    before = len(df)\n",
    "    # df.drop_duplicates(inplace=True)\n",
    "    toc = time.time()\n",
    "    after = len(df)\n",
    "        \n",
    "    print(f'Read {len(df):,} rows from {path.stem!r} in {toc-tic:.2f} sec. {before-after:,} duplicates.')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168738bd-f73f-4c28-b1e0-979fc86d9231",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def my_weight(G, u, v, weight=\"weight\"):\n",
    "    w = 0\n",
    "    for nbr in set(G[u]) & set(G[v]):\n",
    "        w += (G[u][nbr].get(weight, 1) + G[v][nbr].get(weight, 1))/2\n",
    "    return w\n",
    "def make_institution_graph(works_authors_rows):\n",
    "    \n",
    "    institution_id_set = set(works_authors_rows.institution_id)\n",
    "                                  \n",
    "    bip_g = nx.from_pandas_edgelist(\n",
    "        works_authors_rows,\n",
    "        source='work_id', target='institution_id', edge_attr ='weight'\n",
    "    )\n",
    "\n",
    "    inst_graph = nx.bipartite.generic_weighted_projected_graph(bip_g,nodes=institution_id_set,weight_function=my_weight)    \n",
    "    #inst_graph = nx.bipartite.weighted_projected_graph(bip_g,nodes=institution_id_set) \n",
    "\n",
    "    return inst_graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab4a43f-7d48-48ee-ba88-9acbaeb6f007",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "basepath = Path('Tables_final') \n",
    "my_path_ = Path('Model_info')\n",
    "if not os.path.exists(my_path_):\n",
    "    os.makedirs(my_path_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f8e97eb-1f45-484d-be22-9c278ad914ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f030651a-6081-4098-8b3d-b9fae9318283",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-12T10:13:49.784563Z",
     "iopub.status.busy": "2024-09-12T10:13:49.784196Z",
     "iopub.status.idle": "2024-09-12T10:13:49.787698Z",
     "shell.execute_reply": "2024-09-12T10:13:49.787422Z",
     "shell.execute_reply.started": "2024-09-12T10:13:49.784547Z"
    }
   },
   "source": [
    "## Perc. works resp. team size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "892e35b3-72c6-493b-8889-c36f167f6571",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "works = read_parquet(basepath / 'works')\n",
    "\n",
    "#TW training window\n",
    "months_list = list(set(works.reset_index().drop_duplicates('publication_date_1').publication_date_1))\n",
    "months_list.sort()\n",
    "months_list = [i.strftime('%Y-%m-%d') for i in months_list]\n",
    "start_index = 0\n",
    "end_index = 120 #180\n",
    "start_month = months_list[start_index]#.strftime('%Y-%m-%d')\n",
    "end_month = months_list[end_index-1]#.strftime('%Y-%m-%d')\n",
    "print(start_month,end_month)\n",
    "works = works[works.num_authors > 1]\n",
    "works_TW = works.loc[start_month:end_month]\n",
    "works_AW = works.loc[months_list[end_index]:]\n",
    "\n",
    "df_perc_works = ((works.groupby('num_authors').work_id.count().to_frame().cumsum()/len(works))*100).rename(columns={'work_id':'perc_works'})\n",
    "display(df_perc_works)\n",
    "df_perc_works = ((works_AW.groupby('num_authors').work_id.count().to_frame().cumsum()/len(works_AW))*100).rename(columns={'work_id':'perc_works'})\n",
    "display(df_perc_works)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916a1428-5cbd-4031-8b12-b502b35a6e34",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fb567e3f-e915-413f-8feb-61ad6c2368c6",
   "metadata": {},
   "source": [
    "## Edges - 2 authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d741f25c-2488-464c-8697-990420b61fcd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "path_2authors = Path('Model_2authors') \n",
    "if not os.path.exists(path_2authors):\n",
    "    os.makedirs(path_2authors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "887185bf-b3d6-4739-952f-f49ced5bf2fb",
   "metadata": {},
   "source": [
    "### Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15fb3f87-624b-4f76-9351-de88899e315d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "works = read_parquet(basepath / 'works')\n",
    "works_authors_aff = read_parquet(basepath / 'works_authors_aff')\n",
    "\n",
    "works = works.loc['2000-01-01':'2023-12-01'] \n",
    "works = works[works.num_authors>1]\n",
    "\n",
    "works_2authors = set(works[works.num_authors ==2].work_id)\n",
    "print(f'{len(works_2authors)} ({(len(works_2authors)/len(works))*100:.2f}%) works 2 authors')\n",
    "\n",
    "works = works[works.work_id.isin(works_2authors)]\n",
    "works_authors_aff = works_authors_aff[works_authors_aff.work_id.isin(works_2authors)]\n",
    "\n",
    "my_file = \"dfs_2authors.pickle\"\n",
    "pickle.dump([works_2authors,works,works_authors_aff], open(os.path.join(path_2authors, my_file), 'wb')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f6a0891-8661-4eeb-87db-691955e0a9b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2287b237-5415-4fe5-9c37-b941f7f9433e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#INITIALIZATION #preferential attachment  #TW: 2000-2009\n",
    "my_file = \"dfs_2authors.pickle\"\n",
    "with open(os.path.join(path_2authors, my_file),\"rb\") as fp:\n",
    "    [works_2authors,works,works_authors_aff] = pickle.load(fp)  \n",
    "works = works.loc['2000-01-01':'2023-12-01'] \n",
    "works_authors_aff = works_authors_aff.loc['2000-01-01':'2023-12-01']\n",
    "\n",
    "N_dict = works_authors_aff.groupby('publication_date_1').work_id.nunique().to_frame().to_dict()['work_id']\n",
    "months_list = list(N_dict.keys())\n",
    "months_list.sort()\n",
    "months_list = [i.strftime('%Y-%m-%d') for i in months_list]\n",
    "\n",
    "start_index = 0\n",
    "end_index = 120 #180\n",
    "start_month = months_list[start_index]#.strftime('%Y-%m-%d')\n",
    "end_month = months_list[end_index-1]#.strftime('%Y-%m-%d')\n",
    "\n",
    "df_TW = works_authors_aff.loc[start_month:end_month] #df_TW = works_authors_aff.loc[months_list[:120]]\n",
    "inst_set = set(df_TW.institution_id)\n",
    "I = len(inst_set)\n",
    "print(f'TW from {start_month} to {end_month} : {I} institutions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6b463e-7298-4fd4-94ca-68fcdc1210a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#initial strenghts    #consider only institutions in sample in TW\n",
    "#count number (unique) institutions per paper\n",
    "df_TW = df_TW.drop_duplicates(['work_id','institution_id'])\n",
    "df_TW['num_affs'] = df_TW.groupby('work_id')['institution_id'].transform('size')\n",
    "print(f'{min(df_TW.num_affs)}-{max(df_TW.num_affs)} min-max number (unique) affiliations per work')\n",
    "df_TW['weight'] = 2 / ( df_TW['num_affs']*(df_TW['num_affs']-1) ) \n",
    "df_TW.loc[df_TW.num_affs==1,'weight'] = 1 #one affiliation\n",
    "df_TW_noloops = df_TW[df_TW.num_affs>1]\n",
    "df_TW_loops = df_TW[df_TW.num_affs==1]\n",
    "df_TW_loops['institution_id2'] = df_TW_loops['institution_id']\n",
    "df_TW_loops['weight'] = df_TW_loops[['institution_id','institution_id2','weight']].groupby(['institution_id']).weight.transform('sum')\n",
    "df_TW_loops = df_TW_loops.drop_duplicates('institution_id')\n",
    "I_graph = make_institution_graph(df_TW_noloops)\n",
    "I_graph.add_weighted_edges_from([tuple(r) for r in df_TW_loops[['institution_id','institution_id2','weight']].to_numpy()])\n",
    "df_ = pd.DataFrame.from_dict(dict(I_graph.degree(weight='weight')),orient='index').reset_index().rename(columns={'index':'institution_id',0:'strength'})\n",
    "df_['institution_id'] = df_['institution_id'].astype(int)\n",
    "df = df_.sort_values(by='strength',ascending=False)\n",
    "inst_set = set(df.institution_id)\n",
    "I = len(inst_set)\n",
    "print(f'TW from {start_month} to {end_month} : {I} institutions')\n",
    "\n",
    "my_file = \"df_strengths0.csv\"     \n",
    "df.to_csv(os.path.join(path_2authors, my_file),index=False)\n",
    "\n",
    "my_file = \"inst_set.pickle\"\n",
    "pickle.dump(inst_set, open(os.path.join(path_2authors, my_file), 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87247b2d-a449-4fec-bbe9-68e1f180223c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "I_dist = read_parquet(basepath / 'I_dist_threshold')\n",
    "I_dist = I_dist[['source','target','dist']].reset_index(drop=True)\n",
    "I_dist = I_dist.query('source.isin(@inst_set) & target.isin(@inst_set)').reset_index()\n",
    "my_file = \"I_dist_model.csv\" \n",
    "I_dist.to_csv(os.path.join(path_2authors, my_file),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4e1a49-316c-471c-8e78-e42dea60a170",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cdd5ceb0-7142-4286-8e15-ca6e87f3930e",
   "metadata": {},
   "source": [
    "### Data plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86acb54d-9446-42b2-9703-7eaeb54b4ca3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "my_file = \"dfs_2authors.pickle\"\n",
    "with open(os.path.join(path_2authors, my_file),\"rb\") as fp:\n",
    "    [works_2authors,works,works_authors_aff] = pickle.load(fp)  \n",
    "\n",
    "N_dict = works_authors_aff.groupby('publication_date_1').work_id.nunique().to_frame().to_dict()['work_id']\n",
    "\n",
    "months_list = list(N_dict.keys())\n",
    "months_list.sort()\n",
    "months_list = [i.strftime('%Y-%m-%d') for i in months_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b789998-ab46-4857-b620-b71d92017475",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_fit_rolling_breakpoints_2(df,x_column,x_column2,x_label,title,window_size,num_breakpoints):\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(15, 5))\n",
    "    x_dates = list(df['publication_date_1'])\n",
    "    y_data = df[x_column].rolling(window=window_size).mean()[window_size-1:]\n",
    "    y_data2 = df[x_column2].rolling(window=window_size).mean()[window_size-1:]\n",
    "    x_data = x_dates[window_size-1:]\n",
    "    ax.set_ylim([0, 1])\n",
    "    \n",
    "    ax.plot(x_data, y_data, \"o-\", color='orange', markersize=3,label='intra-institution')\n",
    "    ax.plot(x_data, y_data2, \"o-\", color='green', markersize=3,label='inter-institution')\n",
    "\n",
    "    plt.grid(True, linewidth=0.5)\n",
    "    ax.yaxis.set_major_formatter(formatter)\n",
    "\n",
    "    ax.set_xlabel(x_label,size=20)\n",
    "    #ax.set_title(title,size=30)\n",
    "\n",
    "    #ax.xaxis.set_major_locator(mdates.MonthLocator()) # Make ticks on occurrences of each month\n",
    "    ax.xaxis.set_major_formatter(mdates.DateFormatter('%b %y')) # Get only the month to show in the x-axis\n",
    "\n",
    "    ax.axvline(pd.Timestamp(2020, 3, 1),color='r') #ax.axvline(x_dates[84],color='r')\n",
    "\n",
    "    #save for all possible combination of breakpoints the correspondent error \n",
    "    def f(breakpoints, x, y, fcache): \n",
    "        breakpoints = tuple(map(int, sorted(breakpoints)))\n",
    "        if breakpoints not in fcache:\n",
    "            total_error = 0\n",
    "            for f, xi, yi in find_best_piecewise_polynomial(breakpoints, x, y):\n",
    "                total_error += ((f(xi) - yi)**2).sum()\n",
    "            fcache[breakpoints] = total_error \n",
    "        # print('{} --> {}'.format(breakpoints, fcache[breakpoints]))\n",
    "        return fcache[breakpoints]\n",
    "\n",
    "    def find_best_piecewise_polynomial(breakpoints, x, y):\n",
    "        breakpoints = tuple(map(int, sorted(breakpoints)))\n",
    "        xs = np.split(x, breakpoints)\n",
    "        ys = np.split(y, breakpoints)\n",
    "        result = []\n",
    "        for xi, yi in zip(xs, ys):\n",
    "            if len(xi) < 2: continue\n",
    "            coefs = npoly.polyfit(xi, yi, 1)\n",
    "            f = npoly.Polynomial(coefs)\n",
    "            result.append([f, xi, yi])\n",
    "        return result\n",
    "    \n",
    "    x_num = dates.date2num(x_data)\n",
    "    breakpoints = optimize.brute(f, [slice(1, len(x_data), 1)]*num_breakpoints, args=(x_num, y_data, {}), finish=None)\n",
    "    if num_breakpoints==1:\n",
    "        breakpoints = [breakpoints] \n",
    "    for f, xi, yi in find_best_piecewise_polynomial(breakpoints, x_num, y_data):\n",
    "        xi_min = x_data[np.where(x_num == xi.min())[0][0]]\n",
    "        xi_max = x_data[np.where(x_num == xi.max())[0][0]]\n",
    "        x_interval = np.array([xi_min, xi_max]) \n",
    "        #print('y = {:35s}, if x in [{}, {}]'.format(str(f), *x_interval))\n",
    "        x_interval = np.array([xi.min(), xi.max()])\n",
    "        #ax.plot(x_interval, f(x_interval), 'yo-')\n",
    "        coef = f.convert().coef\n",
    "        b = coef[0]\n",
    "        a = coef[1]\n",
    "        if b>0:\n",
    "            ll = 'y = {:.2f} x + {:.0f}'.format(a,b)\n",
    "        else: #b<0\n",
    "            ll = 'y = {:.2f} x - {:.0f}'.format(a,abs(b))    \n",
    "        ax.plot(x_interval, f(x_interval), 'yo-',linewidth=2, markersize=7)       \n",
    "    #save for all possible combination of breakpoints the correspondent error\n",
    "    \n",
    "    def f(breakpoints, x, y, fcache): \n",
    "        breakpoints = tuple(map(int, sorted(breakpoints)))\n",
    "        if breakpoints not in fcache:\n",
    "            total_error = 0\n",
    "            for f, xi, yi in find_best_piecewise_polynomial(breakpoints, x, y):\n",
    "                total_error += ((f(xi) - yi)**2).sum()\n",
    "            fcache[breakpoints] = total_error \n",
    "        # print('{} --> {}'.format(breakpoints, fcache[breakpoints]))\n",
    "        return fcache[breakpoints]\n",
    "\n",
    "    def find_best_piecewise_polynomial(breakpoints, x, y):\n",
    "        breakpoints = tuple(map(int, sorted(breakpoints)))\n",
    "        xs = np.split(x, breakpoints)\n",
    "        ys = np.split(y, breakpoints)\n",
    "        result = []\n",
    "        for xi, yi in zip(xs, ys):\n",
    "            if len(xi) < 2: continue\n",
    "            coefs = npoly.polyfit(xi, yi, 1)\n",
    "            f = npoly.Polynomial(coefs)\n",
    "            result.append([f, xi, yi])\n",
    "        return result\n",
    "    \n",
    "    x_num = dates.date2num(x_data)\n",
    "    breakpoints = optimize.brute(f, [slice(1, len(x_data), 1)]*num_breakpoints, args=(x_num, y_data2, {}), finish=None)\n",
    "    if num_breakpoints==1:\n",
    "        breakpoints = [breakpoints] \n",
    "    for f, xi, yi in find_best_piecewise_polynomial(breakpoints, x_num, y_data2):\n",
    "        xi_min = x_data[np.where(x_num == xi.min())[0][0]]\n",
    "        xi_max = x_data[np.where(x_num == xi.max())[0][0]]\n",
    "        x_interval = np.array([xi_min, xi_max]) \n",
    "        #print('y = {:35s}, if x in [{}, {}]'.format(str(f), *x_interval))\n",
    "        x_interval = np.array([xi.min(), xi.max()])\n",
    "        #ax.plot(x_interval, f(x_interval), 'yo-')\n",
    "        coef = f.convert().coef\n",
    "        b = coef[0]\n",
    "        a = coef[1]\n",
    "        if b>0:\n",
    "            ll = 'y = {:.2f} x + {:.0f}'.format(a,b)\n",
    "        else: #b<0\n",
    "            ll = 'y = {:.2f} x - {:.0f}'.format(a,abs(b))    \n",
    "        ax.plot(x_interval, f(x_interval), 'yo-',linewidth=2, markersize=7) \n",
    "       \n",
    "    ax.legend()        \n",
    "    ax.set_title(title,size=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3dbc24-24f2-4fae-86d4-f16d0b2ac7d3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.dates as dates\n",
    "import matplotlib.dates as mdates\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "from scipy import optimize\n",
    "import numpy.polynomial.polynomial as npoly\n",
    "\n",
    "def form(x,pos):\n",
    "    if x<1e3:\n",
    "        return '%1.3f' % (x)\n",
    "    elif x<1e6:\n",
    "        return '%1.1fK' % (x * 1e-3)\n",
    "    else:\n",
    "        return '%1.1fM' % (x * 1e-6)\n",
    "formatter = FuncFormatter(form)\n",
    "\n",
    "def plot_fit_rolling_breakpoints(df,x_column,x_label,title,window_size,num_breakpoints,ff):\n",
    "    \n",
    "    #save for all possible combination of breakpoints the correspondent error \n",
    "    def f(breakpoints, x, y, fcache): \n",
    "        breakpoints = tuple(map(int, sorted(breakpoints)))\n",
    "        if breakpoints not in fcache:\n",
    "            total_error = 0\n",
    "            for f, xi, yi in find_best_piecewise_polynomial(breakpoints, x, y):\n",
    "                total_error += ((f(xi) - yi)**2).sum()\n",
    "            fcache[breakpoints] = total_error \n",
    "        # print('{} --> {}'.format(breakpoints, fcache[breakpoints]))\n",
    "        return fcache[breakpoints]\n",
    "\n",
    "    def find_best_piecewise_polynomial(breakpoints, x, y):\n",
    "        breakpoints = tuple(map(int, sorted(breakpoints)))\n",
    "        xs = np.split(x, breakpoints)\n",
    "        ys = np.split(y, breakpoints)\n",
    "        result = []\n",
    "        for xi, yi in zip(xs, ys):\n",
    "            if len(xi) < 2: continue\n",
    "            coefs = npoly.polyfit(xi, yi, 1)\n",
    "            f = npoly.Polynomial(coefs)\n",
    "            result.append([f, xi, yi])\n",
    "        return result\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(15, 5))\n",
    "    x_dates = list(df['publication_date_1'])\n",
    "    y_data = df[x_column].rolling(window=window_size).mean()[window_size-1:]\n",
    "    x_data = x_dates[window_size-1:]\n",
    "    \n",
    "    ax.plot(x_data, y_data, \"o-\", color='orange', markersize=3)\n",
    "\n",
    "    plt.grid(True, linewidth=0.5)\n",
    "    ax.yaxis.set_major_formatter(formatter)\n",
    "\n",
    "    ax.set_xlabel(x_label,size=20)\n",
    "    ax.set_title(title,size=30)\n",
    "\n",
    "    ax.xaxis.set_major_formatter(mdates.DateFormatter('%b %y')) # Get only the month to show in the x-axis\n",
    "\n",
    "    ax.axvline(pd.Timestamp(2020, 3, 1),color='r') #ax.axvline(x_dates[84],color='r')\n",
    "\n",
    "    x_num = dates.date2num(x_data)\n",
    "    breakpoints = optimize.brute(f, [slice(1, len(x_data), 1)]*num_breakpoints, args=(x_num, y_data, {}), finish=None)\n",
    "    if num_breakpoints==1:\n",
    "        breakpoints = [breakpoints] \n",
    "\n",
    "    for f, xi, yi in find_best_piecewise_polynomial(breakpoints, x_num, y_data):\n",
    "        xi_min = x_data[np.where(x_num == xi.min())[0][0]]\n",
    "        xi_max = x_data[np.where(x_num == xi.max())[0][0]] \n",
    "        x_interval = np.array([xi.min(), xi.max()])\n",
    "        coef = f.convert().coef\n",
    "        b = coef[0]\n",
    "        a = coef[1]\n",
    "        if ff==1:\n",
    "            if b>0:\n",
    "                ll = 'y = {:.2f} x + {:.2f}'.format(a,b)\n",
    "            else: #b<0\n",
    "                ll = 'y = {:.2f} x - {:.2f}'.format(a,abs(b))    \n",
    "        else:\n",
    "            if b>0:\n",
    "                ll = 'y = {:.5f} x + {:.5f}'.format(a,b)\n",
    "            else: #b<0\n",
    "                ll = 'y = {:.5f} x - {:.5f}'.format(a,abs(b))  \n",
    "        #x_1 = x_dates[window_size-1:][np.where(x_num==x_interval[0])[0][0]]\n",
    "        #x_3 = x_dates[window_size-1:][np.where(x_num==x_interval[1])[0][0]]\n",
    "        ax.plot(x_interval, f(x_interval), 'o-',color='yellow',label=ll, markersize=6)\n",
    "        \n",
    "    ax.legend()\n",
    "    ax.set_title(title,size=30)\n",
    "    #return x_num \n",
    "    \n",
    "x_label='Month'\n",
    "window_size = 6\n",
    "num_breakpoints = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0ff384-c144-4c01-9c7b-99629d85124f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def df_data1_(work_authors_edges_df):\n",
    "    #work_authors_edges_df['intra'] = 0\n",
    "    #work_authors_edges_df.loc[work_authors_edges_df.source_inst == work_authors_edges_df.target_inst,'intra'] = 1\n",
    "    df1 = work_authors_edges_df.groupby('publication_date_1').intra.count().reset_index().rename(columns={'intra':'total'})\n",
    "    df2 = work_authors_edges_df.groupby('publication_date_1').intra.sum().to_frame().reset_index()\n",
    "    intra_inter_df = df1.merge(df2,on='publication_date_1')\n",
    "    intra_inter_df['inter'] = intra_inter_df['total'] - intra_inter_df['intra']\n",
    "    intra_inter_df['frac_intra'] = intra_inter_df['intra'] / intra_inter_df['total']\n",
    "    intra_inter_df['frac_inter'] = intra_inter_df['inter'] / intra_inter_df['total']\n",
    "    df_data1 = intra_inter_df\n",
    "    return df_data1\n",
    "\n",
    "def work_edges_dist_mean_monthly_(works_outside,works_set,path):\n",
    "    my_file = \"work_edges_dist_mean.csv\"    \n",
    "    work_edges_dist_mean = pd.read_csv(os.path.join(path, my_file))\n",
    "    work_edges_dist_mean = work_edges_dist_mean[work_edges_dist_mean.work_id.isin(works_set)]\n",
    "    work_edges_dist_mean = work_edges_dist_mean[~work_edges_dist_mean.work_id.isin(works_outside)]\n",
    "    work_edges_dist_mean['publication_date_1'] = pd.to_datetime(work_edges_dist_mean['publication_date_1'])\n",
    "    work_edges_dist_mean_monthly = work_edges_dist_mean.groupby('publication_date_1').dist.mean().to_frame().reset_index()\n",
    "    return work_edges_dist_mean_monthly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd100669-78ff-47b9-a01f-9d5c409b33c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa7e6fe-6584-405e-9c2e-3511e89b1bff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "work_authors_edges_df_dist = read_parquet(Path('./TeamDistance') / 'work_authors_edges_df_dist')\n",
    "work_authors_edges_df_dist = works[['work_id']].reset_index().merge(work_authors_edges_df_dist,on='work_id')\n",
    "work_authors_edges_df_dist = work_authors_edges_df_dist[work_authors_edges_df_dist.work_id.isin(works_2authors)]\n",
    "work_edges_dist_mean = work_authors_edges_df_dist.groupby('work_id').dist.mean().to_frame().reset_index()\n",
    "work_edges_dist_mean = work_edges_dist_mean.merge(works[['work_id']].reset_index(),on='work_id')\n",
    "work_edges_dist_mean_monthly = work_edges_dist_mean.groupby('publication_date_1').dist.mean().to_frame().reset_index()\n",
    "my_file = \"work_edges_dist_mean.csv\"    \n",
    "work_edges_dist_mean.to_csv(os.path.join(path_2authors, my_file),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a685475-5956-4e4a-b34f-2e00b2d41891",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "my_file = \"work_edges_dist_mean_monthly.csv\"     \n",
    "work_edges_dist_mean_monthly.to_csv(os.path.join(path_2authors, my_file),index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af606fd6-66ab-490d-98e9-f8f32108879c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_data1 = df_data1_(work_authors_edges_df_dist)\n",
    "df_data1['publication_date_1'] = df_data1['publication_date_1'].apply(pd.to_datetime)\n",
    "my_file = \"df_data1.csv\"  \n",
    "df_data1.to_csv(os.path.join(path_2authors, my_file),index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "754abee2-95c6-455c-83c8-044cdf8fa13f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c637e60c-003f-41a3-bf24-fdfeebfa32ac",
   "metadata": {},
   "source": [
    "## Edges - 3 authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "266af36d-7bb4-4907-9872-1dc093280668",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "path_3authors = Path('Model_3authors') \n",
    "if not os.path.exists(path_3authors):\n",
    "    os.makedirs(path_3authors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c525c24-3515-44fc-a32d-2421a8ec8805",
   "metadata": {},
   "source": [
    "### Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2e7fb7-6a4f-4c0f-8d26-582231b49202",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "works = read_parquet(basepath / 'works')\n",
    "works_authors_aff = read_parquet(basepath / 'works_authors_aff')\n",
    "\n",
    "works = works.loc['2000-01-01':'2023-12-01'] \n",
    "works = works[works.num_authors>1]\n",
    "\n",
    "works_3authors = set(works[works.num_authors ==3].work_id)\n",
    "print(f'{len(works_3authors)} ({(len(works_3authors)/len(works))*100:.2f}%) works 3 authors')\n",
    "\n",
    "works = works[works.work_id.isin(works_3authors)]\n",
    "works_authors_aff = works_authors_aff[works_authors_aff.work_id.isin(works_3authors)]\n",
    "\n",
    "my_file = \"dfs_3authors.pickle\"\n",
    "pickle.dump([works_3authors,works,works_authors_aff], open(os.path.join(path_3authors, my_file), 'wb')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308f957e-9ca4-40a5-b842-bdc0b0c082f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#INITIALIZATION #preferential attachment  #TW: 2000-2009\n",
    "my_file = \"dfs_3authors.pickle\"\n",
    "with open(os.path.join(path_3authors, my_file),\"rb\") as fp:\n",
    "    [works_3authors,works,works_authors_aff] = pickle.load(fp)  \n",
    "works = works.loc['2000-01-01':'2023-12-01'] \n",
    "works_authors_aff = works_authors_aff.loc['2000-01-01':'2023-12-01']\n",
    "\n",
    "N_dict = works_authors_aff.groupby('publication_date_1').work_id.nunique().to_frame().to_dict()['work_id']\n",
    "months_list = list(N_dict.keys())\n",
    "months_list.sort()\n",
    "months_list = [i.strftime('%Y-%m-%d') for i in months_list]\n",
    "\n",
    "start_index = 0\n",
    "end_index = 120 #180\n",
    "start_month = months_list[start_index]#.strftime('%Y-%m-%d')\n",
    "end_month = months_list[end_index-1]#.strftime('%Y-%m-%d')\n",
    "\n",
    "df_TW = works_authors_aff.loc[start_month:end_month] #df_TW = works_authors_aff.loc[months_list[:120]]\n",
    "inst_set = set(df_TW.institution_id)\n",
    "I = len(inst_set)\n",
    "print(f'TW from {start_month} to {end_month} : {I} institutions')\n",
    "# my_file = \"inst_set.pickle\"\n",
    "# pickle.dump(inst_set, open(os.path.join(path_3authors, my_file), 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77a8b3b-82d6-4cba-8e16-576f7002cc47",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#initial strenghts    #consider only institutions in sample in TW\n",
    "#count number (unique) institutions per paper\n",
    "df_TW = df_TW.drop_duplicates(['work_id','institution_id'])\n",
    "df_TW['num_affs'] = df_TW.groupby('work_id')['institution_id'].transform('size')\n",
    "print(f'{min(df_TW.num_affs)}-{max(df_TW.num_affs)} min-max number (unique) affiliations per work')\n",
    "df_TW['weight'] = 2 / ( df_TW['num_affs']*(df_TW['num_affs']-1) ) \n",
    "df_TW.loc[df_TW.num_affs==1,'weight'] = 1 #one affiliation\n",
    "df_TW_noloops = df_TW[df_TW.num_affs>1]\n",
    "df_TW_loops = df_TW[df_TW.num_affs==1]\n",
    "df_TW_loops['institution_id2'] = df_TW_loops['institution_id']\n",
    "df_TW_loops['weight'] = df_TW_loops[['institution_id','institution_id2','weight']].groupby(['institution_id']).weight.transform('sum')\n",
    "df_TW_loops = df_TW_loops.drop_duplicates('institution_id')\n",
    "I_graph = make_institution_graph(df_TW_noloops)\n",
    "I_graph.add_weighted_edges_from([tuple(r) for r in df_TW_loops[['institution_id','institution_id2','weight']].to_numpy()])\n",
    "df_ = pd.DataFrame.from_dict(dict(I_graph.degree(weight='weight')),orient='index').reset_index().rename(columns={'index':'institution_id',0:'strength'})\n",
    "df_['institution_id'] = df_['institution_id'].astype(int)\n",
    "df = df_.sort_values(by='strength',ascending=False)\n",
    "inst_set = set(df.institution_id)\n",
    "I = len(inst_set)\n",
    "print(f'TW from {start_month} to {end_month} : {I} institutions')\n",
    "\n",
    "my_file = \"df_strengths0.csv\"     \n",
    "df.to_csv(os.path.join(path_3authors, my_file),index=False)\n",
    "\n",
    "my_file = \"inst_set.pickle\"\n",
    "pickle.dump(inst_set, open(os.path.join(path_3authors, my_file), 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa3be0cb-595d-4eed-b478-5f09c2a096dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "I_dist = read_parquet(basepath / 'I_dist_threshold')\n",
    "I_dist = I_dist[['source','target','dist']].reset_index(drop=True)\n",
    "I_dist_sort = [tuple(row) for row in I_dist.itertuples(index=False)]\n",
    "I_dist_sort = [ sorted(list(x)[:2])+[list(x)[2]] for x in I_dist_sort]\n",
    "I_dist_sort = pd.DataFrame(I_dist_sort, columns = ['source', 'target', 'dist'])\n",
    "my_file = \"I_dist_model.csv\" \n",
    "I_dist_sort.to_csv(os.path.join(path_3authors, my_file),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bec08cd-ec31-45dd-9ad4-48d6b60ae171",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "90bfcd94-fa87-4ab7-bb97-bab7bbf4d86e",
   "metadata": {},
   "source": [
    "### Data plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4253af72-1e31-49be-8c13-1376f0e0f103",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "my_file = \"dfs_3authors.pickle\"\n",
    "with open(os.path.join(path_3authors, my_file),\"rb\") as fp:\n",
    "    [works_3authors,works,works_authors_aff] = pickle.load(fp)  \n",
    "\n",
    "N_dict = works_authors_aff.groupby('publication_date_1').work_id.nunique().to_frame().to_dict()['work_id']\n",
    "\n",
    "months_list = list(N_dict.keys())\n",
    "months_list.sort()\n",
    "months_list = [i.strftime('%Y-%m-%d') for i in months_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0d838c-ddc5-4765-9548-d69404ad2f27",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_fit_rolling_breakpoints_2(df,x_column,x_column2,x_label,title,window_size,num_breakpoints):\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(15, 5))\n",
    "    x_dates = list(df['publication_date_1'])\n",
    "    y_data = df[x_column].rolling(window=window_size).mean()[window_size-1:]\n",
    "    y_data2 = df[x_column2].rolling(window=window_size).mean()[window_size-1:]\n",
    "    x_data = x_dates[window_size-1:]\n",
    "    ax.set_ylim([0, 1])\n",
    "    \n",
    "    ax.plot(x_data, y_data, \"o-\", color='orange', markersize=3,label='intra-institution')\n",
    "    ax.plot(x_data, y_data2, \"o-\", color='green', markersize=3,label='inter-institution')\n",
    "\n",
    "    plt.grid(True, linewidth=0.5)\n",
    "    ax.yaxis.set_major_formatter(formatter)\n",
    "\n",
    "    ax.set_xlabel(x_label,size=20)\n",
    "    #ax.set_title(title,size=30)\n",
    "\n",
    "    #ax.xaxis.set_major_locator(mdates.MonthLocator()) # Make ticks on occurrences of each month\n",
    "    ax.xaxis.set_major_formatter(mdates.DateFormatter('%b %y')) # Get only the month to show in the x-axis\n",
    "\n",
    "    ax.axvline(pd.Timestamp(2020, 3, 1),color='r') #ax.axvline(x_dates[84],color='r')\n",
    "\n",
    "    #save for all possible combination of breakpoints the correspondent error \n",
    "    def f(breakpoints, x, y, fcache): \n",
    "        breakpoints = tuple(map(int, sorted(breakpoints)))\n",
    "        if breakpoints not in fcache:\n",
    "            total_error = 0\n",
    "            for f, xi, yi in find_best_piecewise_polynomial(breakpoints, x, y):\n",
    "                total_error += ((f(xi) - yi)**2).sum()\n",
    "            fcache[breakpoints] = total_error \n",
    "        # print('{} --> {}'.format(breakpoints, fcache[breakpoints]))\n",
    "        return fcache[breakpoints]\n",
    "\n",
    "    def find_best_piecewise_polynomial(breakpoints, x, y):\n",
    "        breakpoints = tuple(map(int, sorted(breakpoints)))\n",
    "        xs = np.split(x, breakpoints)\n",
    "        ys = np.split(y, breakpoints)\n",
    "        result = []\n",
    "        for xi, yi in zip(xs, ys):\n",
    "            if len(xi) < 2: continue\n",
    "            coefs = npoly.polyfit(xi, yi, 1)\n",
    "            f = npoly.Polynomial(coefs)\n",
    "            result.append([f, xi, yi])\n",
    "        return result\n",
    "    \n",
    "    x_num = dates.date2num(x_data)\n",
    "    breakpoints = optimize.brute(f, [slice(1, len(x_data), 1)]*num_breakpoints, args=(x_num, y_data, {}), finish=None)\n",
    "    if num_breakpoints==1:\n",
    "        breakpoints = [breakpoints] \n",
    "    for f, xi, yi in find_best_piecewise_polynomial(breakpoints, x_num, y_data):\n",
    "        xi_min = x_data[np.where(x_num == xi.min())[0][0]]\n",
    "        xi_max = x_data[np.where(x_num == xi.max())[0][0]]\n",
    "        x_interval = np.array([xi_min, xi_max]) \n",
    "        #print('y = {:35s}, if x in [{}, {}]'.format(str(f), *x_interval))\n",
    "        x_interval = np.array([xi.min(), xi.max()])\n",
    "        #ax.plot(x_interval, f(x_interval), 'yo-')\n",
    "        coef = f.convert().coef\n",
    "        b = coef[0]\n",
    "        a = coef[1]\n",
    "        if b>0:\n",
    "            ll = 'y = {:.2f} x + {:.0f}'.format(a,b)\n",
    "        else: #b<0\n",
    "            ll = 'y = {:.2f} x - {:.0f}'.format(a,abs(b))    \n",
    "        ax.plot(x_interval, f(x_interval), 'yo-',linewidth=2, markersize=7)       \n",
    "    #save for all possible combination of breakpoints the correspondent error\n",
    "    \n",
    "    def f(breakpoints, x, y, fcache): \n",
    "        breakpoints = tuple(map(int, sorted(breakpoints)))\n",
    "        if breakpoints not in fcache:\n",
    "            total_error = 0\n",
    "            for f, xi, yi in find_best_piecewise_polynomial(breakpoints, x, y):\n",
    "                total_error += ((f(xi) - yi)**2).sum()\n",
    "            fcache[breakpoints] = total_error \n",
    "        # print('{} --> {}'.format(breakpoints, fcache[breakpoints]))\n",
    "        return fcache[breakpoints]\n",
    "\n",
    "    def find_best_piecewise_polynomial(breakpoints, x, y):\n",
    "        breakpoints = tuple(map(int, sorted(breakpoints)))\n",
    "        xs = np.split(x, breakpoints)\n",
    "        ys = np.split(y, breakpoints)\n",
    "        result = []\n",
    "        for xi, yi in zip(xs, ys):\n",
    "            if len(xi) < 2: continue\n",
    "            coefs = npoly.polyfit(xi, yi, 1)\n",
    "            f = npoly.Polynomial(coefs)\n",
    "            result.append([f, xi, yi])\n",
    "        return result\n",
    "    \n",
    "    x_num = dates.date2num(x_data)\n",
    "    breakpoints = optimize.brute(f, [slice(1, len(x_data), 1)]*num_breakpoints, args=(x_num, y_data2, {}), finish=None)\n",
    "    if num_breakpoints==1:\n",
    "        breakpoints = [breakpoints] \n",
    "    for f, xi, yi in find_best_piecewise_polynomial(breakpoints, x_num, y_data2):\n",
    "        xi_min = x_data[np.where(x_num == xi.min())[0][0]]\n",
    "        xi_max = x_data[np.where(x_num == xi.max())[0][0]]\n",
    "        x_interval = np.array([xi_min, xi_max]) \n",
    "        #print('y = {:35s}, if x in [{}, {}]'.format(str(f), *x_interval))\n",
    "        x_interval = np.array([xi.min(), xi.max()])\n",
    "        #ax.plot(x_interval, f(x_interval), 'yo-')\n",
    "        coef = f.convert().coef\n",
    "        b = coef[0]\n",
    "        a = coef[1]\n",
    "        if b>0:\n",
    "            ll = 'y = {:.2f} x + {:.0f}'.format(a,b)\n",
    "        else: #b<0\n",
    "            ll = 'y = {:.2f} x - {:.0f}'.format(a,abs(b))    \n",
    "        ax.plot(x_interval, f(x_interval), 'yo-',linewidth=2, markersize=7) \n",
    "       \n",
    "    ax.legend()        \n",
    "    ax.set_title(title,size=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db94191-efb1-4c54-ae78-eb88e9c4e61a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.dates as dates\n",
    "import matplotlib.dates as mdates\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "from scipy import optimize\n",
    "import numpy.polynomial.polynomial as npoly\n",
    "\n",
    "def form(x,pos):\n",
    "    if x<1e3:\n",
    "        return '%1.3f' % (x)\n",
    "    elif x<1e6:\n",
    "        return '%1.1fK' % (x * 1e-3)\n",
    "    else:\n",
    "        return '%1.1fM' % (x * 1e-6)\n",
    "formatter = FuncFormatter(form)\n",
    "\n",
    "def plot_fit_rolling_breakpoints(df,x_column,x_label,title,window_size,num_breakpoints,ff):\n",
    "    \n",
    "    #save for all possible combination of breakpoints the correspondent error \n",
    "    def f(breakpoints, x, y, fcache): \n",
    "        breakpoints = tuple(map(int, sorted(breakpoints)))\n",
    "        if breakpoints not in fcache:\n",
    "            total_error = 0\n",
    "            for f, xi, yi in find_best_piecewise_polynomial(breakpoints, x, y):\n",
    "                total_error += ((f(xi) - yi)**2).sum()\n",
    "            fcache[breakpoints] = total_error \n",
    "        # print('{} --> {}'.format(breakpoints, fcache[breakpoints]))\n",
    "        return fcache[breakpoints]\n",
    "\n",
    "    def find_best_piecewise_polynomial(breakpoints, x, y):\n",
    "        breakpoints = tuple(map(int, sorted(breakpoints)))\n",
    "        xs = np.split(x, breakpoints)\n",
    "        ys = np.split(y, breakpoints)\n",
    "        result = []\n",
    "        for xi, yi in zip(xs, ys):\n",
    "            if len(xi) < 2: continue\n",
    "            coefs = npoly.polyfit(xi, yi, 1)\n",
    "            f = npoly.Polynomial(coefs)\n",
    "            result.append([f, xi, yi])\n",
    "        return result\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(15, 5))\n",
    "    x_dates = list(df['publication_date_1'])\n",
    "    y_data = df[x_column].rolling(window=window_size).mean()[window_size-1:]\n",
    "    x_data = x_dates[window_size-1:]\n",
    "    \n",
    "    ax.plot(x_data, y_data, \"o-\", color='orange', markersize=3)\n",
    "\n",
    "    plt.grid(True, linewidth=0.5)\n",
    "    ax.yaxis.set_major_formatter(formatter)\n",
    "\n",
    "    ax.set_xlabel(x_label,size=20)\n",
    "    ax.set_title(title,size=30)\n",
    "\n",
    "    ax.xaxis.set_major_formatter(mdates.DateFormatter('%b %y')) # Get only the month to show in the x-axis\n",
    "\n",
    "    ax.axvline(pd.Timestamp(2020, 3, 1),color='r') #ax.axvline(x_dates[84],color='r')\n",
    "\n",
    "    x_num = dates.date2num(x_data)\n",
    "    breakpoints = optimize.brute(f, [slice(1, len(x_data), 1)]*num_breakpoints, args=(x_num, y_data, {}), finish=None)\n",
    "    if num_breakpoints==1:\n",
    "        breakpoints = [breakpoints] \n",
    "\n",
    "    for f, xi, yi in find_best_piecewise_polynomial(breakpoints, x_num, y_data):\n",
    "        xi_min = x_data[np.where(x_num == xi.min())[0][0]]\n",
    "        xi_max = x_data[np.where(x_num == xi.max())[0][0]] \n",
    "        x_interval = np.array([xi.min(), xi.max()])\n",
    "        coef = f.convert().coef\n",
    "        b = coef[0]\n",
    "        a = coef[1]\n",
    "        if ff==1:\n",
    "            if b>0:\n",
    "                ll = 'y = {:.2f} x + {:.2f}'.format(a,b)\n",
    "            else: #b<0\n",
    "                ll = 'y = {:.2f} x - {:.2f}'.format(a,abs(b))    \n",
    "        else:\n",
    "            if b>0:\n",
    "                ll = 'y = {:.5f} x + {:.5f}'.format(a,b)\n",
    "            else: #b<0\n",
    "                ll = 'y = {:.5f} x - {:.5f}'.format(a,abs(b))  \n",
    "        #x_1 = x_dates[window_size-1:][np.where(x_num==x_interval[0])[0][0]]\n",
    "        #x_3 = x_dates[window_size-1:][np.where(x_num==x_interval[1])[0][0]]\n",
    "        ax.plot(x_interval, f(x_interval), 'o-',color='yellow',label=ll, markersize=6)\n",
    "        \n",
    "    ax.legend()\n",
    "    ax.set_title(title,size=30)\n",
    "    #return x_num \n",
    "    \n",
    "x_label='Month'\n",
    "window_size = 6\n",
    "num_breakpoints = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9155d37-53e6-4a8b-809f-4bade8a1b0d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def work_edges_dist_mean_monthly_(works_outside,works_set,path):\n",
    "    my_file = \"work_edges_dist_mean.csv\"    \n",
    "    work_edges_dist_mean = pd.read_csv(os.path.join(path, my_file))\n",
    "    work_edges_dist_mean = work_edges_dist_mean[work_edges_dist_mean.work_id.isin(works_set)]\n",
    "    work_edges_dist_mean = work_edges_dist_mean[~work_edges_dist_mean.work_id.isin(works_outside)]\n",
    "    work_edges_dist_mean['publication_date_1'] = pd.to_datetime(work_edges_dist_mean['publication_date_1'])\n",
    "    work_edges_dist_mean_monthly = work_edges_dist_mean.groupby('publication_date_1').dist.mean().to_frame().reset_index()\n",
    "    return work_edges_dist_mean_monthly\n",
    "\n",
    "def work_edges_dist_max_monthly_(works_outside,works_set,path):\n",
    "    my_file = \"work_edges_dist_max.csv\"    \n",
    "    work_edges_dist_mean = pd.read_csv(os.path.join(path, my_file))\n",
    "    work_edges_dist_mean = work_edges_dist_mean[work_edges_dist_mean.work_id.isin(works_set)]\n",
    "    work_edges_dist_mean = work_edges_dist_mean[~work_edges_dist_mean.work_id.isin(works_outside)]\n",
    "    work_edges_dist_mean['publication_date_1'] = pd.to_datetime(work_edges_dist_mean['publication_date_1'])\n",
    "    work_edges_dist_mean_monthly = work_edges_dist_mean.groupby('publication_date_1').dist.mean().to_frame().reset_index()\n",
    "    return work_edges_dist_mean_monthly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c7b919-636f-45ce-bc07-e864cdd98173",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "work_authors_edges_df_dist = read_parquet(Path('./TeamDistance') / 'work_authors_edges_df_dist')\n",
    "work_authors_edges_df_dist = works[['work_id']].reset_index().merge(work_authors_edges_df_dist,on='work_id')\n",
    "work_authors_edges_df_dist = work_authors_edges_df_dist[work_authors_edges_df_dist.work_id.isin(works_3authors)]\n",
    "work_edges_dist_mean = work_authors_edges_df_dist.groupby('work_id').dist.mean().to_frame().reset_index()\n",
    "work_edges_dist_mean = work_edges_dist_mean.merge(works[['work_id']].reset_index(),on='work_id')\n",
    "work_edges_dist_mean_monthly = work_edges_dist_mean.groupby('publication_date_1').dist.mean().to_frame().reset_index()\n",
    "my_file = \"work_edges_dist_mean.csv\"    \n",
    "work_edges_dist_mean.to_csv(os.path.join(path_3authors, my_file),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4454b97-4198-49d0-b706-ada0a6936e5c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "my_file = \"work_edges_dist_mean_monthly.csv\"     \n",
    "work_edges_dist_mean_monthly.to_csv(os.path.join(path_3authors, my_file),index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ced8ef-3a3f-4842-9ceb-257481f91473",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data1 = df_data1_(work_authors_edges_df_dist)\n",
    "df_data1['publication_date_1'] = df_data1['publication_date_1'].apply(pd.to_datetime)\n",
    "my_file = \"df_data1.csv\"    \n",
    "df_data1.to_csv(os.path.join(path_3authors, my_file),index=False)\n",
    "x_column = 'frac_intra'\n",
    "x_label = 'publication_date_1'\n",
    "title = 'Fraction collabs. intra-inst. - works 3 authors'\n",
    "window_size = 1\n",
    "num_breakpoints = 1\n",
    "plot_fit_rolling_breakpoints(df_data1,x_column,x_label,title,window_size,num_breakpoints,ff=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269cfe05-2f20-40af-9350-90f2f9c6035a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ccc404-3a6f-43f1-b48d-035d4ac84529",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data2 = work_edges_dist_mean_monthly_({},works_3authors,path_3authors)\n",
    "df_data2['publication_date_1'] = df_data2['publication_date_1'].apply(pd.to_datetime)\n",
    "x_column='publication_date_1'\n",
    "x_column='dist'\n",
    "x_label='month'\n",
    "title='Monthly average mean team distance - works 3 authors'\n",
    "window_size=1\n",
    "num_breakpoints=1\n",
    "ff=1\n",
    "plot_fit_rolling_breakpoints(df_data2,x_column,x_label,title,window_size,num_breakpoints,ff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d3b5cb-8465-4bbc-86e3-5354a485b7c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67613ba9-f320-4c1d-b6b6-aea3e105f044",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "my_file = \"dfs_3authors.pickle\"\n",
    "with open(os.path.join(path_3authors, my_file),\"rb\") as fp:\n",
    "    [works_3authors,works_3authors,works_authors_aff_3authors] = pickle.load(fp)  \n",
    "inst_set_whole_3authors = set((works_authors_aff_3authors.drop_duplicates('institution_id')).institution_id)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4427c6f-b1d4-43c7-893f-e8a0b2230ef1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "N_dict = works_authors_aff_3authors.groupby('publication_date_1').work_id.nunique().to_frame().to_dict()['work_id']\n",
    "months_list = list(N_dict.keys())\n",
    "months_list.sort()\n",
    "months_list = [i.strftime('%Y-%m-%d') for i in months_list]\n",
    "start_index = 0\n",
    "end_index = 120 \n",
    "start_month = months_list[start_index]\n",
    "end_month = months_list[end_index-1]\n",
    "works_authors_aff_3authors_TW = works_authors_aff_3authors.loc[start_month:end_month]\n",
    "\n",
    "inst_set_TW_3authors = set((works_authors_aff_3authors_TW .drop_duplicates('institution_id')).institution_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ed1f6d-415c-4262-bc4d-ff5e04d2621b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#restrict to papers not including authors from institutions not in TW\n",
    "inst_set_TW_3authors_compl = inst_set_whole_3authors - inst_set_TW_3authors\n",
    "works_outside = set(works_authors_aff_3authors[works_authors_aff_3authors.institution_id.isin(inst_set_TW_3authors_compl)].work_id)\n",
    "works_3authors = works_3authors[~works_3authors.work_id.isin(works_outside)]\n",
    "works_authors_aff_3authors_new = works_authors_aff_3authors[~works_authors_aff_3authors.work_id.isin(works_outside)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5604f3-7847-483d-b4a6-8b8e95882b0e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f'3 authors. [2000,2023] {len(inst_set_whole_3authors)} institutions, [2000,2009] {len(inst_set_TW_3authors)} institutions ({(len(inst_set_TW_3authors)/len(inst_set_whole_3authors))*100:.2f}%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7754e0f2-c54a-47dd-8d32-f41ec789eed1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f'3 authors. All institutions: {len(set(works_authors_aff_3authors.work_id))} works, restrict to institutions in TW {len(set(works_authors_aff_3authors_new.work_id))} works ({(len(set(works_authors_aff_3authors_new.work_id))/len(set(works_authors_aff_3authors.work_id)))*100:.2f}%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf990f7-fb24-4db5-b601-b0ee4026fe25",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#delate tail\n",
    "df_strengths_3authors_TW = works_authors_aff_3authors_TW.groupby('institution_id').work_id.count().to_frame().sort_values(by='work_id',ascending=False).reset_index().rename(columns={'work_id':'strength'})\n",
    "df_strengths_3authors_TW['institution_id'] = df_strengths_3authors_TW['institution_id'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4dc7403-c2f9-465a-801b-6bb95a20b8f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for n in [20,50,70,100]:\n",
    "    my_file = \"dfs_3authors.pickle\"\n",
    "    with open(os.path.join(path_3authors, my_file),\"rb\") as fp:\n",
    "        [works_3authors,works_3authors,works_authors_aff_3authors] = pickle.load(fp) \n",
    "    I = set((df_strengths_3authors_TW[df_strengths_3authors_TW.strength>=n]).institution_id)\n",
    "    I_compl = inst_set_whole_3authors - I\n",
    "    works_outside = set(works_authors_aff_3authors[works_authors_aff_3authors.institution_id.isin(I_compl)].work_id)\n",
    "    works_3authors = works_3authors[~works_3authors.work_id.isin(works_outside)]\n",
    "    works_authors_aff_3authors_new = works_authors_aff_3authors[~works_authors_aff_3authors.work_id.isin(works_outside)]\n",
    "    print(f'3 authors - tail {n}. TW {len(I)} institutions ({(len(I)/len(inst_set_TW_3authors))*100:.2f}%), {len(set(works_authors_aff_3authors_new.work_id))} works ({(len(set(works_authors_aff_3authors_new.work_id))/len(set(works_authors_aff_3authors.work_id)))*100:.2f}%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9aef1a0-3745-4010-a242-527a08382b6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c9b66795-f478-4149-8b3e-e0f7158de9c2",
   "metadata": {},
   "source": [
    "## Edges - all works "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b0819c-a0f2-4843-92db-b00f54e90a0d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "path_allauthors = Path('Model_allworks')\n",
    "if not os.path.exists(path_allauthors):\n",
    "    os.makedirs(path_allauthors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ffda7d-7f9f-40f7-adbf-93b9ece1f3ed",
   "metadata": {},
   "source": [
    "### Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1cd6ae-0bbc-4ebf-82e3-2c6d45316691",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "works = read_parquet(basepath / 'works')\n",
    "works_authors_aff = read_parquet(basepath / 'works_authors_aff')\n",
    "\n",
    "works = works.loc['2000-01-01':'2023-12-01'] \n",
    "works = works[works.num_authors>1]\n",
    "\n",
    "works_allauthors = set(works.work_id)\n",
    "print(f'{len(works_allauthors)} ({(len(works_allauthors)/len(works))*100:.2f}%) works all authors')\n",
    "\n",
    "works = works[works.work_id.isin(works_allauthors)]\n",
    "works_authors_aff = works_authors_aff[works_authors_aff.work_id.isin(works_allauthors)]\n",
    "\n",
    "my_file = \"dfs_allauthors.pickle\"\n",
    "pickle.dump([works_allauthors,works,works_authors_aff], open(os.path.join(path_allauthors, my_file), 'wb')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210eb673-0f01-45e6-9726-c6be2c414a93",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#INITIALIZATION #preferential attachment  #TW: 2000-2009\n",
    "my_file = \"dfs_allauthors.pickle\"\n",
    "with open(os.path.join(path_allauthors, my_file),\"rb\") as fp:\n",
    "    [works_allauthors,works,works_authors_aff] = pickle.load(fp)  \n",
    "works = works.loc['2000-01-01':'2023-12-01'] \n",
    "works_authors_aff = works_authors_aff.loc['2000-01-01':'2023-12-01']\n",
    "\n",
    "N_dict = works_authors_aff.groupby('publication_date_1').work_id.nunique().to_frame().to_dict()['work_id']\n",
    "months_list = list(N_dict.keys())\n",
    "months_list.sort()\n",
    "months_list = [i.strftime('%Y-%m-%d') for i in months_list]\n",
    "\n",
    "start_index = 0\n",
    "end_index = 120 #180\n",
    "start_month = months_list[start_index]#.strftime('%Y-%m-%d')\n",
    "end_month = months_list[end_index-1]#.strftime('%Y-%m-%d')\n",
    "\n",
    "df_TW = works_authors_aff.loc[start_month:end_month] #df_TW = works_authors_aff.loc[months_list[:120]]\n",
    "inst_set = set(df_TW.institution_id)\n",
    "I = len(inst_set)\n",
    "print(f'TW from {start_month} to {end_month} : {I} institutions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b0271c-44b1-4081-baa3-c581d6c9c4df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#initial strenghts    #consider only institutions in sample in TW\n",
    "#count number (unique) institutions per paper\n",
    "df_TW = df_TW.drop_duplicates(['work_id','institution_id'])\n",
    "df_TW['num_affs'] = df_TW.groupby('work_id')['institution_id'].transform('size')\n",
    "print(f'{min(df_TW.num_affs)}-{max(df_TW.num_affs)} min-max number (unique) affiliations per work')\n",
    "df_TW['weight'] = 2 / ( df_TW['num_affs']*(df_TW['num_affs']-1) ) \n",
    "df_TW.loc[df_TW.num_affs==1,'weight'] = 1 #one affiliation\n",
    "df_TW_noloops = df_TW[df_TW.num_affs>1]\n",
    "df_TW_loops = df_TW[df_TW.num_affs==1]\n",
    "df_TW_loops['institution_id2'] = df_TW_loops['institution_id']\n",
    "df_TW_loops['weight'] = df_TW_loops[['institution_id','institution_id2','weight']].groupby(['institution_id']).weight.transform('sum')\n",
    "df_TW_loops = df_TW_loops.drop_duplicates('institution_id')\n",
    "I_graph = make_institution_graph(df_TW_noloops)\n",
    "I_graph.add_weighted_edges_from([tuple(r) for r in df_TW_loops[['institution_id','institution_id2','weight']].to_numpy()])\n",
    "df_ = pd.DataFrame.from_dict(dict(I_graph.degree(weight='weight')),orient='index').reset_index().rename(columns={'index':'institution_id',0:'strength'})\n",
    "df_['institution_id'] = df_['institution_id'].astype(int)\n",
    "df = df_.sort_values(by='strength',ascending=False)\n",
    "inst_set = set(df.institution_id)\n",
    "I = len(inst_set)\n",
    "print(f'TW from {start_month} to {end_month} : {I} institutions')\n",
    "\n",
    "my_file = \"df_strengths0.csv\"     \n",
    "df.to_csv(os.path.join(path_allauthors, my_file),index=False)\n",
    "\n",
    "my_file = \"inst_set.pickle\"\n",
    "pickle.dump(inst_set, open(os.path.join(path_allauthors, my_file), 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f006eb6a-8ac9-4e29-af2c-326c3831945b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "I_dist = read_parquet(basepath / 'I_dist_threshold')\n",
    "I_dist = I_dist[['source','target','dist']].reset_index(drop=True)\n",
    "I_dist = I_dist.query('source.isin(@inst_set) & target.isin(@inst_set)').reset_index()\n",
    "my_file = \"I_dist_model.csv\" \n",
    "I_dist.to_csv(os.path.join(path_allauthors, my_file),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b80c992-561a-4e7b-ac06-481bb14a1bfb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3274f6-9dc4-4f2f-ad93-468a50c1a37d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "work_authors_edges_df_dist = read_parquet(Path('./TeamDistance') / 'work_authors_edges_df_dist')\n",
    "work_authors_edges_df_dist = works[['work_id']].reset_index().merge(work_authors_edges_df_dist,on='work_id')\n",
    "work_authors_edges_df_dist = work_authors_edges_df_dist[work_authors_edges_df_dist.work_id.isin(works_allauthors)]\n",
    "work_edges_dist_mean = work_authors_edges_df_dist.groupby('work_id').dist.mean().to_frame().reset_index()\n",
    "work_edges_dist_mean = work_edges_dist_mean.merge(works[['work_id']].reset_index(),on='work_id')\n",
    "work_edges_dist_mean_monthly = work_edges_dist_mean.groupby('publication_date_1').dist.mean().to_frame().reset_index()\n",
    "my_file = \"work_edges_dist_mean.csv\"    \n",
    "work_edges_dist_mean.to_csv(os.path.join(path_allauthors, my_file),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1fcc18e-2a7a-4fe9-9483-5d2a4c87ef67",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "my_file = \"work_edges_dist_mean_monthly.csv\"     \n",
    "work_edges_dist_mean_monthly.to_csv(os.path.join(path_allauthors, my_file),index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b470a33c-9798-4bd1-8f1f-60eb9c8de670",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_data1 = df_data1_(work_authors_edges_df_dist)\n",
    "df_data1['publication_date_1'] = df_data1['publication_date_1'].apply(pd.to_datetime)\n",
    "my_file = \"df_data1.csv\"  \n",
    "df_data1.to_csv(os.path.join(path_allauthors, my_file),index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15af6d8-1015-463a-82d7-338155d2da8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a42f43-1dbf-49f1-b568-c963800df402",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6933f5fc-6555-4107-976c-d7e769ee2aab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
