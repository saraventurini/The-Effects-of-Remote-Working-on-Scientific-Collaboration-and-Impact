{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54cafcea-8c3d-45bc-a447-ef187fbbb0e8",
   "metadata": {},
   "source": [
    "# Model - triangles - 3 authors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dda6401-430d-4d54-ac26-2eed63579723",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from pathlib import Path\n",
    "import networkx as nx\n",
    "import igraph as ig\n",
    "import pickle\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.spatial import distance\n",
    "import seaborn as sns\n",
    "from time import time\n",
    "from tqdm.auto import tqdm\n",
    "import random \n",
    "import os\n",
    "from itertools import chain, combinations\n",
    "import itertools\n",
    "\n",
    "import collections\n",
    "\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "import plotly.io as pio\n",
    "plt.rcParams.update({'font.size': 22})\n",
    "sns.set(style=\"ticks\", context=\"talk\")\n",
    "plt.style.use(\"dark_background\")\n",
    "pd.options.plotting.backend = 'plotly'\n",
    "pio.templates.default = 'plotly_dark+presentation'\n",
    "\n",
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "import matplotlib.dates as dates\n",
    "import matplotlib.dates as mdates\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "from scipy import optimize\n",
    "import numpy.polynomial.polynomial as npoly\n",
    "\n",
    "def form(x,pos):\n",
    "    if x<1e3:\n",
    "        return '%1.3f' % (x)\n",
    "    elif x<1e6:\n",
    "        return '%1.1fK' % (x * 1e-3)\n",
    "    else:\n",
    "        return '%1.1fM' % (x * 1e-6)\n",
    "formatter = FuncFormatter(form)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def read_parquet(name, **args):\n",
    "    path = name\n",
    "    print(f'Reading {name!r}')\n",
    "    tic = time()\n",
    "    df = pd.read_parquet(path, engine='fastparquet', **args)\n",
    "    before = len(df)\n",
    "    # df.drop_duplicates(inplace=True)\n",
    "    toc = time()\n",
    "    after = len(df)\n",
    "        \n",
    "    print(f'Read {len(df):,} rows from {path.stem!r} in {toc-tic:.2f} sec. {before-after:,} duplicates.')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc9c6b31-0f12-4fc9-89af-0d10ff8c8b87",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "basepath = Path('Tables_final') \n",
    "my_path_ = Path('Model_3authors')\n",
    "if not os.path.exists(my_path_):\n",
    "    os.makedirs(my_path_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a6ebc3d-5a4a-4d70-9ff4-921bd251ec09",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36aa90e7-c49a-4ccb-9547-88f27889ebfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#all calculations in 'Model_edges-3authors'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b932a6-f6fb-4288-9d2b-5050254ceb2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for each top n institutions by strength in TW ( data to plot do not change)\n",
    "#calculate possible 3 combinations\n",
    "#calculate dist: arithmetic mean, max, root mean square radius (dist from center of mass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47270dc7-b2cb-4e6f-a0d8-841b6cb1862f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23152f84-ab7b-4743-b411-2c85987af976",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "my_file = \"df_strengths0.csv\"     \n",
    "df_0 = pd.read_csv(os.path.join(my_path_, my_file))\n",
    "my_file = \"inst_set.pickle\"\n",
    "with open(os.path.join(my_path_, my_file),\"rb\") as fp:\n",
    "    inst_set = pickle.load(fp)   \n",
    "len(df_0),len(inst_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e73a162-10ab-4fbd-9eb8-a3074587aa30",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f'max strength {df_0.strength.max():.2f}')\n",
    "for n in [0,5,10,15,20]:\n",
    "    df_n = df_0[df_0.strength>=n]\n",
    "    my_file = \"df_tail\"+str(n)+\"_strengths0.csv\" \n",
    "    df_n.to_csv(os.path.join(my_path_, my_file),index=False)\n",
    "    insts_tailn_TW = set(df_n.institution_id)\n",
    "    my_file = \"insts_tail\"+str(n)+\"_TW.pickle\"\n",
    "    pickle.dump(insts_tailn_TW, open(os.path.join(my_path_, my_file), 'wb'))\n",
    "    print(f'tail {n}: {len(df_n)} institutions ({(len(df_n)/len(df_0))*100:.2f}%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d39f3e-cb24-41f1-a532-2b5885009fd3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f'max strength {df_0.strength.max():.2f}')\n",
    "n=20\n",
    "df_n = df_0[df_0.strength>=n]\n",
    "my_file = \"df_tail\"+str(n)+\"_strengths0.csv\" \n",
    "df_n.to_csv(os.path.join(my_path_, my_file),index=False)\n",
    "insts_tailn_TW = set(df_n.institution_id)\n",
    "my_file = \"insts_tail\"+str(n)+\"_TW.pickle\"\n",
    "pickle.dump(insts_tailn_TW, open(os.path.join(my_path_, my_file), 'wb'))\n",
    "print(f'tail {n}: {len(df_n)} institutions ({(len(df_n)/len(df_0))*100:.2f}%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e889e763-4550-4421-a516-dcf423269e3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c20a8b-47b7-41b1-8c14-bf13db5d7f89",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n = 20 #5: 8mins\n",
    "my_file = \"insts_tail\"+str(n)+\"_TW.pickle\"\n",
    "with open(os.path.join(my_path_, my_file),\"rb\") as fp:\n",
    "    inst_set = pickle.load(fp) \n",
    "inst_set = list(inst_set)\n",
    "inst_set.sort()\n",
    "all_triangles = set(itertools.combinations(list(inst_set), 3))#all possible triangles i<j<k \n",
    "#all_triangles = [sorted(list(x)) for x in all_triangles] #sort institutions\n",
    "all_triangles = pd.DataFrame(all_triangles, columns =['i', 'j', 'k'])\n",
    "# #check limit memory\n",
    "# lll = range(100)\n",
    "# all_triangles = set(itertools.combinations(list(lll), 3))#all possible triangles i<j<k\n",
    "# all_triangles = pd.DataFrame(all_triangles, columns =['i', 'j', 'k'])\n",
    "print(len(all_triangles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79eb83aa-1df6-400e-bdef-99ed857acd82",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(inst_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945b634f-68f6-4266-996e-4cd2e80132ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#distance\n",
    "I_dist = read_parquet(basepath / 'I_dist_threshold')\n",
    "I_dist = I_dist[['source','target','dist']].reset_index(drop=True)\n",
    "I_dist = I_dist.query('source.isin(@inst_set) & target.isin(@inst_set)')\n",
    "all_triangles = all_triangles.merge(I_dist.rename(columns={'source':'i','target':'j','dist':'d_ij'}),on=['i','j'])\n",
    "all_triangles = all_triangles.merge(I_dist.rename(columns={'source':'j','target':'k','dist':'d_jk'}),on=['j','k'])\n",
    "all_triangles = all_triangles.merge(I_dist.rename(columns={'source':'i','target':'k','dist':'d_ik'}),on=['i','k'])\n",
    "all_triangles['dist_mean'] = (all_triangles['d_ij']+all_triangles['d_jk']+all_triangles['d_ik'])/3\n",
    "all_triangles['dist_max'] = all_triangles[['d_ij','d_jk','d_ik']].max(axis=1)\n",
    "#all_triangles = all_triangles.drop(columns=['d_ij','d_jk','d_ik'])\n",
    "all_triangles.to_parquet(os.path.join(my_path_, 'all_triangles'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd8b2e1-b22e-4815-a2bd-55c919bd2275",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a21b48df-43fd-4b86-a5a6-631927398017",
   "metadata": {},
   "source": [
    "## Model 2 variables\n",
    "\n",
    "B_{ijk} = beta * ((s_i*s_j*s_k)^(1/3)/(d_{ijk}+c)^alpha) </br>\n",
    "B_{iii} = gamma * s_i </br>\n",
    "\n",
    "P_{ijk} = B_{ijk}/N(alpha,beta,gamma)</br>\n",
    "P_{iii} = B_{iii}/N(alpha,beta,gamma)</br>\n",
    "\n",
    "with N(alpha,beta,gamma) = sum_{(i,j,k)} B_{ijk} + sum_{i} B_{iii}</br>\n",
    "d_{ijk} = (d_{ij} + d_{jk} + d_{ik})/3</br>\n",
    "\n",
    "Parameters: a=1/3, c=10 </br>\n",
    "Variables: alpha>=0, beta>=0, gamma>=0 </br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e290194e-e2dc-45cb-9796-966b152dfa3c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def strength_update(df_intra,df_inter,df_inter3,W,a,c,params):\n",
    "    N = (params[2] *df_intra['m_source'] ).sum() + ((params[1] *df_inter['m_prod']) / ((2/3)*df_inter['dist']+c)**params[0] ).sum() + (params[1] *(df_inter3['m_prod'] / ((df_inter3['dist']+c)**params[0] )) ).sum()    \n",
    "    df_inter_copy = df_inter.copy()\n",
    "    df_inter['a'] = (params[1] *df_inter['m_prod']) / ((2/3)*df_inter['dist']+c)**params[0] \n",
    "    df_inter_source = df_inter.groupby(['source','m_source']).a.sum().to_frame().reset_index()\n",
    "    df_inter_target = df_inter.groupby(['target','m_target']).a.sum().to_frame().reset_index().rename(columns={'target':'source','m_target':'m_source'})\n",
    "    df_inter3_copy = df_inter3.copy()\n",
    "    df_inter3['a'] = (1/3) * (params[1] *(df_inter3['m_prod'] / ((df_inter3['dist']+c)**params[0] )))                      \n",
    "    df_inter3_i = df_inter3.groupby(['i','m_i']).a.sum().to_frame().reset_index().rename(columns={'i':'source','m_i':'m_source'})\n",
    "    df_inter3_j = df_inter3.groupby(['j','m_j']).a.sum().to_frame().reset_index().rename(columns={'j':'source','m_j':'m_source'})\n",
    "    df_inter3_k = df_inter3.groupby(['k','m_k']).a.sum().to_frame().reset_index().rename(columns={'k':'source','m_k':'m_source'})\n",
    "    df_intra['a'] = df_intra['m_source']*2*params[2]\n",
    "    df = pd.concat([df_inter3_i,df_inter3_j,df_inter3_k,df_inter_source,df_inter_target,df_intra]).groupby(['source','m_source']).a.sum().to_frame().reset_index()\n",
    "    df['m_source'] = df['m_source'] + (W/N)*df['a']\n",
    "    df = df[['source','m_source']].rename(columns={'source':'institution_id','m_source':'strength'})\n",
    "    inst_str_dict = df.set_index('institution_id').to_dict()['strength']\n",
    "    return inst_str_dict\n",
    "\n",
    "def model_function1(df_intra,df_inter,df_inter3,a,c,params):    \n",
    "    N = (params[2]*df_intra['m_source'] ).sum() + ((params[1]*df_inter['m_prod']) / ((2/3)*df_inter['dist']+c)**params[0] ).sum() + ((params[1] *df_inter3['m_prod']) / ((df_inter3['dist']+c)**params[0] )).sum()    \n",
    "    u1 = ( (3*params[2]*df_intra['m_source']).sum() + ((params[1]*df_inter['m_prod']) / (((2/3)*df_inter['dist']+c)**params[0] )).sum()) / (3*N)    \n",
    "    return u1\n",
    "    \n",
    "def model_function2(df_intra,df_inter,df_inter3,a,c,params):\n",
    "    N = (params[2]*df_intra['m_source'] ).sum() + ((params[1]*df_inter['m_prod']) / ((2/3)*df_inter['dist']+c)**params[0]).sum() + (params[1]*(df_inter3['m_prod'] / ((df_inter3['dist']+c)**params[0])) ).sum()    \n",
    "    u2 = (((2/3)*df_inter['dist']*(params[1]*df_inter['m_prod']) / (((2/3)*df_inter['dist']+c)**params[0])).sum() + (df_inter3['dist']*(params[1]*(df_inter3['m_prod'] / ((df_inter3['dist']+c)**params[0]) ))).sum()) / N\n",
    "    return u2\n",
    "    \n",
    "def objective_function(params,a,c, x_data, y_data):\n",
    "    df_intra = x_data[0]\n",
    "    df_inter = x_data[1]\n",
    "    df_inter3 = x_data[2]\n",
    "\n",
    "    y_pred1 = model_function1(df_intra,df_inter,df_inter3,a,c,params)\n",
    "    y_pred2 = model_function2(df_intra,df_inter,df_inter3,a,c,params)\n",
    "    of = ((y_pred1 - y_data[0]) / y_data[0])**2     +  ((y_pred2 - y_data[1]) / y_data[1])**2  \n",
    "\n",
    "    return of "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147e375d-2145-45ff-a16d-cc4b586ddd7c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def model_tail():\n",
    "    \n",
    "    my_file = \"insts_tail\"+str(20)+\"_TW.pickle\"\n",
    "    with open(os.path.join(my_path_, my_file),\"rb\") as fp:\n",
    "        inst_set = pickle.load(fp)     \n",
    "    my_file = \"dfs_3authors.pickle\"\n",
    "    with open(os.path.join(my_path_, my_file),\"rb\") as fp:\n",
    "        [works_3authors,works,works_authors_aff] = pickle.load(fp)  \n",
    "    works = works.loc['2000-01-01':'2023-12-01'] \n",
    "    works_authors_aff = works_authors_aff.loc['2000-01-01':'2023-12-01']\n",
    "    N_dict = works_authors_aff.groupby('publication_date_1').work_id.nunique().to_frame().to_dict()['work_id']\n",
    "    months_list = list(N_dict.keys())\n",
    "    months_list.sort()\n",
    "    months_list = [i.strftime('%Y-%m-%d') for i in months_list]\n",
    "    I_dist = read_parquet(basepath / 'I_dist_threshold')\n",
    "    I_dist = I_dist[['source','target','dist']].reset_index(drop=True)\n",
    "    I_dist = I_dist.query('source.isin(@inst_set) & target.isin(@inst_set)')\n",
    "\n",
    "    c = 100 \n",
    "    a = 1/3\n",
    "\n",
    "    #initial strenghts    \n",
    "    my_file = \"df_tail\"+str(20)+\"_strengths0.csv\"      \n",
    "    df_0 = pd.read_csv(os.path.join(my_path_, my_file))\n",
    "    inst_str_dict = df_0[['institution_id','strength']].set_index('institution_id').to_dict()['strength']\n",
    "\n",
    "    start_index = 0\n",
    "    end_index = 120 #180\n",
    "    start_month = months_list[start_index]#.strftime('%Y-%m-%d')\n",
    "    end_month = months_list[end_index-1]#.strftime('%Y-%m-%d')\n",
    "\n",
    "    #calculate #each month: F_INTRA\n",
    "    my_file = \"df_data1.csv\"    \n",
    "    df_data1 = pd.read_csv(os.path.join(my_path_, my_file))\n",
    "    df_data1 = df_data1.set_index('publication_date_1')\n",
    "    df_data3 = df_data1\n",
    "    df_data3 = df_data3[['total']]\n",
    "    df_data3['total'] = df_data3['total'].astype(int)\n",
    "    df_data3 = df_data3.loc[months_list[end_index]:months_list[-1]]\n",
    "    df_data3_list = list(df_data3['total'])\n",
    "    df_data1 = df_data1[['frac_intra']]\n",
    "    df_data1 = df_data1.rename(columns={'frac_intra':'F_aff'})\n",
    "    df_data1 = df_data1.loc[months_list[end_index]:months_list[-1]] #[months_list[end_index-1]:months_list[-1]]\n",
    "    #each month: average distance\n",
    "    my_file = \"work_edges_dist_mean_monthly.csv\"    \n",
    "    df_data2 = pd.read_csv(os.path.join(my_path_, my_file))\n",
    "    df_data2 = df_data2.set_index('publication_date_1').loc[months_list[end_index]:months_list[-1]]\n",
    "\n",
    "    df = I_dist.copy()\n",
    "    df['m_source'] = df['source'].map(inst_str_dict)\n",
    "    df['m_target'] = df['target'].map(inst_str_dict)\n",
    "    df_intra = df[df.source == df.target]\n",
    "    df_inter = df[df.source != df.target]\n",
    "    df_inter['m_prod1'] = (df_inter['m_source']**2)*df_inter['m_target']\n",
    "    df_inter['m_prod2'] = df_inter['m_source']*(df_inter['m_target']**2)\n",
    "    df_inter['m_prod'] = ((df_inter['m_prod1']**a)+(df_inter['m_prod2']**a))\n",
    "\n",
    "    df_inter3 = read_parquet(my_path_ / 'all_triangles')\n",
    "    df_inter3 = df_inter3[['i','j','k','dist_mean']].rename(columns={'dist_mean':'dist'})\n",
    "    df_inter3['m_i'] = df_inter3['i'].map(inst_str_dict)\n",
    "    df_inter3['m_j'] = df_inter3['j'].map(inst_str_dict)\n",
    "    df_inter3['m_k'] = df_inter3['k'].map(inst_str_dict)\n",
    "    df_inter3['m_prod'] = (df_inter3['m_i']*df_inter3['m_j']*df_inter3['m_k'])**a\n",
    "    df_inter3 = df_inter3[['i','j','k','m_i','m_j','m_k','m_prod','dist']]\n",
    "\n",
    "    opt_dict = {}\n",
    "    x_data = [df_intra,df_inter,df_inter3]\n",
    "    y_data = np.array([list(df_data1['F_aff'])[0],list(df_data2['dist'])[0]])\n",
    "\n",
    "    #randomstart #starting point can be not feasible\n",
    "    np.random.seed(0)\n",
    "    initial_params_list = [[2.0,1.0,1.0]] + [list(np.concatenate([np.random.uniform(0, 5, 1),np.random.uniform(0, 1e3, 2)])) for _ in range(9)]\n",
    "    err_ = +np.inf\n",
    "    initial_params_ = np.nan\n",
    "    result_ = np.nan\n",
    "    for initial_params in tqdm(initial_params_list):\n",
    "        result = minimize(objective_function, initial_params, args=(a,c,x_data,y_data), bounds=((0, np.inf), (0, np.inf), (0, np.inf)), tol = 1e-10, options={'eps': 1e-10, 'ftol': 1e-15})\n",
    "        err = result.fun\n",
    "        success = result.success\n",
    "        print(err,success,result.message)\n",
    "        if err<err_ and success:\n",
    "            initial_params_ = initial_params\n",
    "            result_ = result\n",
    "            err_ = err\n",
    "\n",
    "    my_file = \"initial_params_randomstart_triangles.pickle\"\n",
    "    pickle.dump([initial_params_,result_], open(os.path.join(my_path_, my_file), 'wb'))\n",
    "\n",
    "    params = result_.x\n",
    "    err = result_.fun\n",
    "    success = result_.success\n",
    "    message = result_.message\n",
    "    print(f'{0} {list(df_data3.index)[0]} {params[0]:.5f} {params[1]:.5f} {params[2]:.5f} {params[1]/params[2]:.5f}  {err:.2e} {success} {message}')\n",
    "    opt_dict[0] = {'optimized_alpha':params[0], 'optimized_beta':params[1], 'optimized_gamma':params[2],'beta/gamma':params[1]/params[2],'optimized_err':err, 'success':success, 'message':message}\n",
    "\n",
    "    #update strenght with parameters\n",
    "    for i in tqdm(range(len(df_data3_list)-1)):\n",
    "        W = int(1e5) #df_data3_list[i]\n",
    "        inst_str_dict = strength_update(df_intra,df_inter,df_inter3,W,a,c,params)\n",
    "        \n",
    "        #save\n",
    "        if (i-1)%10 == 0:\n",
    "            my_file = \"conditions_i\"+str(i-1)+\"_triangles.pickle\"\n",
    "            pickle.dump([params,inst_str_dict,opt_dict], open(os.path.join(my_path_, my_file), 'wb'))\n",
    "        \n",
    "        df = I_dist.copy()\n",
    "        df['m_source'] = df['source'].map(inst_str_dict)\n",
    "        df['m_target'] = df['target'].map(inst_str_dict)\n",
    "        df_intra = df[df.source == df.target]\n",
    "        df_inter = df[df.source != df.target]\n",
    "        df_inter['m_prod1'] = df_inter['m_source']**2*df_inter['m_target']\n",
    "        df_inter['m_prod2'] = df_inter['m_source']*df_inter['m_target']**2\n",
    "        df_inter['m_prod'] = (df_inter['m_prod1']**a+df_inter['m_prod2']**a)\n",
    "\n",
    "        df_inter3['m_i'] = df_inter3['i'].map(inst_str_dict)\n",
    "        df_inter3['m_j'] = df_inter3['j'].map(inst_str_dict)\n",
    "        df_inter3['m_k'] = df_inter3['k'].map(inst_str_dict)\n",
    "        df_inter3['m_prod'] = (df_inter3['m_i']*df_inter3['m_j']*df_inter3['m_k'])**a\n",
    "        df_inter3 = df_inter3[['i','j','k','m_i','m_j','m_k','m_prod','dist']]\n",
    "\n",
    "        x_data = [df_intra,df_inter,df_inter3]\n",
    "        y_data = np.array([list(df_data1['F_aff'])[i+1],list(df_data2['dist'])[i+1]])\n",
    "\n",
    "        result = minimize(objective_function, params, args=(a,c,x_data,y_data), bounds=((0, np.inf), (0, np.inf), (0, np.inf)), tol = 1e-10, options={'eps': 1e-10, 'ftol': 1e-15})\n",
    "        params = result.x\n",
    "        err = result.fun\n",
    "        success = result.success\n",
    "        message = result.message\n",
    "        print(f'{i+1} {list(df_data3.index)[i+1]} {params[0]:.5f} {params[1]:.5f} {params[2]:.5f} {params[1]/params[2]:.5f} {err:.2e} {success} {message}')\n",
    "        opt_dict[i+1] = {'optimized_alpha':params[0], 'optimized_beta':params[1], 'optimized_gamma':params[2],'beta/gamma':params[1]/params[2],'optimized_err':err, 'success':success,'message':message}\n",
    "\n",
    "    opt_df = pd.DataFrame.from_dict(opt_dict).T\n",
    "    opt_df['month'] = list(df_data3.index)\n",
    "    my_file = \"opt_df_triangles.csv\"   \n",
    "    opt_df.to_csv(os.path.join(my_path_, my_file),index=False)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033c536e-0c7c-42c0-a135-d32836549f65",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31dc0dea-1cbe-4631-8979-c1423b2ac274",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_model(df,ylabel,title,color,log=False):\n",
    "    plt.style.use(\"dark_background\")\n",
    "    fig, ax = plt.subplots(figsize=(15, 5))   \n",
    "    if log:\n",
    "        ax.semilogy(list(df['month']), df[ylabel], \"o-\", color=color, markersize=3)\n",
    "    else:\n",
    "        ax.plot(list(df['month']), df[ylabel], \"o-\", color=color, markersize=3)\n",
    "    ax.axvline(pd.Timestamp(2020, 3, 1),color='r')\n",
    "    plt.grid(True, linewidth=0.5)\n",
    "    ax.ticklabel_format(axis='y')\n",
    "    ax.set_xlabel('month',size=20)\n",
    "    ax.set_title(title,size=30)\n",
    "\n",
    "my_file = \"opt_df_triangles.csv\" \n",
    "opt_df = pd.read_csv(os.path.join(my_path_, my_file))\n",
    "opt_df['month'] = opt_df['month'].apply(pd.to_datetime)\n",
    "plot_model(opt_df,'optimized_alpha','Optimized alpha','orange')\n",
    "plot_model(opt_df,'beta/gamma','Optimized beta/gamma','red')\n",
    "plot_model(opt_df,'optimized_err','Optimized error','blue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a685092-c4e2-4da6-904e-c6d95a76fbf5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "924d202d-2b0a-40e2-8918-a3223a1a1255",
   "metadata": {},
   "source": [
    "### Simulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb0e8dd-3676-40f5-9ddf-f1db228d9fae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def simulation():\n",
    "    c = 100\n",
    "    a = 1/3\n",
    " \n",
    "    my_file = \"opt_df_triangles.csv\" \n",
    "    opt_df = pd.read_csv(os.path.join(my_path_, my_file))\n",
    "    opt_df['month'] = opt_df['month'].apply(pd.to_datetime)\n",
    "\n",
    "    my_file = \"insts_tail\"+str(20)+\"_TW.pickle\"\n",
    "    with open(os.path.join(my_path_, my_file),\"rb\") as fp:\n",
    "        inst_set = pickle.load(fp)     \n",
    "    my_file = \"dfs_3authors.pickle\"\n",
    "    with open(os.path.join(my_path_, my_file),\"rb\") as fp:\n",
    "        [works_3authors,works,works_authors_aff] = pickle.load(fp)  \n",
    "    works = works.loc['2000-01-01':'2023-12-01'] \n",
    "    works_authors_aff = works_authors_aff.loc['2000-01-01':'2023-12-01']\n",
    "    N_dict = works_authors_aff.groupby('publication_date_1').work_id.nunique().to_frame().to_dict()['work_id']\n",
    "    months_list = list(N_dict.keys())\n",
    "    months_list.sort()\n",
    "    months_list = [i.strftime('%Y-%m-%d') for i in months_list]\n",
    "\n",
    "    #initial strenghts    \n",
    "    my_file = \"df_tail\"+str(20)+\"_strengths0.csv\"  #\"df_strengths0_tail\"+str(20)+\".csv\"      \n",
    "    df_0 = pd.read_csv(os.path.join(my_path_, my_file))\n",
    "    inst_str_dict = df_0[['institution_id','strength']].set_index('institution_id').to_dict()['strength']\n",
    "\n",
    "    start_index = 0\n",
    "    end_index = 120 #180\n",
    "    start_month = months_list[start_index]#.strftime('%Y-%m-%d')\n",
    "    end_month = months_list[end_index-1]#.strftime('%Y-%m-%d')\n",
    "\n",
    "    #calculate #each month: F_INTRA\n",
    "    my_file = \"df_data1.csv\"    \n",
    "    df_data1 = pd.read_csv(os.path.join(my_path_, my_file))\n",
    "    df_data1 = df_data1.set_index('publication_date_1')\n",
    "    df_data3 = df_data1\n",
    "    df_data3 = df_data3[['total']]\n",
    "    df_data3['total'] = df_data3['total'].astype(int)\n",
    "    df_data3 = df_data3.loc[months_list[end_index]:months_list[-1]]\n",
    "    df_data3_list = list(df_data3['total'])\n",
    "    df_data1 = df_data1[['frac_intra']]\n",
    "    df_data1 = df_data1.rename(columns={'frac_intra':'F_aff'})\n",
    "    df_data1 = df_data1.loc[months_list[end_index]:months_list[-1]] #[months_list[end_index-1]:months_list[-1]]\n",
    "    #each month: average distance\n",
    "    my_file = \"work_edges_dist_mean_monthly.csv\"    \n",
    "    df_data2 = pd.read_csv(os.path.join(my_path_, my_file))\n",
    "    df_data2 = df_data2.set_index('publication_date_1').loc[months_list[end_index]:months_list[-1]]\n",
    "\n",
    "    df_inter3 = read_parquet(my_path_/ 'all_triangles')\n",
    "    df_inter3 = df_inter3[['i','j','k','dist_mean']].rename(columns={'dist_mean':'dist'})\n",
    "    \n",
    "    I_dist = read_parquet(basepath / 'I_dist_threshold')\n",
    "    I_dist = I_dist[['source','target','dist']].reset_index(drop=True)\n",
    "    I_dist = I_dist.query('source.isin(@inst_set) & target.isin(@inst_set)')  \n",
    "    df_intra = I_dist[I_dist.source == I_dist.target].reset_index(drop=True)\n",
    "    df_inter = I_dist[I_dist.source != I_dist.target].reset_index(drop=True)\n",
    "    df_inter.index += len(df_intra)\n",
    "    df_inter3 = df_inter3.reset_index(drop=True)\n",
    "    df_inter3.index += len(df_intra)+len(df_inter)\n",
    "\n",
    "        \n",
    "    random.seed(0)\n",
    "    for s in tqdm(range(1)):   \n",
    "        \n",
    "        df_ = df_0\n",
    "\n",
    "        fra_intra_list = []\n",
    "        mean_dist_list = []\n",
    "        for i in tqdm(range(len(df_data3_list))):\n",
    "\n",
    "            #update \n",
    "            alpha = list(opt_df['optimized_alpha'])[i]  \n",
    "            beta = list(opt_df['optimized_beta'])[i]\n",
    "            gamma = list(opt_df['optimized_gamma'])[i]\n",
    "\n",
    "            inst_str_dict = df_[['institution_id','strength']].set_index('institution_id').to_dict()['strength']\n",
    "\n",
    "            df_intra['m_source'] = df_intra['source'].map(inst_str_dict)\n",
    "            #df_intra['m_target'] = df_intra['target'].map(inst_str_dict)\n",
    "            df_inter['m_source'] = df_inter['source'].map(inst_str_dict)\n",
    "            df_inter['m_target'] = df_inter['target'].map(inst_str_dict)\n",
    "            df_inter['m_prod1'] = (df_inter['m_source']**2)*df_inter['m_target']\n",
    "            df_inter['m_prod2'] = df_inter['m_source']*(df_inter['m_target']**2)\n",
    "            df_inter['m_prod'] = ((df_inter['m_prod1']**a)+(df_inter['m_prod2']**a)) #3 authors\n",
    "            df_inter = df_inter.drop(columns=['m_prod1','m_prod2'])\n",
    "            df_inter3['m_i'] = df_inter3['i'].map(inst_str_dict)\n",
    "            df_inter3['m_j'] = df_inter3['j'].map(inst_str_dict)\n",
    "            df_inter3['m_k'] = df_inter3['k'].map(inst_str_dict)\n",
    "            df_inter3['m_prod'] = (df_inter3['m_i']*df_inter3['m_j']*df_inter3['m_k'])**a\n",
    "            df_inter3 = df_inter3[['i','j','k','m_i','m_j','m_k','m_prod','dist']]  \n",
    "            \n",
    "            df_intra['p']= gamma*df_intra['m_source'] \n",
    "            df_inter['p'] = df_inter['m_prod'] * (beta/ ((2/3)*df_inter['dist']+c)**alpha) \n",
    "            df_inter3['p'] = df_inter3['m_prod'] * (beta/ ((df_inter3['dist']+c)**alpha)) \n",
    "\n",
    "            hyperedges_probabilities = list(itertools.chain(list(itertools.chain(df_intra['p'], df_inter['p'])),df_inter3['p']))\n",
    "            hyperedges_probabilities = np.array(hyperedges_probabilities)/(df_intra['p'].sum()+df_inter['p'].sum()+df_inter3['p'].sum())\n",
    "            W = int(1e5) #df_data3_list[i]\n",
    "            hyperedges_model = random.choices(np.arange(0, len(hyperedges_probabilities)), weights=hyperedges_probabilities, k=W)\n",
    "\n",
    "            #count edges\n",
    "            counter = dict(collections.Counter(hyperedges_model))\n",
    "            df_intra['count'] = df_intra.index.to_series().map(counter)\n",
    "            df_intra['count'] = df_intra['count'].fillna(0)\n",
    "            df_inter['count'] = df_inter.index.to_series().map(counter)\n",
    "            df_inter['count'] = df_inter['count'].fillna(0)\n",
    "            df_inter3['count'] = df_inter3.index.to_series().map(counter)\n",
    "            df_inter3['count'] = df_inter3['count'].fillna(0)\n",
    "\n",
    "            #count intra-inter\n",
    "            frac_intra = (3*sum(df_intra['count'])+1*sum(df_inter['count']))/(3*df_intra['count'].sum()+3*df_inter['count'].sum()+3*df_inter3['count'].sum())\n",
    "            fra_intra_list.append(frac_intra)\n",
    "\n",
    "            #mean team distace\n",
    "            df_inter['mean_dist'] = (2/3)*df_inter['dist']*df_inter['count']\n",
    "            df_inter3['mean_dist'] = df_inter3['dist']*df_inter3['count']\n",
    "            mean_dist = (df_inter.mean_dist.sum()+df_inter3.mean_dist.sum())/W\n",
    "            mean_dist_list.append(mean_dist)\n",
    "\n",
    "            #update strength\n",
    "            df_intra['count'] = df_intra['count']*2\n",
    "            df_inter3['count'] = df_inter3['count']*(1/3)\n",
    "            temp = pd.concat([\n",
    "                df_intra[['source','count']].rename(columns={'source':'institution_id'}),\n",
    "                df_inter[['source','count']].rename(columns={'source':'institution_id'}),\n",
    "                df_inter[['target','count']].rename(columns={'target':'institution_id'}),\n",
    "                df_inter3[['i','count']].rename(columns={'i':'institution_id'}),\n",
    "                df_inter3[['j','count']].rename(columns={'j':'institution_id'}),\n",
    "                df_inter3[['k','count']].rename(columns={'k':'institution_id'})])\n",
    "            temp = temp.groupby('institution_id')['count'].sum().to_frame().reset_index()\n",
    "            df_old = df_\n",
    "            df_ = df_[['institution_id','strength']]\n",
    "            df_ = df_.merge(temp,on='institution_id',how='left')\n",
    "            df_['count'] = df_['count'].fillna(0)\n",
    "            df_['strength'] = df_['strength'] + df_['count']\n",
    "            df_ = df_[['institution_id','strength']]\n",
    "\n",
    "\n",
    "        model_data = pd.DataFrame.from_dict({'fra_intra':fra_intra_list,'mean_dist':mean_dist_list})\n",
    "        model_data['month'] = months_list[end_index:]\n",
    "        my_file = \"simulation_triangles_\"+str(s)+\".csv\"  \n",
    "        model_data.to_csv(os.path.join(my_path_, my_file))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92074e73-dd60-476c-a329-2f910b0cbdbf",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "simulation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af34fa1a-4a66-4e96-8b9a-b6bc7f77aa3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0758ac04-d781-4ecf-a76e-66eedeff4430",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_simulation():\n",
    "    \n",
    "    my_file = \"simulation_triangles_\"+str(0)+\".csv\"  \n",
    "    model_data = pd.read_csv(os.path.join(my_path_, my_file))\n",
    "    model_data['month'] = pd.to_datetime(model_data['month'])\n",
    "    months_list = list(model_data['month'])\n",
    "    months_list.sort()\n",
    "    months_list = [i.strftime('%Y-%m-%d') for i in months_list]\n",
    "    \n",
    "    #calculate #each month: F_INTRA\n",
    "    my_file = \"df_data1.csv\"    \n",
    "    df_data1 = pd.read_csv(os.path.join(my_path_, my_file))\n",
    "    df_data1 = df_data1.set_index('publication_date_1')\n",
    "    df_data1 = df_data1[['frac_intra']]\n",
    "    df_data1 = df_data1.rename(columns={'frac_intra':'F_aff'})\n",
    "    df_data1 = df_data1.loc[months_list] #[months_list[end_index]:months_list[-1]]\n",
    "    #each month: average distance\n",
    "    my_file = \"work_edges_dist_mean_monthly.csv\"    \n",
    "    work_edges_dist_mean_monthly = pd.read_csv(os.path.join(my_path_, my_file))\n",
    "    work_edges_dist_mean_monthly = work_edges_dist_mean_monthly.set_index('publication_date_1')\n",
    "    work_edges_dist_mean_monthly = work_edges_dist_mean_monthly.loc[months_list]\n",
    "    df_data2 = work_edges_dist_mean_monthly\n",
    "\n",
    "    for s in range(1):\n",
    "        my_file = \"simulation_triangles_\"+str(s)+\".csv\" \n",
    "        model_data = pd.read_csv(os.path.join(my_path_, my_file))\n",
    "        model_data['month'] = pd.to_datetime(model_data['month'])\n",
    "        months_list = list(model_data['month'])\n",
    "        months_list.sort()\n",
    "        months_list = [i.strftime('%Y-%m-%d') for i in months_list]\n",
    "\n",
    "        plt.style.use(\"dark_background\")\n",
    "        fig, ax = plt.subplots(figsize=(15, 5))\n",
    "        x_dates = list(model_data['month'])\n",
    "        x_data = x_dates\n",
    "        y_data1 = list(df_data1.F_aff)   \n",
    "        y_data2 = list(model_data['fra_intra'])\n",
    "        ax.plot(x_data, y_data1, \"o-\", markersize=3,label='data')\n",
    "        ax.plot(x_data, y_data2, \"o-\", markersize=3,label='model')\n",
    "        ax.axvline(pd.Timestamp(2020, 3, 1),color='r')\n",
    "        plt.grid(True, linewidth=0.5)\n",
    "        ax.yaxis.set_major_formatter(formatter)\n",
    "        ax.set_xlabel('month',size=20)\n",
    "        ax.legend() \n",
    "        ax.set_title('Frac intra-insts collabs - with exact params',size=30)\n",
    "        \n",
    "    for s in range(1):\n",
    "        my_file = \"simulation_triangles_\"+str(s)+\".csv\" \n",
    "        model_data = pd.read_csv(os.path.join(my_path_, my_file))\n",
    "        model_data['month'] = pd.to_datetime(model_data['month'])\n",
    "        months_list = list(model_data['month'])\n",
    "        months_list.sort()\n",
    "        months_list = [i.strftime('%Y-%m-%d') for i in months_list]\n",
    "\n",
    "        plt.style.use(\"dark_background\")\n",
    "        fig, ax = plt.subplots(figsize=(15, 5))\n",
    "        x_data = x_dates\n",
    "        y_data1 = list(df_data2.dist)   \n",
    "        y_data2 = list(model_data['mean_dist'])\n",
    "        ax.plot(x_data, y_data1, \"o-\", markersize=3,label='data')\n",
    "        ax.plot(x_data, y_data2, \"o-\", markersize=3,label='model')\n",
    "        ax.axvline(pd.Timestamp(2020, 3, 1),color='r')\n",
    "        plt.grid(True, linewidth=0.5)\n",
    "        ax.yaxis.set_major_formatter(formatter)\n",
    "        ax.set_xlabel('month',size=20)\n",
    "        ax.legend() \n",
    "        ax.set_title('Avg team dist - with exact params',size=30)\n",
    "\n",
    "plot_simulation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba3447d-44fb-45c5-a1e4-9b2c227c3928",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe304f8-00e1-432e-aa68-7f4809320446",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "500740d5-e867-423a-ac1d-1dc88056c7ab",
   "metadata": {},
   "source": [
    "## Model 1 variable\n",
    "\n",
    "B_{ijk} = beta * ((s_i*s_j*s_k)^(1/3)/(d_{ijk}+c)^alpha) </br>\n",
    "B_{iii} = gamma * s_i </br>\n",
    "\n",
    "P_{ijk} = B_{ijk}/N(alpha,beta,gamma)</br>\n",
    "P_{iii} = B_{iii}/N(alpha,beta,gamma)</br>\n",
    "\n",
    "with N(alpha,beta,gamma) = sum_{(i,j,k)} B_{ijk} + sum_{i} B_{iii}</br>\n",
    "d_{ijk} = (d_{ij} + d_{jk} + d_{ik})/3</br>\n",
    "\n",
    "Parameters: c=10 </br>\n",
    "Variables: alpha>=0 </br>\n",
    "Fixed: beta>=0, gamma>=0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ddc7a6f-a377-4788-b8ff-c968cad9960a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "my_file = \"opt_df_triangles.csv\" \n",
    "opt_df = pd.read_csv(os.path.join(my_path_, my_file))\n",
    "opt_df['month'] = opt_df['month'].apply(pd.to_datetime)\n",
    "beta = opt_df['optimized_beta'].mean()\n",
    "gamma = opt_df['optimized_gamma'].mean()\n",
    "print(f'beta {beta}, gamma {gamma}, beta/gamma {beta/gamma}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6154ffc4-b21e-4855-90e5-f8a7772084fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def strength_update(df_intra,df_inter,df_inter3,W,a,c,beta,gamma,params):\n",
    "    N = (gamma*df_intra['m_source'] ).sum() + ((beta*df_inter['m_prod']) / ((2/3)*df_inter['dist']+c)**params).sum() + (beta*(df_inter3['m_prod'] / ((df_inter3['dist']+c)**params)) ).sum()    \n",
    "    df_inter_copy = df_inter.copy()\n",
    "    df_inter['a'] = (beta*df_inter['m_prod']) / ((2/3)*df_inter['dist']+c)**params\n",
    "    df_inter_source = df_inter.groupby(['source','m_source']).a.sum().to_frame().reset_index()\n",
    "    df_inter_target = df_inter.groupby(['target','m_target']).a.sum().to_frame().reset_index().rename(columns={'target':'source','m_target':'m_source'})\n",
    "    df_inter3_copy = df_inter3.copy()\n",
    "    df_inter3['a'] = (1/3) * (beta*(df_inter3['m_prod'] / ((df_inter3['dist']+c)**params)))                      \n",
    "    df_inter3_i = df_inter3.groupby(['i','m_i']).a.sum().to_frame().reset_index().rename(columns={'i':'source','m_i':'m_source'})\n",
    "    df_inter3_j = df_inter3.groupby(['j','m_j']).a.sum().to_frame().reset_index().rename(columns={'j':'source','m_j':'m_source'})\n",
    "    df_inter3_k = df_inter3.groupby(['k','m_k']).a.sum().to_frame().reset_index().rename(columns={'k':'source','m_k':'m_source'})\n",
    "    df_intra['a'] = df_intra['m_source']*2*gamma\n",
    "    df = pd.concat([df_inter3_i,df_inter3_j,df_inter3_k,df_inter_source,df_inter_target,df_intra]).groupby(['source','m_source']).a.sum().to_frame().reset_index()\n",
    "    df['m_source'] = df['m_source'] + (W/N)*df['a']\n",
    "    df = df[['source','m_source']].rename(columns={'source':'institution_id','m_source':'strength'})\n",
    "    inst_str_dict = df.set_index('institution_id').to_dict()['strength']\n",
    "    return inst_str_dict\n",
    "\n",
    "def model_function1(df_intra,df_inter,df_inter3,a,c,beta,gamma,params):    \n",
    "    N = (gamma*df_intra['m_source'] ).sum() + ((beta*df_inter['m_prod']) / ((2/3)*df_inter['dist']+c)**params).sum() + (beta*(df_inter3['m_prod'] / ((df_inter3['dist']+c)**params)) ).sum()    \n",
    "    u1 = (( 3*gamma*df_intra['m_source'] ).sum() +    ((beta*df_inter['m_prod']) / (((2/3)*df_inter['dist']+c)**params)).sum()) / (3*N)    \n",
    "    return u1\n",
    "    \n",
    "def model_function2(df_intra,df_inter,df_inter3,a,c,beta,gamma,params):\n",
    "    N = (gamma*df_intra['m_source'] ).sum() + ((beta*df_inter['m_prod']) / ((2/3)*df_inter['dist']+c)**params).sum() + (beta*(df_inter3['m_prod'] / ((df_inter3['dist']+c)**params)) ).sum()    \n",
    "    u2 = (((2/3)*df_inter['dist']*(beta*df_inter['m_prod']) / (((2/3)*df_inter['dist']+c)**params)).sum() + (df_inter3['dist']*(beta*(df_inter3['m_prod'] / ((df_inter3['dist']+c)**params) ))).sum()) / N\n",
    "    return u2\n",
    "    \n",
    "def objective_function(params,a,c,beta,gamma, x_data, y_data):\n",
    "    df_intra = x_data[0]\n",
    "    df_inter = x_data[1]\n",
    "    df_inter3 = x_data[2]\n",
    "\n",
    "    y_pred1 = model_function1(df_intra,df_inter,df_inter3,a,c,beta,gamma,params)\n",
    "    y_pred2 = model_function2(df_intra,df_inter,df_inter3,a,c,beta,gamma,params)\n",
    "    of = ((y_pred1 - y_data[0]) / y_data[0])**2     +  ((y_pred2 - y_data[1]) / y_data[1])**2  \n",
    "\n",
    "    return of "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff585ff-f066-4962-8d7a-0f27915d0b1a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def model_b():\n",
    "    \n",
    "    my_file = \"insts_tail\"+str(20)+\"_TW.pickle\"\n",
    "    with open(os.path.join(my_path_, my_file),\"rb\") as fp:\n",
    "        inst_set = pickle.load(fp)     \n",
    "    my_file = \"dfs_3authors.pickle\"\n",
    "    with open(os.path.join(my_path_, my_file),\"rb\") as fp:\n",
    "        [works_3authors,works,works_authors_aff] = pickle.load(fp)  \n",
    "    works = works.loc['2000-01-01':'2023-12-01'] \n",
    "    works_authors_aff = works_authors_aff.loc['2000-01-01':'2023-12-01']\n",
    "    N_dict = works_authors_aff.groupby('publication_date_1').work_id.nunique().to_frame().to_dict()['work_id']\n",
    "    months_list = list(N_dict.keys())\n",
    "    months_list.sort()\n",
    "    months_list = [i.strftime('%Y-%m-%d') for i in months_list]\n",
    "    I_dist = read_parquet(basepath / 'I_dist_threshold')\n",
    "    I_dist = I_dist[['source','target','dist']].reset_index(drop=True)\n",
    "    I_dist = I_dist.query('source.isin(@inst_set) & target.isin(@inst_set)')\n",
    "\n",
    "    c = 100 \n",
    "    a = 1/3\n",
    "\n",
    "    #initial strenghts    \n",
    "    my_file = \"df_tail\"+str(20)+\"_strengths0.csv\"     \n",
    "    df_0 = pd.read_csv(os.path.join(my_path_, my_file))\n",
    "    inst_str_dict = df_0[['institution_id','strength']].set_index('institution_id').to_dict()['strength']\n",
    "\n",
    "    start_index = 0\n",
    "    end_index = 120 #180\n",
    "    start_month = months_list[start_index]#.strftime('%Y-%m-%d')\n",
    "    end_month = months_list[end_index-1]#.strftime('%Y-%m-%d')\n",
    "\n",
    "    #calculate #each month: F_INTRA\n",
    "    my_file = \"df_data1.csv\"    \n",
    "    df_data1 = pd.read_csv(os.path.join(my_path_, my_file))\n",
    "    df_data1 = df_data1.set_index('publication_date_1')\n",
    "    df_data3 = df_data1\n",
    "    df_data3 = df_data3[['total']]\n",
    "    df_data3['total'] = df_data3['total'].astype(int)\n",
    "    df_data3 = df_data3.loc[months_list[end_index]:months_list[-1]]\n",
    "    df_data3_list = list(df_data3['total'])\n",
    "    df_data1 = df_data1[['frac_intra']]\n",
    "    df_data1 = df_data1.rename(columns={'frac_intra':'F_aff'})\n",
    "    df_data1 = df_data1.loc[months_list[end_index]:months_list[-1]] #[months_list[end_index-1]:months_list[-1]]\n",
    "    #each month: average distance\n",
    "    my_file = \"work_edges_dist_mean_monthly.csv\"    \n",
    "    df_data2 = pd.read_csv(os.path.join(my_path_, my_file))\n",
    "    df_data2 = df_data2.set_index('publication_date_1').loc[months_list[end_index]:months_list[-1]]\n",
    "\n",
    "    df = I_dist.copy()\n",
    "    df['m_source'] = df['source'].map(inst_str_dict)\n",
    "    df['m_target'] = df['target'].map(inst_str_dict)\n",
    "    df_intra = df[df.source == df.target]\n",
    "    df_inter = df[df.source != df.target]\n",
    "    df_inter['m_prod1'] = (df_inter['m_source']**2)*df_inter['m_target']\n",
    "    df_inter['m_prod2'] = df_inter['m_source']*(df_inter['m_target']**2)\n",
    "    df_inter['m_prod'] = ((df_inter['m_prod1']**a)+(df_inter['m_prod2']**a))\n",
    "\n",
    "    df_inter3 = read_parquet(my_path_ / 'all_triangles')\n",
    "    df_inter3 = df_inter3[['i','j','k','dist_mean']].rename(columns={'dist_mean':'dist'})\n",
    "    df_inter3['m_i'] = df_inter3['i'].map(inst_str_dict)\n",
    "    df_inter3['m_j'] = df_inter3['j'].map(inst_str_dict)\n",
    "    df_inter3['m_k'] = df_inter3['k'].map(inst_str_dict)\n",
    "    df_inter3['m_prod'] = (df_inter3['m_i']*df_inter3['m_j']*df_inter3['m_k'])**a\n",
    "    df_inter3 = df_inter3[['i','j','k','m_i','m_j','m_k','m_prod','dist']]\n",
    "\n",
    "    opt_dict = {}\n",
    "    x_data = [df_intra,df_inter,df_inter3]\n",
    "    y_data = np.array([list(df_data1['F_aff'])[0],list(df_data2['dist'])[0]])\n",
    "\n",
    "    my_file = \"opt_df_triangles.csv\" \n",
    "    opt_df = pd.read_csv(os.path.join(my_path_, my_file))\n",
    "    opt_df['month'] = opt_df['month'].apply(pd.to_datetime)\n",
    "    beta = opt_df['optimized_beta'].mean()\n",
    "    gamma = opt_df['optimized_gamma'].mean()\n",
    "\n",
    "    #randomstart #starting point can be not feasible\n",
    "    np.random.seed(0)\n",
    "    initial_params_list = [2.0] + [np.random.uniform(0, 10, 1)[0] for _ in range(19)]\n",
    "    err_ = +np.inf\n",
    "    initial_params_ = np.nan\n",
    "    result_ = np.nan\n",
    "    for initial_params in tqdm(initial_params_list):\n",
    "        result = minimize(objective_function, initial_params, args=(a,c,beta,gamma,x_data,y_data), bounds=[(0, np.inf)], tol = 1e-10) #, options={'eps': 1e-10, 'ftol': 1e-15}\n",
    "        err = result.fun\n",
    "        success = result.success\n",
    "        print(err,success,result.message)\n",
    "        if err<err_ and success:\n",
    "            initial_params_ = initial_params\n",
    "            result_ = result\n",
    "            err_ = err\n",
    "\n",
    "    my_file = \"initial_params_randomstart_triangles_b.pickle\"\n",
    "    pickle.dump([initial_params_,result_], open(os.path.join(my_path_, my_file), 'wb'))\n",
    "\n",
    "    params = result_.x\n",
    "    err = result_.fun\n",
    "    success = result_.success\n",
    "    message = result_.message\n",
    "    print(f'{0} {list(df_data3.index)[0]} {params[0]:.5f} {err:.2e} {success} {message}')\n",
    "    opt_dict[0] = {'optimized_alpha':params[0], 'optimized_err':err, 'success':success, 'message':message}\n",
    "\n",
    "    #update strenght with parameters\n",
    "    for i in tqdm(range(len(df_data3_list)-1)):\n",
    "        W = int(1e5) #df_data3_list[i]\n",
    "        inst_str_dict = strength_update(df_intra,df_inter,df_inter3,W,a,c,beta,gamma,params)\n",
    "        \n",
    "        #save\n",
    "        if (i-1)%10 == 0:\n",
    "            my_file = \"conditions_i\"+str(i-1)+\"_triangles_b.pickle\"\n",
    "            pickle.dump([params,inst_str_dict,opt_dict], open(os.path.join(my_path_, my_file), 'wb'))\n",
    "        \n",
    "        df = I_dist.copy()\n",
    "        df['m_source'] = df['source'].map(inst_str_dict)\n",
    "        df['m_target'] = df['target'].map(inst_str_dict)\n",
    "        df_intra = df[df.source == df.target]\n",
    "        df_inter = df[df.source != df.target]\n",
    "        df_inter['m_prod1'] = df_inter['m_source']**2*df_inter['m_target']\n",
    "        df_inter['m_prod2'] = df_inter['m_source']*df_inter['m_target']**2\n",
    "        df_inter['m_prod'] = (df_inter['m_prod1']**a+df_inter['m_prod2']**a)\n",
    "\n",
    "        df_inter3['m_i'] = df_inter3['i'].map(inst_str_dict)\n",
    "        df_inter3['m_j'] = df_inter3['j'].map(inst_str_dict)\n",
    "        df_inter3['m_k'] = df_inter3['k'].map(inst_str_dict)\n",
    "        df_inter3['m_prod'] = (df_inter3['m_i']*df_inter3['m_j']*df_inter3['m_k'])**a\n",
    "        df_inter3 = df_inter3[['i','j','k','m_i','m_j','m_k','m_prod','dist']]\n",
    "\n",
    "        x_data = [df_intra,df_inter,df_inter3]\n",
    "        y_data = np.array([list(df_data1['F_aff'])[i+1],list(df_data2['dist'])[i+1]])\n",
    "\n",
    "        result = minimize(objective_function, params, args=(a,c,beta,gamma,x_data,y_data), bounds=[(0, np.inf)], tol = 1e-10) #, options={'eps': 1e-10, 'ftol': 1e-15}\n",
    "        params = result.x\n",
    "        err = result.fun\n",
    "        success = result.success\n",
    "        message = result.message\n",
    "        print(f'{i+1} {list(df_data3.index)[i+1]} {params[0]:.5f} {err:.2e} {success} {message}')\n",
    "        opt_dict[i+1] = {'optimized_alpha':params[0],'optimized_err':err, 'success':success,'message':message}\n",
    "\n",
    "    opt_df = pd.DataFrame.from_dict(opt_dict).T\n",
    "    opt_df['month'] = list(df_data3.index)\n",
    "    my_file = \"opt_df_triangles_b.csv\"   \n",
    "    opt_df.to_csv(os.path.join(my_path_, my_file),index=False)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d671207-2021-4095-862d-66526db0c282",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_b()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "385cbe31-5f8b-4ef1-b05d-c796dfa9e135",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caff4af1-5e6a-45d7-a9bd-df76d2e5556b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_model_b(df,ylabel,title,color,log=False):\n",
    "    plt.style.use(\"dark_background\")\n",
    "    fig, ax = plt.subplots(figsize=(15, 5))   \n",
    "    if log:\n",
    "        ax.semilogy(list(df['month']), df[ylabel], \"o-\", color=color, markersize=3)\n",
    "    else:\n",
    "        ax.plot(list(df['month']), df[ylabel], \"o-\", color=color, markersize=3)\n",
    "    ax.axvline(pd.Timestamp(2020, 3, 1),color='r')\n",
    "    plt.grid(True, linewidth=0.5)\n",
    "    ax.ticklabel_format(axis='y')\n",
    "    ax.set_xlabel('month',size=20)\n",
    "    ax.set_title(title,size=30)\n",
    "\n",
    "my_file = \"opt_df_triangles_b.csv\"   \n",
    "opt_df = pd.read_csv(os.path.join(my_path_, my_file))\n",
    "opt_df['month'] = opt_df['month'].apply(pd.to_datetime)\n",
    "plot_model_b(opt_df,'optimized_alpha','Optimized alpha','orange')\n",
    "plot_model_b(opt_df,'optimized_err','Optimized error','blue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3c2f4d-71b5-43c9-858f-87440ab7d290",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5a83bf4d-66f9-445d-852e-a33ec1641473",
   "metadata": {},
   "source": [
    "### Simulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de14066-1e8a-4d3c-93ce-f6564b3796c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def simulation_b():\n",
    "    c = 100\n",
    "    a = 1/3\n",
    " \n",
    "    my_file = \"opt_df_triangles.csv\" \n",
    "    opt_df = pd.read_csv(os.path.join(my_path_, my_file))\n",
    "    opt_df['month'] = opt_df['month'].apply(pd.to_datetime)\n",
    "    beta = opt_df['optimized_beta'].mean()\n",
    "    gamma = opt_df['optimized_gamma'].mean()\n",
    "    \n",
    "    my_file = \"opt_df_triangles_b.csv\" \n",
    "    opt_df = pd.read_csv(os.path.join(my_path_, my_file))\n",
    "    opt_df['month'] = opt_df['month'].apply(pd.to_datetime)    \n",
    "    \n",
    "    my_file = \"insts_tail\"+str(20)+\"_TW.pickle\"\n",
    "    with open(os.path.join(my_path_, my_file),\"rb\") as fp:\n",
    "        inst_set = pickle.load(fp)     \n",
    "    my_file = \"dfs_3authors.pickle\"\n",
    "    with open(os.path.join(my_path_, my_file),\"rb\") as fp:\n",
    "        [works_3authors,works,works_authors_aff] = pickle.load(fp)  \n",
    "    works = works.loc['2000-01-01':'2023-12-01'] \n",
    "    works_authors_aff = works_authors_aff.loc['2000-01-01':'2023-12-01']\n",
    "    N_dict = works_authors_aff.groupby('publication_date_1').work_id.nunique().to_frame().to_dict()['work_id']\n",
    "    months_list = list(N_dict.keys())\n",
    "    months_list.sort()\n",
    "    months_list = [i.strftime('%Y-%m-%d') for i in months_list]\n",
    "\n",
    "    #initial strenghts    \n",
    "    my_file = \"df_tail\"+str(20)+\"_strengths0.csv\"      \n",
    "    df_0 = pd.read_csv(os.path.join(my_path_, my_file))\n",
    "    inst_str_dict = df_0[['institution_id','strength']].set_index('institution_id').to_dict()['strength']\n",
    "\n",
    "    start_index = 0\n",
    "    end_index = 120 #180\n",
    "    start_month = months_list[start_index]#.strftime('%Y-%m-%d')\n",
    "    end_month = months_list[end_index-1]#.strftime('%Y-%m-%d')\n",
    "\n",
    "    #calculate #each month: F_INTRA\n",
    "    my_file = \"df_data1.csv\"    \n",
    "    df_data1 = pd.read_csv(os.path.join(my_path_, my_file))\n",
    "    df_data1 = df_data1.set_index('publication_date_1')\n",
    "    df_data3 = df_data1\n",
    "    df_data3 = df_data3[['total']]\n",
    "    df_data3['total'] = df_data3['total'].astype(int)\n",
    "    df_data3 = df_data3.loc[months_list[end_index]:months_list[-1]]\n",
    "    df_data3_list = list(df_data3['total'])\n",
    "    df_data1 = df_data1[['frac_intra']]\n",
    "    df_data1 = df_data1.rename(columns={'frac_intra':'F_aff'})\n",
    "    df_data1 = df_data1.loc[months_list[end_index]:months_list[-1]] #[months_list[end_index-1]:months_list[-1]]\n",
    "    #each month: average distance\n",
    "    my_file = \"work_edges_dist_mean_monthly.csv\"    \n",
    "    df_data2 = pd.read_csv(os.path.join(my_path_, my_file))\n",
    "    df_data2 = df_data2.set_index('publication_date_1').loc[months_list[end_index]:months_list[-1]]\n",
    "\n",
    "    df_inter3 = read_parquet(my_path_ / 'all_triangles')\n",
    "    df_inter3 = df_inter3[['i','j','k','dist_mean']].rename(columns={'dist_mean':'dist'})\n",
    "    \n",
    "    I_dist = read_parquet(basepath / 'I_dist_threshold')\n",
    "    I_dist = I_dist[['source','target','dist']].reset_index(drop=True)\n",
    "    I_dist = I_dist.query('source.isin(@inst_set) & target.isin(@inst_set)')    \n",
    "    df_intra = I_dist[I_dist.source == I_dist.target].reset_index(drop=True)\n",
    "    df_inter = I_dist[I_dist.source != I_dist.target].reset_index(drop=True)\n",
    "    df_inter.index += len(df_intra)\n",
    "    df_inter3 = df_inter3.reset_index(drop=True)\n",
    "    df_inter3.index += len(df_intra)+len(df_inter)\n",
    "        \n",
    "    random.seed(0)\n",
    "    for s in tqdm(range(10)):   \n",
    "        \n",
    "        df_ = df_0\n",
    "\n",
    "        fra_intra_list = []\n",
    "        mean_dist_list = []\n",
    "        for i in tqdm(range(len(df_data3_list))):\n",
    "\n",
    "            #update \n",
    "            alpha = list(opt_df['optimized_alpha'])[i]  \n",
    "\n",
    "            inst_str_dict = df_[['institution_id','strength']].set_index('institution_id').to_dict()['strength']\n",
    "\n",
    "            df_intra['m_source'] = df_intra['source'].map(inst_str_dict)\n",
    "            #df_intra['m_target'] = df_intra['target'].map(inst_str_dict)\n",
    "            df_inter['m_source'] = df_inter['source'].map(inst_str_dict)\n",
    "            df_inter['m_target'] = df_inter['target'].map(inst_str_dict)\n",
    "            df_inter['m_prod1'] = df_inter['m_source']**2*df_inter['m_target']\n",
    "            df_inter['m_prod2'] = df_inter['m_source']*df_inter['m_target']**2\n",
    "            df_inter['m_prod'] = (df_inter['m_prod1']**a+df_inter['m_prod2']**a) #3 authors\n",
    "            df_inter = df_inter.drop(columns=['m_prod1','m_prod2'])\n",
    "            df_inter3['m_i'] = df_inter3['i'].map(inst_str_dict)\n",
    "            df_inter3['m_j'] = df_inter3['j'].map(inst_str_dict)\n",
    "            df_inter3['m_k'] = df_inter3['k'].map(inst_str_dict)\n",
    "            df_inter3['m_prod'] = (df_inter3['m_i']*df_inter3['m_j']*df_inter3['m_k'])**a\n",
    "            df_inter3 = df_inter3[['i','j','k','m_i','m_j','m_k','m_prod','dist']]  \n",
    "            \n",
    "            df_intra['p']= gamma*df_intra['m_source'] \n",
    "            df_inter['p'] = df_inter['m_prod'] * ( beta / ((2/3)*df_inter['dist']+c)**alpha) \n",
    "            df_inter3['p'] = df_inter3['m_prod'] * ( beta / ((df_inter3['dist']+c)**alpha)) \n",
    "\n",
    "            hyperedges_probabilities = list(itertools.chain(list(itertools.chain(df_intra['p'], df_inter['p'])),df_inter3['p']))\n",
    "            hyperedges_probabilities = np.array(hyperedges_probabilities)/(df_intra['p'].sum()+df_inter['p'].sum()+df_inter3['p'].sum())\n",
    "            W = int(1e5) #df_data3_list[i]\n",
    "            hyperedges_model = random.choices(np.arange(0, len(hyperedges_probabilities)), weights=hyperedges_probabilities, k=W)\n",
    "\n",
    "            #count edges\n",
    "            counter = dict(collections.Counter(hyperedges_model))\n",
    "            df_intra['count'] = df_intra.index.to_series().map(counter)\n",
    "            df_intra['count'] = df_intra['count'].fillna(0)\n",
    "            df_inter['count'] = df_inter.index.to_series().map(counter)\n",
    "            df_inter['count'] = df_inter['count'].fillna(0)\n",
    "            df_inter3['count'] = df_inter3.index.to_series().map(counter)\n",
    "            df_inter3['count'] = df_inter3['count'].fillna(0)\n",
    "\n",
    "            #count intra-inter\n",
    "            frac_intra = (3*sum(df_intra['count'])+1*sum(df_inter['count']))/(3*df_intra['count'].sum()+3*df_inter['count'].sum()+3*df_inter3['count'].sum())\n",
    "            fra_intra_list.append(frac_intra)\n",
    "\n",
    "            #mean team distace\n",
    "            df_inter['mean_dist'] = (2/3)*df_inter['dist']*df_inter['count']\n",
    "            df_inter3['mean_dist'] = df_inter3['dist']*df_inter3['count']\n",
    "            mean_dist = (df_inter.mean_dist.sum()+df_inter3.mean_dist.sum())/W\n",
    "            mean_dist_list.append(mean_dist)\n",
    "\n",
    "            #update strength\n",
    "            df_intra['count'] = df_intra['count']*2\n",
    "            df_inter3['count'] = df_inter3['count']*(1/3)\n",
    "            temp = pd.concat([\n",
    "                df_intra[['source','count']].rename(columns={'source':'institution_id'}),\n",
    "                df_inter[['source','count']].rename(columns={'source':'institution_id'}),\n",
    "                df_inter[['target','count']].rename(columns={'target':'institution_id'}),\n",
    "                df_inter3[['i','count']].rename(columns={'i':'institution_id'}),\n",
    "                df_inter3[['j','count']].rename(columns={'j':'institution_id'}),\n",
    "                df_inter3[['k','count']].rename(columns={'k':'institution_id'})])\n",
    "            temp = temp.groupby('institution_id')['count'].sum().to_frame().reset_index()\n",
    "            df_old = df_\n",
    "            df_ = df_[['institution_id','strength']]\n",
    "            df_ = df_.merge(temp,on='institution_id',how='left')\n",
    "            df_['count'] = df_['count'].fillna(0)\n",
    "            df_['strength'] = df_['strength'] + df_['count']\n",
    "            df_ = df_[['institution_id','strength']]\n",
    "\n",
    "\n",
    "        model_data = pd.DataFrame.from_dict({'fra_intra':fra_intra_list,'mean_dist':mean_dist_list})\n",
    "        model_data['month'] = months_list[end_index:]\n",
    "        my_file = \"simulation_triangles_\"+str(s)+\"_b.csv\" \n",
    "        model_data.to_csv(os.path.join(my_path_, my_file))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383a8f02-3a27-4a09-ad4f-a7585b40ad69",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "simulation_b()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ef0921-3802-46c3-b66d-de8e1c1c44ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a19e40b-08dc-4fc9-b837-24e6b49dd6b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_simulation_b():\n",
    "    \n",
    "    my_file = \"simulation_triangles_\"+str(0)+\"_b.csv\"  \n",
    "    model_data = pd.read_csv(os.path.join(my_path_, my_file))\n",
    "    model_data['month'] = pd.to_datetime(model_data['month'])\n",
    "    months_list = list(model_data['month'])\n",
    "    months_list.sort()\n",
    "    months_list = [i.strftime('%Y-%m-%d') for i in months_list]\n",
    "    \n",
    "    #calculate #each month: F_INTRA\n",
    "    my_file = \"df_data1.csv\"    \n",
    "    df_data1 = pd.read_csv(os.path.join(my_path_, my_file))\n",
    "    df_data1 = df_data1.set_index('publication_date_1')\n",
    "    df_data1 = df_data1[['frac_intra']]\n",
    "    df_data1 = df_data1.rename(columns={'frac_intra':'F_aff'})\n",
    "    df_data1 = df_data1.loc[months_list] #[months_list[end_index]:months_list[-1]]\n",
    "    #each month: average distance\n",
    "    my_file = \"work_edges_dist_mean_monthly.csv\"    \n",
    "    work_edges_dist_mean_monthly = pd.read_csv(os.path.join(my_path_, my_file))\n",
    "    work_edges_dist_mean_monthly = work_edges_dist_mean_monthly.set_index('publication_date_1')\n",
    "    work_edges_dist_mean_monthly = work_edges_dist_mean_monthly.loc[months_list]\n",
    "    df_data2 = work_edges_dist_mean_monthly\n",
    "\n",
    "    for s in range(10):\n",
    "        my_file = \"simulation_triangles_\"+str(s)+\"_b.csv\" \n",
    "        model_data = pd.read_csv(os.path.join(my_path_, my_file))\n",
    "        model_data['month'] = pd.to_datetime(model_data['month'])\n",
    "        months_list = list(model_data['month'])\n",
    "        months_list.sort()\n",
    "        months_list = [i.strftime('%Y-%m-%d') for i in months_list]\n",
    "\n",
    "        plt.style.use(\"dark_background\")\n",
    "        fig, ax = plt.subplots(figsize=(15, 5))\n",
    "        x_dates = list(model_data['month'])\n",
    "        x_data = x_dates\n",
    "        y_data1 = list(df_data1.F_aff)   \n",
    "        y_data2 = list(model_data['fra_intra'])\n",
    "        ax.plot(x_data, y_data1, \"o-\", markersize=3,label='data')\n",
    "        ax.plot(x_data, y_data2, \"o-\", markersize=3,label='model')\n",
    "        ax.axvline(pd.Timestamp(2020, 3, 1),color='r')\n",
    "        plt.grid(True, linewidth=0.5)\n",
    "        ax.yaxis.set_major_formatter(formatter)\n",
    "        ax.set_xlabel('month',size=20)\n",
    "        ax.legend() \n",
    "        ax.set_title('Frac intra-insts collabs - with exact params',size=30)\n",
    "        \n",
    "    for s in range(10):\n",
    "        my_file = \"simulation_triangles_\"+str(s)+\"_b.csv\"  \n",
    "        model_data = pd.read_csv(os.path.join(my_path_, my_file))\n",
    "        model_data['month'] = pd.to_datetime(model_data['month'])\n",
    "        months_list = list(model_data['month'])\n",
    "        months_list.sort()\n",
    "        months_list = [i.strftime('%Y-%m-%d') for i in months_list]\n",
    "\n",
    "        plt.style.use(\"dark_background\")\n",
    "        fig, ax = plt.subplots(figsize=(15, 5))\n",
    "        x_data = x_dates\n",
    "        y_data1 = list(df_data2.dist)   \n",
    "        y_data2 = list(model_data['mean_dist'])\n",
    "        ax.plot(x_data, y_data1, \"o-\", markersize=3,label='data')\n",
    "        ax.plot(x_data, y_data2, \"o-\", markersize=3,label='model')\n",
    "        ax.axvline(pd.Timestamp(2020, 3, 1),color='r')\n",
    "        plt.grid(True, linewidth=0.5)\n",
    "        ax.yaxis.set_major_formatter(formatter)\n",
    "        ax.set_xlabel('month',size=20)\n",
    "        ax.legend() \n",
    "        ax.set_title('Avg team dist - with exact params',size=30)\n",
    "\n",
    "plot_simulation_b()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3d2e2e-f84b-47bf-9ff6-44ec72ef2d36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee00f79-9df1-48f9-b23c-264639c6fd0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "484e533b-6701-4cef-8b75-6682e56f83df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
